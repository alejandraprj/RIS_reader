TY  - JOUR
AU  - Schuettpelz, E
AU  - Frandsen, PB
AU  - Dikow, RB
AU  - Brown, A
AU  - Orli, S
AU  - Peters, M
AU  - Metallo, A
AU  - Funk, VA
AU  - Dorr, LJ
TI  - Applications of deep convolutional neural networks to digitized natural history collections
T2  - BIODIVERSITY DATA JOURNAL
LA  - English
KW  - convolutional neural networks
KW  - deep learning
KW  - machine learning
KW  - mass digitization
KW  - natural history collections
KW  - SPECIMENS
AB  - Natural history collections contain data that are critical for many scientific endeavors. Recent efforts in mass digitization are generating large datasets from these collections that can provide unprecedented insight. Here, we present examples of how deep convolutional neural networks can be applied in analyses of imaged herbarium specimens. We first demonstrate that a convolutional neural network can detect mercury-stained specimens across a collection with 90% accuracy. We then show that such a network can correctly distinguish two morphologically similar plant families 96% of the time. Discarding the most challenging specimen images increases accuracy to 94% and 99%, respectively. These results highlight the importance of mass digitization and deep learning approaches and reveal how they can together deliver powerful new investigative tools.
AD  - Smithsonian Inst, Natl Museum Nat Hist, Washington, DC 20560 USAAD  - Smithsonian Inst, Off Chief Informat Officer, Washington, DC 20560 USAAD  - NVIDIA, Santa Clara, CA USAC3  - Smithsonian InstitutionC3  - Smithsonian National Museum of Natural HistoryC3  - Smithsonian InstitutionC3  - Nvidia CorporationCR  - Barkworth ME, 2012, ZOOKEYS, P55, DOI 10.3897/zookeys.209.3205
CR  - Beaman RS, 2012, ZOOKEYS, P7, DOI 10.3897/zookeys.209.3313
CR  - Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z
CR  - Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
CR  - Hawks C, 2004, TAXON, V53, P783, DOI 10.2307/4135451
CR  - He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Suarez AV, 2004, BIOSCIENCE, V54, P66, DOI 10.1641/0006-3568(2004)054[0066:TVOMCF]2.0.CO;2
CR  - Unger J, 2016, BMC EVOL BIOL, V16, DOI 10.1186/s12862-016-0827-5
PU  - PENSOFT PUBLISHERS
PI  - SOFIA
PA  - 12 PROF GEORGI ZLATARSKI ST, SOFIA, 1700, BULGARIA
DA  - NOV 2
PY  - 2017
VL  - 5
DO  - 10.3897/BDJ.5.e21139
AN  - WOS:000449745900001
N1  - Times Cited in Web of Science Core Collection:  27
Total Times Cited:  27
Cited Reference Count:  9
ER  -

TY  - JOUR
AU  - Ruff, ZJ
AU  - Lesmeister, DB
AU  - Appel, CL
AU  - Sullivan, CM
TI  - Workflow and convolutional neural network for automated identification of animal sounds
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Bioacoustics
KW  - Machine learning
KW  - Wildlife
KW  - Ecology
KW  - Passive acoustic monitoring
KW  - Artificial intelligence
AB  - The use of passive acoustic monitoring in wildlife ecology has increased dramatically in recent years as researchers take advantage of improvements in autonomous recording units and analytical methods. These technologies have allowed researchers to collect large quantities of acoustic data which must then be processed to extract meaningful information, e.g. target species detections. A persistent issue in acoustic monitoring is the challenge of efficiently automating the detection of species of interest, and deep learning has emerged as a powerful approach to accomplish this task. Here we report on the development and application of a deep convolutional neural network for the automated detection of 14 forest-adapted birds and mammals by classifying spectrogram images generated from short audio clips. The neural network performed well for most species, with precision exceeding 90% and recall exceeding 50% at high score thresholds, indicating high power to detect these species when they were present and vocally active, combined with a low proportion of false positives. We describe a multi-step workflow that integrates this neural network to efficiently process large volumes of audio data with a combination of automated detection and human review. This workflow reduces the necessary human effort by > 99% compared to full manual review of the data. As an optional component of this workflow, we developed a graphical interface for the neural network that can be run through RStudio using the Shiny package, creating a portable and user-friendly way for field biologists and managers to efficiently process audio data and detect these target species close to the point of collection and with minimal delays using consumer-grade computers.
AD  - USDA Forest Serv, Pacific Northwest Res Stn, Corvallis, OR USAAD  - Oak Ridge Inst Sci & Educ, Oak Ridge, TN USAAD  - Oregon State Univ, Dept Fisheries & Wildlife, Corvallis, OR 97331 USAAD  - Oregon State Univ, Ctr Genome Res & Biocomp, Corvallis, OR 97331 USAC3  - United States Department of Agriculture (USDA)C3  - United States Forest ServiceC3  - United States Department of Energy (DOE)C3  - Oak Ridge Institute for Science & EducationC3  - Oregon State UniversityC3  - Oregon State UniversityFU  - USDA Forest Service; USDI Bureau of Land Management
FX  - Funding for this research was provided by the USDA Forest Service and USDI Bureau of Land Management, and we thank R. Davis, B. Hollen, and G. McFadden for facilitating that funding. We thank the many field biologists and lab technicians that collected and validated much of the data presented here, including C. Cardillo, D. Culp, L. Duchac, Z. Farrand, T. Garrido, E. Guzman, A. Ingrassia, D. Jackobsma, E. Johnston, R. Justice, K. McLaughlin, A. Munes, P. Papajcik, J. Runjaic, and S. Sabin. The primary computer system used for the development of the convolutional neural network was owned and administered by the Center for Genome Research and Biocomputing at Oregon State University. The findings and conclusions in this publication are those of the authors and should not be construed to represent any official U.S. Department of Agriculture or U.S. Government determination or policy. The use of trade or firm names in this publication is for reader information and does not imply endorsement by the U.S. Government of any product or service.
CR  - Abadi M., 2015, TensorFlow: large-scale machine learning on heterogeneous systems
CR  - [Anonymous], 107375, DOI [10.1016/j.apacoust.2020.107375., DOI 10.1016/J.APACOUST.2020.107375]
CR  - Artuso C., 2013, BIRDS N AM, DOI [10.2173/bna.372., 10.2173/bna.372, DOI 10.2173/BNA.372]
CR  - Boarman W.I., 1999, BIRDS N AM, DOI [10.2173/bna.476., DOI 10.2173/BNA.476]
CR  - BRAND LR, 1976, ANIM BEHAV, V24, P319, DOI 10.1016/S0003-3472(76)80040-1
CR  - Brodrick PG, 2019, TRENDS ECOL EVOL, V34, P734, DOI 10.1016/j.tree.2019.03.006
CR  - Cannings R.J., 2017, BIRDS N AM, DOI [10.2173/bna.wesowl1.03, DOI 10.2173/BNA.WESOWL1.03]
CR  - Chollet F., 2015, KERAS
CR  - Duchac LS, 2020, CONDOR, V122, DOI 10.1093/condor/duaa017
CR  - Dugger KM, 2016, CONDOR, V118, P57, DOI 10.1650/CONDOR-15-24.1
CR  - FORSMAN ED, 1984, J WILDLIFE MANAGE, P5
CR  - Forsman ED, 2011, STUD AVIAN BIOL, P1
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Gutierrez R.J., 1999, BIRDS N AM ONLINE, DOI [10.2173/bna.457., DOI 10.2173/BNA.457]
CR  - Gutierrez R. J., 2020, BIRDS OF THE WORLD, DOI [10.2173/bow.spoowl.01, DOI 10.2173/BOW.SPOOWL.01]
CR  - Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
CR  - Holt D.W., 2000, BIRDS N AM, DOI 10.2173/bna.494.
CR  - Hutter F., 2019, INT C LEARN REPR 201
CR  - Jackson J.A., 2011, BIRDS N AM, DOI [10.2173/bna.148., DOI 10.2173/BNA.148]
CR  - Jenkins JMA, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42426-0
CR  - Keppie D.M., 2000, BIRDS N AM, DOI [10.2173/bna.530., DOI 10.2173/BNA.530]
CR  - Kingma DP, 2015, INT C LEARN REPR 201
CR  - Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
CR  - LeBien J, 2020, ECOL INFORM, V59, DOI 10.1016/j.ecoinf.2020.101113
CR  - Lesmeister D.B., RES PAPER PNW RPXXX
CR  - Lesmeister D.B, 2018 ANN RES REPORT
CR  - Lesmeister Damon B., 2018, U S Forest Service Pacific Northwest Research Station General Technical Report PNW-GTR, V1, P245
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Odom KJ, 2010, CONDOR, V112, P549, DOI 10.1525/cond.2010.090163
CR  - Prince P, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030553
CR  - Rasmussen J.L., 2008, BIRDS N AM, DOI [10.2173/bna.42., DOI 10.2173/BNA.42]
CR  - RStudio Team, 2020, RSTUDIO INT DEV R
CR  - Ruff ZJ, 2020, REMOTE SENS ECOL CON, V6, P79, DOI 10.1002/rse2.125
CR  - Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
CR  - Salamon J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0166866
CR  - Sanders T.A., 2015, BAND TAILED PIGEON P
CR  - Sebastian-Gonzalez E, 2015, ECOL EVOL, V5, P4696, DOI 10.1002/ece3.1743
CR  - Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
CR  - SMITH CC, 1978, J MAMMAL, V59, P793, DOI 10.2307/1380144
CR  - Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Venier LA, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01029-120202
CR  - Walker L.E. P., 2016, BIRDS N AM, DOI [10.2173/ bna.343., DOI 10.2173/BNA.343]
CR  - Walters E.L., 2014, BIRDS N AM
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Wiens JD, 2019, BIOL CONSERV, V238, DOI 10.1016/j.biocon.2019.108238
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yund PO, 2000, TRENDS ECOL EVOL, V15, P10, DOI 10.1016/S0169-5347(99)01744-9
CR  - Zhong M, 2020, APPL ACOUST, V166, DOI 10.1016/j.apacoust.2020.107375
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2021
VL  - 124
DO  - 10.1016/j.ecolind.2021.107419
AN  - WOS:000624332200007
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  49
ER  -

TY  - CPAPER
AU  - Ben Atitallah, S
AU  - Driss, M
AU  - Boulila, W
AU  - Koubaa, A
AU  - Atitallah, N
AU  - Ben Ghezala, H
ED  - Watrobski, J
ED  - Salabun, W
ED  - Toro, C
ED  - Zanni-Merk, C
ED  - Howlett, RJ
ED  - Jain, LC
TI  - An Enhanced Randomly Initialized Convolutional Neural Network for Columnar Cactus Recognition in Unmanned Aerial Vehicle imagery
T2  - KNOWLEDGE-BASED AND INTELLIGENT INFORMATION & ENGINEERING SYSTEMS (KSE 2021)
LA  - English
CP  - 25th KES International Conference on Knowledge-Based and Intelligent Information & Engineering Systems (KES)
KW  - Convolutional neural networks
KW  - Weight initialization
KW  - Randomization
KW  - Remote sensing images
KW  - Recognition
KW  - Columnar cactus
AB  - Recently, Convolutional Neural Networks (CNNs) have made a great performance for remote sensing image classification. Plant recognition using CNNs is one of the active deep learning research topics due to its added-value in different related fields, especially environmental conservation and natural areas preservation. Automatic recognition of plants in protected areas helps in the surveillance process of these zones and ensures the sustainability of their ecosystems. In this work, we propose an Enhanced Randomly Initialized Convolutional Neural Network (ERI-CNN) for the recognition of columnar cactus, which is an endemic plant that exists in the Tehuacan-Cuicatlan Valley in southeastern Mexico. We used a public dataset created by a group of researchers that consists of more than 20000 remote sensing images. The experimental results confirm the effectiveness of the proposed model compared to other models reported in the literature like InceptionV3 and the modified LeNet-5 CNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall, 97.5% as f1-score, and 0.056 loss. (C) 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://crativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.
AD  - Univ Manouba, Natl Sch Comp Sci, RIADI Lab, Manouba, TunisiaAD  - Taibah Univ, Coll Comp Sci & Engn, IS Dept, Medina, Saudi ArabiaAD  - Prince Sultan Univ, Robot & Internet Things Lab, Riyadh, Saudi ArabiaAD  - Arab Open Univ, Fac Comp Studies, Riyadh, Saudi ArabiaAD  - Univ Sfax, Natl Engn Sch Sfax, CES Lab, Sfax, TunisiaC3  - Universite de la ManoubaC3  - Taibah UniversityC3  - Prince Sultan UniversityC3  - Arab Open University-Saudi ArabiaC3  - Universite de SfaxC3  - Ecole Nationale dIngenieurs de Sfax (ENIS)CR  - Ben Atitallah S, 2020, COMPUT SCI REV, V38, DOI 10.1016/j.cosrev.2020.100303
CR  - Boulila W, 2021, COMPUT ELECTRON AGR, V182, DOI 10.1016/j.compag.2021.106014
CR  - Boulila W, 2017, J COMPUT SCI-NETH, V23, P58, DOI 10.1016/j.jocs.2017.10.006
CR  - Chebbi I, 2018, 2018 4TH INTERNATIONAL CONFERENCE ON ADVANCED TECHNOLOGIES FOR SIGNAL AND IMAGE PROCESSING (ATSIP)
CR  - Chi MM, 2016, P IEEE, V104, P2207, DOI 10.1109/JPROC.2016.2598228
CR  - Dai HN, 2020, ENTERP INF SYST-UK, V14, P1279, DOI 10.1080/17517575.2019.1633689
CR  - Delgado JA, 2019, FRONT SUSTAIN FOOD S, V3, DOI 10.3389/fsufs.2019.00054
CR  - Dudek G, 2019, LECT NOTES COMPUT SC, V11506, P517, DOI 10.1007/978-3-030-20521-8_43
CR  - Ferchichi Ahlem, 2017, Vietnam Journal of Computer Science, V4, P195, DOI 10.1007/s40595-016-0088-7
CR  - Gallicchio C., 2020, STUD COMPUT INTELL, V896, P43, DOI [10.1007/978-3-030-43883-8_3, DOI 10.1007/978-3-030-43883-8_3]
CR  - Hajjaji Y, 2021, COMPUT SCI REV, V39, DOI 10.1016/j.cosrev.2020.100318
CR  - Hong D., 2020, IEEE T GEOSCI ELECT, V59, P4340
CR  - Hu G., GEOCARTO INT, P1
CR  - Kattenborn T, 2020, REMOTE SENS ECOL CON, V6, P472, DOI 10.1002/rse2.146
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - LI H, 2020, INT C APPL CRYPT NET, P127, DOI DOI 10.1016/B978-0-12-818981-8.00007-2
CR  - Li WJ, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9010022
CR  - Lopez-Jimenez E, 2019, ECOL INFORM, V52, P131, DOI 10.1016/j.ecoinf.2019.05.005
CR  - Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015
CR  - Meng T, 2020, INFORM FUSION, V57, P115, DOI 10.1016/j.inffus.2019.12.001
CR  - Mohammadi M, 2018, IEEE COMMUN SURV TUT, V20, P2923, DOI 10.1109/COMST.2018.2844341
CR  - Nezami S, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12071070
CR  - Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
CR  - Stoyanova M, 2020, IEEE COMMUN SURV TUT, V22, P1191, DOI 10.1109/COMST.2019.2962586
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
CR  - Yuan QQ, 2020, REMOTE SENS ENVIRON, V241, DOI 10.1016/j.rse.2020.111716
CR  - Zechen Zheng, 2019, 2019 IEEE International Conferences on Ubiquitous Computing & Communications (IUCC) and Data Science and Computational Intelligence (DSCI) and Smart Computing, Networking and Services (SmartCNS), P118, DOI 10.1109/IUCC/DSCI/SmartCNS.2019.00048
CR  - Zhu YX, 2019, NEUROCOMPUTING, V365, P191, DOI 10.1016/j.neucom.2019.07.016
CR  - Zou WT, 2019, IEEE ACCESS, V7, P46621, DOI 10.1109/ACCESS.2019.2907999
PU  - ELSEVIER SCIENCE BV
PI  - AMSTERDAM
PA  - SARA BURGERHARTSTRAAT 25, PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS
PY  - 2021
VL  - 192
SP  - 573
EP  - 581
DO  - 10.1016/j.procs.2021.08.059
AN  - WOS:000720289000058
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  30
ER  -

TY  - CPAPER
AU  - Bowley, C
AU  - Mattingly, M
AU  - Barnas, A
AU  - Ellis-Felege, S
AU  - Desell, T
ED  - Shi, Y
ED  - Fu, H
ED  - Tian, Y
ED  - Krzhizhanovskaya, VV
ED  - Lees, MH
ED  - Dongarra, J
ED  - Sloot, PMA
TI  - Detecting Wildlife in Unmanned Aerial Systems Imagery Using Convolutional Neural Networks Trained with an Automated Feedback Loop
T2  - COMPUTATIONAL SCIENCE - ICCS 2018, PT I
LA  - English
CP  - 18th International Conference on Computational Science (ICCS)
KW  - CITIZEN-SCIENCE
KW  - GRABCUT
AB  - Using automated processes to detect wildlife in uncontrolled outdoor imagery in the field of wildlife ecology is a challenging task. This is especially true in imagery provided by an Unmanned Aerial System (UAS), where the relative size of wildlife is small and visually similar to its background. This work presents an automated feedback loop which can be used to train convolutional neural networks with extremely unbalanced class sizes, which alleviates some of these challenges. This work utilizes UAS imagery collected by the Wildlife@Home project, which has employed citizen scientists and trained experts to go through collected UAS imagery and classify it. Classified data is used as inputs to convolutional neural networks (CNNs) which seek to automatically mark which areas of the imagery contain wildlife. The output of the CNN is then passed to a blob counter which returns a population estimate for the image. The feedback loop was developed to help train the CNNs to better differentiate between the wildlife and the visually similar background and deal with the disparate amount of wildlife training images versus background training images. Utilizing the feedback loop dramatically reduced population count error rates from previously published work, from +150% to -3.93% on citizen scientist data and +88% to +5.24% on expert data.
AD  - Univ North Dakota, Dept Comp Sci, Grand Forks, ND 58202 USAAD  - Univ North Dakota, Dept Biol, Grand Forks, ND USAC3  - University of North Dakota Grand ForksC3  - University of North Dakota Grand ForksCR  - Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
CR  - Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
CR  - Bowley C., 2017, 2017 IEEE 12 INT C E
CR  - Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906
CR  - Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
CR  - Chretien LP, 2016, WILDLIFE SOC B, V40, P181, DOI 10.1002/wsb.629
CR  - COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
CR  - Fischer DA, 2012, MON NOT R ASTRON SOC, V419, P2900, DOI 10.1111/j.1365-2966.2011.19932.x
CR  - Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
CR  - Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
CR  - Gomez A., 2016, ARXIV160306169
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Krizhevsky A, 2009, LEARNING MULTIPLE LA
CR  - Lecun Y., 2010, CORTES
CR  - Lintott CJ, 2008, MON NOT R ASTRON SOC, V389, P1179, DOI 10.1111/j.1365-2966.2008.13689.x
CR  - Maas A. L., 2013, PROC ICML, V28, P1
CR  - Mattingly M, 2016, P IEEE INT C E-SCI, P223, DOI 10.1109/eScience.2016.7870903
CR  - Nesterov Y., 1983, SOV MATH DOKLADY, V27, P372
CR  - Ng AY., 2004, P 21 INT C MACH LEAR, P78, DOI [10.1145/1015330.1015435, DOI 10.1145/1015330.1015435]
CR  - Phillips T., 2008, TRACKING NESTING SUC
CR  - Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
CR  - Voss MA, 2010, AM BIOL TEACH, V72, P437, DOI 10.1525/abt.2010.72.7.9
CR  - Wood C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1001220
CR  - Xu SX, 2016, ECOL INFORM, V33, P24, DOI 10.1016/j.ecoinf.2016.03.005
CR  - York DG, 2000, ASTRON J, V120, P1579, DOI 10.1086/301513
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
PY  - 2018
VL  - 10860
SP  - 69
EP  - 82
DO  - 10.1007/978-3-319-93698-7_6
AN  - WOS:000541531400004
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  27
ER  -

TY  - JOUR
AU  - Hou, J
AU  - He, YX
AU  - Yang, HB
AU  - Connor, T
AU  - Gao, J
AU  - Wang, YJ
AU  - Zeng, YC
AU  - Zhang, JD
AU  - Huang, JY
AU  - Zheng, BC
AU  - Zhou, SQ
TI  - Identification of animal individuals using deep learning: A case study of giant panda
T2  - BIOLOGICAL CONSERVATION
LA  - English
KW  - Deep learning
KW  - convolutional neural network
KW  - Individual identification
KW  - Giant panda
AB  - Giant panda (Ailuropoda melanoleuca) is an iconic species of conservation. However, long-term monitoring of wild giant pandas has been a challenge, largely due to the lack of appropriate method for the identification of target panda individuals. Although there are some traditional methods, such as distance-bamboo stem fragments methods, molecular biological method, and manual visual identification, they all have some limitations that can restrict their application. Therefore, it is urgent to explore a reliable and efficient approach to identify giant panda individuals. Here, we applied the deep learning technology and developed a novel face-identification model based on convolutional neural network to identify giant panda individuals. The model was able to identify 95% of giant panda individuals in the validation dataset. In all simulated field situations where the quality of photo data was degraded, the model still accurately identified more than 90% of panda individuals. The identification accuracy of our model is robust to brightness, small rotation, and cleanness of photos, although large rotation angle (> 20 degrees) of photos has significant influence on the identification accuracy of the model (P < 0.01). Our model can be applied in future studies of giant panda such as long-term monitoring, big data analysis for behavior and be adapted for individual identification of other wildlife species.
AD  - China West Normal Univ, Key Lab Southwest China Wildlife Resources Conser, Minist Educ, Nanchong 637009, Sichuan, Peoples R ChinaAD  - China West Normal Univ, Sch Math & Informat, Nanchang 637009, Sichuan, Peoples R ChinaAD  - Smithsonian Inst, Conservat Biol Inst, Natl Zool Pk, Front Royal, VA 22630 USAAD  - Michigan State Univ, Ctr Syst Integrat & Sustainabil, Dept Fisheries & Wildlife, E Lansing, MI 48823 USAAD  - Univ Arizona, Dept Ecol & Evolutionary Biol, Tucson, AZ 85721 USAAD  - Wolong Nat Reserve, CCRCGP, Wolong 623006, Sichuan, Peoples R ChinaC3  - China West Normal UniversityC3  - China West Normal UniversityC3  - Smithsonian InstitutionC3  - Smithsonian National Zoological Park & Conservation Biology InstituteC3  - Michigan State UniversityC3  - University of ArizonaFU  - National Natural Science Foundation of China [41571517, 31572293]; Key Laboratory of Southwest China Wildlife Resources Conservation (China West Normal University), Ministry of Education of China [XNYB17-2]; Sichuan Science and Technology Program [2019YFG0299]; Research innovation team funding project of China West Normal University [OCTD2018-9]; fund of China West Normal University [YC358, 17E073, 17E074]
FX  - The authors wish to thank H. Luo, P. Wu, Z. Guo, A. Li and L. Li from China west normal university for data collection and collation. We appreciated D. Qi from for his help in data collection. This study was financially supported by the sources of funding: the National Natural Science Foundation of China (41571517, 31572293), the Key Laboratory of Southwest China Wildlife Resources Conservation (China West Normal University), Ministry of Education of China (XNYB17-2), Sichuan Science and Technology Program (2019YFG0299), Research innovation team funding project of China West Normal University (OCTD2018-9), and the fund of China West Normal University (YC358, 17E073, 17E074).
CR  - Bengio Y., 2010, P 13 INT C ARTIFICIA, P249, DOI DOI 10.1177/1753193409103364.
CR  - Bergqvist AS, 2015, LIVEST SCI, V180, P233, DOI 10.1016/j.livsci.2015.06.025
CR  - Brugiere D, 1998, BIOL CONSERV, V86, P15, DOI 10.1016/S0006-3207(98)00015-9
CR  - Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
CR  - Cronin KA, 2012, ANIM BEHAV, V84, P1085, DOI 10.1016/j.anbehav.2012.08.009
CR  - Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
CR  - Fls B.L., 2015, 40 YEARS EVOLUTION D
CR  - Freytag A., 2016, GERM C PATT REC
CR  - Galvez N, 2016, BIOL CONSERV, V204, P350, DOI 10.1016/j.biocon.2016.10.019
CR  - Hou Jin, 2019, Acta Theriologica Sinica, V39, P43, DOI 10.16829/j.slxb.150211
CR  - Hull V, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162266
CR  - Hull V, 2015, J MAMMAL, V96, P230, DOI 10.1093/jmammal/gyu031
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Jackson RM, 2006, WILDLIFE SOC B, V34, P772, DOI 10.2193/0091-7648(2006)34[772:ESLPAU]2.0.CO;2
CR  - Karanth KU, 2014, ORYX, V48, P484, DOI 10.1017/S0030605314000532
CR  - Li BBV, 2018, BIOL CONSERV, V218, P83, DOI 10.1016/j.biocon.2017.11.029
CR  - Lui Y.M., 2009, IEEE INT C BIOM THEO
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Ramos-Fernandez G., 2003, POPULATION SIZE HABI
CR  - Ronneberger O., 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
CR  - Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
CR  - Shi Xuewei, 2016, Acta Ecologica Sinica, V36, P7528
CR  - Shinde AK, 2004, INDIAN J ANIM SCI, V74, P216
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Wong S.N., 2010, AM J PRIMATOL, V68, P465
CR  - Youssif A., 2011, COMPUTER INFORM SCI, V4, P115, DOI DOI 10.5539/CIS.V4N2P115
CR  - Zhan XJ, 2006, CURR BIOL, V16, pR451, DOI 10.1016/j.cub.2006.05.042
CR  - Zhang J Q, 2015, J MAMMAL, V96, P2251
CR  - Zhang JD, 2017, ECOL EVOL, V7, P2575, DOI 10.1002/ece3.2873
CR  - Zhang ZJ, 2014, INTEGR ZOOL, V9, P46, DOI 10.1111/1749-4877.12030
CR  - Zheng X, 2016, J ZOOL, V300, P247, DOI 10.1111/jzo.12377
CR  - Zohdy S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0109528
PU  - ELSEVIER SCI LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
DA  - FEB
PY  - 2020
VL  - 242
DO  - 10.1016/j.biocon.2020.108414
AN  - WOS:000517855100013
N1  - Times Cited in Web of Science Core Collection:  18
Total Times Cited:  19
Cited Reference Count:  32
ER  -

TY  - JOUR
AU  - Quinn, CA
AU  - Burns, P
AU  - Gill, G
AU  - Baligar, S
AU  - Snyder, RL
AU  - Salas, L
AU  - Goetz, SJ
AU  - Clark, ML
TI  - Soundscape classification with convolutional neural networks reveals temporal and geographic patterns in ecoacoustic data
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Machine learning
KW  - Convolutional neural network (CNN)
KW  - Ecoacoustics
KW  - Anthropophony
KW  - Biophony
KW  - Naturally quiet landscapes
KW  - Soundscape ecology
KW  - ACOUSTIC INDEXES
KW  - BIODIVERSITY
KW  - NOISE
KW  - ENVIRONMENT
KW  - TEMPERATE
KW  - IMPACTS
AB  - Interest in ecoacoustics has resulted in an influx of acoustic data and novel methodologies to classify and relate landscape sound activity to biodiversity and ecosystem health. However, indicators used to summarize sound and quantify the effects of disturbances on biodiversity can be inconsistent when applied across ecological gradients. This study used an acoustic dataset of 487,148 min from 746 sites collected over 4 years across Sonoma County, California, USA, by citizen scientists. We built a custom labeled dataset of soundscape components and applied a deep learning framework to test our ability to predict these soundscape components: human noise (Anthropophony), wildlife vocalizations (Biophony), weather phenomena (Geophony), Quiet periods, and microphone Interference. These soundscape components allowed us to balance predicting variation in environmental recordings and relative time to build a custom labeled dataset. We used these data to quantify soundscape patterns across space and time that could be useful for environmental planning, ecosystem conservation and restoration, and biodiversity monitoring. We describe a pre-trained convolutional neural network, fine-tuned with our sound reference data, with classification achieving an overall F0.75-score of 0.88, precision of 0.94, and recall of 0.80 across the five target soundscape components. We deployed the model to predict soundscape components for all acoustic data and assess their hourly patterns. We noted an increase in Biophony in the early morning and evening, coinciding with peak animal community vocalization (e.g., dawn chorus). Anthropophony increased during morning/daylight hours and was lowest in the evenings, coinciding with diurnal patterns in human activity. Further, we examined soundscape patterns related to geographic properties at recording sites. Anthropophony decreased with increasing distance to major roads, while Quiet increased. Biophony and Quiet were comparable to Anthropophony at more urban/developed and agriculture/barren sites, while Biophony and Quiet were significantly higher than Anthropophony at less-developed shrubland, oak woodland, and conifer forest sites. These results demonstrate that acoustic classification of broad soundscape components is possible with small datasets, and classifications can be applied to a large acoustic dataset to gain ecological knowledge.
AD  - No Arizona Univ, Sch Informat Comp & Cyber Syst, 1295 Knoles Dr, Flagstaff, AZ 86011 USAAD  - Sonoma State Univ, Dept Comp Sci, Rohnert Pk, CA USAAD  - Univ Calif, Elect Engn & Comp Sci, Merced, CA USAAD  - Point Blue Conservat Sci, Petaluma, CA USAAD  - Sonoma State Univ, Ctr Interdisciplinary Geospatial Anal Geog Enviro, Rohnert Pk, CA USAC3  - Northern Arizona UniversityC3  - California State University SystemC3  - Sonoma State UniversityC3  - University of California SystemC3  - University of California MercedC3  - California State University SystemC3  - Sonoma State UniversityFU  - NASA's Citizen Science for Earth Systems Program [16-CSESP 2016-0009, 80NSSC18M0107]; Arizona's Technology and Research Initiative Fund
FX  - The Soundscapes to Landscapes project was funded by NASA's Citizen Science for Earth Systems Program 16-CSESP 2016-0009 under cooperative agreement 80NSSC18M0107. We are grateful to the hundreds of citizen scientists who participated in Soundscapes to Landscapes between 2017-2021. The following citizen scientists are recognized by name for each of their expert contributions of over 100 volunteer hours spent working on the project: Wendy Schackwitz (777 hours), David Leland (702 hours), Taylour Stephens (279 hours), Jade Spector (208 hours), Tiffany Erickson (190 hours), Teresa Tuffli (140 hours), Miles Tuffli (129 hours), Katie Clas (121 hours), Bob Hasenick (119 hours). This work benefited from the Intellichirp Northern Arizona University capstone team, who helped develop the S2L ABGQI ROI dataset (Steven Enriquez, Josh Kruse, Michael Ewers, Zhenyu Lei) and the Sonoma State University Koret team who helped with early model development (Alex Dewey, Antone Silveria, Jonathan Calderon Chavez, Vincent Valenzuela). We want to thank Pepperwood Preserve for the generous site access across multiple field seasons and for providing meteorological data. We ran computational analyses on Northern Arizona University's Monsoon computing cluster, funded by Arizona's Technology and Research Initiative Fund.
CR  - Abadi M., 2015, TensorFlow: large-scale machine learning on heterogeneous systems
CR  - Abdi L, 2016, IEEE T KNOWL DATA EN, V28, P238, DOI 10.1109/TKDE.2015.2458858
CR  - Aletta F, 2020, NOISE MAPP, V7, P123, DOI 10.1515/noise-2020-0011
CR  - Araya-Salas M, 2017, METHODS ECOL EVOL, V8, P184, DOI 10.1111/2041-210X.12624
CR  - Balantic CM, 2020, BIOACOUSTICS, V29, P296, DOI 10.1080/09524622.2019.1605309
CR  - Barber JR, 2011, LANDSCAPE ECOL, V26, P1281, DOI 10.1007/s10980-011-9646-7
CR  - Bedoya C, 2017, ECOL INDIC, V75, P95, DOI 10.1016/j.ecolind.2016.12.018
CR  - Boelman NT, 2007, ECOL APPL, V17, P2137, DOI 10.1890/07-0004.1
CR  - Bradfer-Lawrence T, 2019, METHODS ECOL EVOL, V10, P1796, DOI 10.1111/2041-210X.13254
CR  - Burns P, 2020, ENVIRON RES LETT, V15, DOI 10.1088/1748-9326/ab80ee
CR  - Bush A, 2017, NAT ECOL EVOL, V1, DOI 10.1038/s41559-017-0176
CR  - Buxton RT, 2019, FRONT ECOL ENVIRON, V17, P559, DOI 10.1002/fee.2112
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Coban EB, 2020, INT CONF ACOUST SPEE, P726, DOI 10.1109/ICASSP40776.2020.9053338
CR  - Depraetere M, 2012, ECOL INDIC, V13, P46, DOI 10.1016/j.ecolind.2011.05.006
CR  - Desjonqueres C, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31798-4
CR  - Doser JW, 2020, LANDSCAPE ECOL, V35, P689, DOI 10.1007/s10980-020-00973-2
CR  - Droge S., Ecol. Indic., V120, P2021
CR  - Duchac LS, 2020, CONDOR, V122, DOI 10.1093/condor/duaa017
CR  - Dumyahn SL, 2011, LANDSCAPE ECOL, V26, P1327, DOI 10.1007/s10980-011-9635-x
CR  - Eldridge A, 2018, ECOL INDIC, V95, P939, DOI 10.1016/j.ecolind.2018.06.012
CR  - Fairbrass AJ, 2019, METHODS ECOL EVOL, V10, P186, DOI 10.1111/2041-210X.13114
CR  - Fairbrass AJ, 2017, ECOL INDIC, V83, P169, DOI 10.1016/j.ecolind.2017.07.064
CR  - Farina A, 2014, ECOL INFORM, V21, P120, DOI 10.1016/j.ecoinf.2013.10.008
CR  - Ferrell R.M., 2021, PEPPERWOOD MET SOIL
CR  - Ferrell R.M., 2021, PEPPERWOOD LONG TERM
CR  - Francis CD, 2017, J ENVIRON MANAGE, V203, P245, DOI 10.1016/j.jenvman.2017.07.041
CR  - Francis CD, 2013, FRONT ECOL ENVIRON, V11, P305, DOI 10.1890/120183
CR  - Furumo PR, 2019, LANDSCAPE ECOL, V34, P911, DOI 10.1007/s10980-019-00815-w
CR  - Gage S.H., 2015, ECOL AGR LANDSCAPES, P360
CR  - Gasc A, 2018, LANDSCAPE ECOL, V33, P1399, DOI 10.1007/s10980-018-0675-3
CR  - Gordon TAC, 2018, P NATL ACAD SCI USA, V115, P5193, DOI 10.1073/pnas.1719291115
CR  - Grant PBC, 2016, CONSERV BIOL, V30, P1320, DOI 10.1111/cobi.12748
CR  - Hill AP, 2018, METHODS ECOL EVOL, V9, P1199, DOI 10.1111/2041-210X.12955
CR  - Holgate B, 2021, ECOL INDIC, V126, DOI 10.1016/j.ecolind.2021.107627
CR  - Joo W, 2011, LANDSCAPE URBAN PLAN, V103, P259, DOI 10.1016/j.landurbplan.2011.08.001
CR  - Kahl S., 2018, ARXIV PREPRINT ARXIV
CR  - Kahl S, 2020, IDENTIFYING BIRDS SO
CR  - Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236
CR  - Kasten EP, 2012, ECOL INFORM, V12, P50, DOI 10.1016/j.ecoinf.2012.08.001
CR  - Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
CR  - Krause B, 2002, EARTH ISL J, V17, P27
CR  - Krause B, 2016, BIOL CONSERV, V195, P245, DOI 10.1016/j.biocon.2016.01.013
CR  - LeBien J, 2020, ECOL INFORM, V59, DOI 10.1016/j.ecoinf.2020.101113
CR  - Lecun Y., 2015, Nature, V521, P436, DOI 10.1038/nature14539
CR  - Lellouch L, 2014, METHODS ECOL EVOL, V5, P495, DOI 10.1111/2041-210X.12178
CR  - Lie A, 2016, INT ARCH OCC ENV HEA, V89, P351, DOI 10.1007/s00420-015-1083-5
CR  - Lin TH, 2020, REMOTE SENS ECOL CON, V6, P236, DOI 10.1002/rse2.141
CR  - LYON RH, 1973, SCIENCE, V179, P1083, DOI 10.1126/science.179.4078.1083
CR  - MacLaren A.R., 2018, Development and Validation of Automated Detection Tools for Vocalizations of Rare and Endangered Anurans, V9, P144, DOI 10.3996/052017-JFWM-047
CR  - McFee B., 2015, PROC 14 PYTHON SCI C, V8, P18, DOI 10.25080/Majora-7b98e3ed-003
CR  - Metcalf OC, 2021, METHODS ECOL EVOL, V12, P421, DOI 10.1111/2041-210X.13521
CR  - Metcalf OC, 2020, ECOL INDIC, V109, DOI 10.1016/j.ecolind.2019.105793
CR  - Mohammed R, 2020, INT CONF INFORM COMM, P243, DOI 10.1109/ICICS49469.2020.239556
CR  - Mullet TC, 2017, BIOSEMIOTICS-NETH, V10, P319, DOI 10.1007/s12304-017-9288-5
CR  - Mullet TC, 2017, NAT AREA J, V37, P332, DOI 10.3375/043.037.0308
CR  - Mullet TC, 2016, LANDSCAPE ECOL, V31, P1117, DOI 10.1007/s10980-015-0323-0
CR  - Naguib M., 2014, SINGING SPACE TIME B, DOI [10.1007/978-94-007-7414-8, DOI 10.1007/978-94-007-7414-8]
CR  - Newport J, 2014, ECOL MANAG RESTOR, V15, P204, DOI 10.1111/emr.12120
CR  - Pavan G., 2017, ECOACOUSTICS ECOL RO, P235, DOI [10.1002/9781119230724.ch14, DOI 10.1002/9781119230724.CH14]
CR  - Perez-Granados C, 2021, IBIS, V163, P765, DOI 10.1111/ibi.12944
CR  - Piczak KJ, 2015, IEEE INT WORKS MACH
CR  - Pieretti N, 2011, ECOL INDIC, V11, P868, DOI 10.1016/j.ecolind.2010.11.005
CR  - Pieretti N, 2013, J ACOUST SOC AM, V134, P891, DOI 10.1121/1.4807812
CR  - Pijanowski BC, 2011, LANDSCAPE ECOL, V26, P1213, DOI 10.1007/s10980-011-9600-8
CR  - Ploton P, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-18321-y
CR  - Python Software Foundation, 2016, PYTHON LANGUAGE REFE
CR  - R Core Team, 2019, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
CR  - Rappaport DI, 2020, ECOL INDIC, V113, DOI 10.1016/j.ecolind.2020.106172
CR  - Rice WL, 2020, LANDSCAPE URBAN PLAN, V194, DOI 10.1016/j.landurbplan.2019.103701
CR  - Rose SJ, 2018, BIOACOUSTICS, V27, P13, DOI 10.1080/09524622.2016.1272003
CR  - Ruff ZJ, 2021, ECOL INDIC, V124, DOI 10.1016/j.ecolind.2021.107419
CR  - Salamon J, 2017, INT CONF ACOUST SPEE, P141, DOI 10.1109/ICASSP.2017.7952134
CR  - Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
CR  - Sanchez-Giraldo C, 2020, REMOTE SENS ECOL CON, V6, P248, DOI 10.1002/rse2.162
CR  - Scarpelli MDA, 2021, FRONT ECOL EVOL, V9, DOI 10.3389/fevo.2021.738537
CR  - Schafer R. Murray, 1993, SOUNDSCAPE OUR SONIC
CR  - Sethi SS, 2020, P NATL ACAD SCI USA, V117, P17049, DOI 10.1073/pnas.2004702117
CR  - Shaw T, 2021, ENVIRON SUSTAIN IND, V11, DOI 10.1016/j.indic.2021.100141
CR  - Shiu Y., 2020, Use of deep neural networks for automated detection of marine mammal species, P1, DOI 10.1038/s41598-020-57549-y
CR  - Shonfield J, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00974-120114
CR  - Slabbekoorn H, 2008, MOL ECOL, V17, P72, DOI 10.1111/j.1365-294X.2007.03487.x
CR  - SOUTHWORTH M, 1969, ENVIRON BEHAV, V1, P49
CR  - Sueur J, 2014, ACTA ACUST UNITED AC, V100, P772, DOI 10.3813/AAA.918757
CR  - Sueur J, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0004065
CR  - Towsey M, 2013, NOISE REMOVAL WAVEFO
CR  - Venables W.N., 2002, MODERN APPL STAT S, V4
CR  - Villanueva-Rivera LJ, 2011, LANDSCAPE ECOL, V26, P1233, DOI 10.1007/s10980-011-9636-9
CR  - Ware HE, 2015, P NATL ACAD SCI USA, V112, P12105, DOI 10.1073/pnas.1504710112
CR  - Wearn OR, 2019, NAT MACH INTELL, V1, P72, DOI 10.1038/s42256-019-0022-7
CR  - WILEY RH, 1978, BEHAV ECOL SOCIOBIOL, V3, P69, DOI 10.1007/BF00300047
CR  - Yip DA, 2017, CONDOR, V119, P73, DOI 10.1650/CONDOR-16-93.1
CR  - Yosinski J., 2014, Adv. Neural Inf. Process. Syst., V4, P3320
CR  - Zhong M, 2020, APPL ACOUST, V166, DOI 10.1016/j.apacoust.2020.107375
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2022
VL  - 138
DO  - 10.1016/j.ecolind.2022.108831
AN  - WOS:000792783400005
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  94
ER  -

TY  - JOUR
AU  - Marty, T
AU  - Yuki, T
AU  - Derrien, S
TI  - Safe Overclocking for CNN Accelerators Through Algorithm-Level Error Detection
T2  - IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS
LA  - English
KW  - Timing
KW  - Convolution
KW  - Clocks
KW  - Field programmable gate arrays
KW  - Throughput
KW  - Prototypes
KW  - Hardware
KW  - Clocks
KW  - energy conservation
KW  - fault tolerance
KW  - field programmable gate arrays
KW  - high performance computing
KW  - neural network hardware
KW  - FAULT-TOLERANCE
AB  - In this article, we propose a technique for improving the efficiency of convolutional neural network hardware accelerators based on timing speculation (overclocking) and fault tolerance. We augment the accelerator with a lightweight error detection mechanism to protect against timing errors in convolution layers, enabling aggressive timing speculation. The error detection mechanism we have developed works at the algorithm-level, utilizing algebraic properties of the computation, allowing the full implementation to be realized using high-level synthesis tools. Our prototype on ZC706 demonstrated up to 60% higher throughput with negligible area overhead for various wordlength implementations.
AD  - Univ Rennes, CNRS, INRIA, IRISA, F-35000 Rennes, FranceC3  - Centre National de la Recherche Scientifique (CNRS)C3  - InriaC3  - Universite de Rennes 1FU  - Brittany Region
FX  - This article was supported in part by the Brittany Region.
CR  - [Anonymous], 2019, HERO OPEN HETEROGENE
CR  - Boehmer E, 2011, INDIAN POSTCOLONIAL: A CRITICAL READER, P1
CR  - BOSILCA G, 2015, INT J NETWORKING COM, V5, P2
CR  - Chen ZZ, 2013, ACM SIGPLAN NOTICES, V48, P167, DOI 10.1145/2517327.2442533
CR  - Davidson J.L., 2014, P 24 INT C FIELD PRO, P1
CR  - Ding CW, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P395, DOI 10.1145/3123939.3124552
CR  - Du P, 2012, ACM SIGPLAN NOTICES, V47, P225, DOI 10.1145/2370036.2145845
CR  - Ernst D, 2003, 36TH INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, PROCEEDINGS, P7
CR  - Fojtik M, 2013, IEEE J SOLID-ST CIRC, V48, P66, DOI 10.1109/JSSC.2012.2220912
CR  - Geng T, 2018, I C FIELD PROG LOGIC, P394, DOI 10.1109/FPL.2018.00074
CR  - Gojman B, 2014, ACM T RECONFIG TECHN, V7, P1
CR  - Han S, 2015, P ADV NEUR INF PROC, P1135
CR  - HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475
CR  - Jacobs A., 2012, 2012 22nd International Conference on Field Programmable Logic and Applications (FPL), P300, DOI 10.1109/FPL.2012.6339222
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Leng JW, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P294, DOI 10.1145/2830772.2830811
CR  - Levine J. M., 2013, PLATELETS, P1
CR  - Levine J. M., 2014, P 2014 ACM SIGDA INT, P65
CR  - Li SC, 2017, ANN IEEE SYM FIELD P, P28, DOI 10.1109/FCCM.2017.21
CR  - Liu J, 2017, SHOCK VIB, V2017, DOI 10.1155/2017/2670218
CR  - Lu LQ, 2017, ANN IEEE SYM FIELD P, P101, DOI 10.1109/FCCM.2017.64
CR  - Nunez-Yanez J, 2019, IEEE T COMPUT, V68, P676, DOI 10.1109/TC.2018.2879333
CR  - Nunez-Yanez JL, 2016, IEEE T COMPUT, V65, P1484, DOI 10.1109/TC.2015.2435771
CR  - Piestrak SJ, 2014, 2014 17TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN (DSD), P575, DOI 10.1109/DSD.2014.110
CR  - REDDY ALN, 1990, IEEE T COMPUT, V39, P1304, DOI 10.1109/12.59860
CR  - RoyChowdhury A, 1996, IEEE T COMPUT, V45, P394, DOI 10.1109/12.494098
CR  - Shi K, 2013, ANN IEEE SYM FIELD P, P29, DOI 10.1109/FCCM.2013.10
CR  - Venieris SI, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3186332
CR  - WANG SJ, 1994, IEEE T COMPUT, V43, P849, DOI 10.1109/12.293265
CR  - Yuki T., 2013, P 26 INT WORKSH LANG, P169
CR  - Zhang C., 2015, PROC ACMSIGDA INT S, P161, DOI [10.1145/2684746.2689060, DOI 10.1145/2684746.2689060]
CR  - Zhang C, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P35, DOI 10.1145/3020078.3021727
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
DA  - DEC
PY  - 2020
VL  - 39
IS  - 12
SP  - 4777
EP  - 4790
DO  - 10.1109/TCAD.2020.2981056
AN  - WOS:000592111400037
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Curran, B
AU  - Nekooei, SM
AU  - Chen, G
ED  - Long, G
ED  - Yu, X
ED  - Wang, S
TI  - Accurate New Zealand Wildlife Image Classification-Deep Learning Approach
T2  - AI 2021: ADVANCES IN ARTIFICIAL INTELLIGENCE
LA  - English
CP  - 34th Australasian Joint Conference on Artificial Intelligence (AI)
KW  - Convolutional neural network
KW  - Image classification
KW  - Wellington camera trap dataset
KW  - SPECIES RECOGNITION
AB  - Image classification is a major machine learning problem that has a wide range of applications in the real world. The Wellington Wildlife Camera Trap dataset contains images taken from vibration triggered cameras in sequences of three. State-of-the-art deep convolutional neural network (CNN) models, such as DenseNet-121 and ResNet-50, are unable to achieve the required accuracy of classification on this dataset. This research aims to improve the performance in multi-class classification tasks on the Wellington Dataset through a newly developed dual-input channel neural network. Our experiment results provide clear evidence that the new CNN model can achieve high accuracy and confidence on this challenging and scientifically important dataset. It is able to significantly reduce the amount of time required to manually classify wildlife images for conservation research in New Zealand.
AD  - Victoria Univ Wellington, Wellington, New ZealandC3  - Victoria University WellingtonCR  - Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
CR  - Anton V, 2018, NEW ZEAL J ECOL, V42, P74, DOI 10.20417/nzjecol.42.3
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Chen RL, 2019, ECOL EVOL, V9, P9453, DOI 10.1002/ece3.5410
CR  - Favorskaya M, 2019, PROCEDIA COMPUT SCI, V159, P933, DOI 10.1016/j.procs.2019.09.260
CR  - Gomes HM, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054925
CR  - HALL EL, 1971, IEEE T COMPUT, VC 20, P1032, DOI 10.1109/T-C.1971.223399
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - Keskar N. S., 2016, ARXIV160904836
CR  - Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
CR  - Maas A. L., 2013, P ICML WORKSH DEEP L, P3
CR  - Mou LC, 2017, IEEE T GEOSCI REMOTE, V55, P3639, DOI 10.1109/TGRS.2016.2636241
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Selfridge O.G., 1955, P W JOINT COMP C, P91
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
CR  - Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
PY  - 2022
VL  - 13151
SP  - 632
EP  - 644
DO  - 10.1007/978-3-030-97546-3_51
AN  - WOS:000787242700051
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  22
ER  -

TY  - JOUR
AU  - Guo, YH
AU  - Rothfus, TA
AU  - Ashour, AS
AU  - Si, L
AU  - Du, CL
AU  - Ting, TF
TI  - Varied channels region proposal and classification network for wildlife image classification under complex environment
T2  - IET IMAGE PROCESSING
LA  - English
KW  - feature extraction
KW  - object recognition
KW  - object detection
KW  - learning (artificial intelligence)
KW  - neural nets
KW  - image segmentation
KW  - image classification
KW  - convolution
KW  - cameras
KW  - varied channels region proposal
KW  - classification network
KW  - wildlife image classification
KW  - deep convolutional neural network
KW  - automatic wildlife animal classification
KW  - camera trapped images
KW  - different aims
KW  - background images
KW  - region proposal component
KW  - region candidates
KW  - classification component
KW  - animals
KW  - potential animal regions
KW  - low contrast animal images
KW  - object detection network
KW  - faster region convolutional neural network
AB  - A varied channels region proposal and classification network (VCRPCN) is developed based on a deep convolutional neural network (DCNN) and the characteristics of the animals appearing for automatic wildlife animal classification in camera trapped images, the architecture of the network is improved by feeding different channels into different components of the network to accomplish different aims, i.e. the animal images and their background images are employed in the region proposal component to extract region candidates for the animal's location, and the animal images combined with the region candidates are fed into the classification component to identify their categories. This novel architecture considers changes to the image due to the animals' appearances, and identifies potential animal regions in images and extracts their local features to describe and classify them. Five hundred low contrast animal images have been collected. All images have low contrast due to being acquired during the night. Cross-validation is employed to statistically measure the performance of the proposed algorithm. The experimental results demonstrate that in comparison with the well-known object detection network, faster R-CNN, the proposed VCRPCN achieved higher accuracy with the same dataset and training configuration with an average accuracy improvement of 21%.
AD  - Univ Illinois, Dept Comp Sci, Springfield, IL 62703 USAAD  - Univ Illinois, Therkildsen Field Stn Emiquon, Springfield, IL USAAD  - Univ Illinois, Dept Environm Studies, Springfield, IL USAAD  - Tanta Univ, Dept Elect & Elect Commun Engn, Tanta, EgyptAD  - North China Univ Technol, Sch Comp Sci, Beijing, Peoples R ChinaC3  - University of Illinois SystemC3  - University of Illinois SpringfieldC3  - University of Illinois SystemC3  - University of Illinois SpringfieldC3  - University of Illinois SystemC3  - University of Illinois SpringfieldC3  - Egyptian Knowledge Bank (EKB)C3  - Tanta UniversityC3  - North China University of TechnologyCR  - Borji A, 2014, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2014.22
CR  - Bridle J. S., 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28
CR  - Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
CR  - Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
CR  - GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
CR  - Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
CR  - Kays R., 2010, MONITORING WILD ANIM
CR  - Law H., 2018, P EUROPEAN C COMPUTE, P734
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Matuska S, 2016, RADIOENGINEERING, V25, P161, DOI 10.13164/re.2016.0161
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Redmon Joseph, 2016, YOU ONLY LOOK ONCE U, DOI DOI 10.1109/CVPR.2016.91
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
CR  - Vitousek PM, 1997, SCIENCE, V277, P494, DOI 10.1126/science.277.5325.494
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Wilber MJ, 2013, IEEE WORK APP COMP, P206, DOI 10.1109/WACV.2013.6475020
CR  - Yu X., 2013, EURASIP J IMAGE VIDE, P561
CR  - Zhang S, 2015, IEEE SENS J, V15, P2679, DOI 10.1109/JSEN.2014.2382174
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - MAR 27
PY  - 2020
VL  - 14
IS  - 4
SP  - 585
EP  - 591
DO  - 10.1049/iet-ipr.2019.1042
AN  - WOS:000520961700001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  32
ER  -

TY  - JOUR
AU  - Bowley, C
AU  - Mattingly, M
AU  - Barnas, A
AU  - Ellis-Felege, S
AU  - Desell, T
TI  - An analysis of altitude, citizen science and a convolutional neural network feedback loop on object detection in Unmanned Aerial Systems
T2  - JOURNAL OF COMPUTATIONAL SCIENCE
LA  - English
CP  - 18th International Conference on Computational Science (ICCS)
KW  - Unmanned Aerial Systems
KW  - Wildlife ecology
KW  - Convolutional neural networks
KW  - Citizen science
KW  - GRABCUT
AB  - Using automated processes to detect wildlife in uncontrolled outdoor imagery in the field of wildlife ecology is a challenging task. In imagery provided by Unmanned Aerial Systems (UAS), this is especially true where individuals are small and visually similar to background substrates. To address these challenges, this work presents an automated feedback loop which can operate on large scale imagery, such as UAS generated orthomosaics, to train convolutional neural networks (CNNs) with extremely unbalanced class sizes. This feedback loop was used to help train CNNs using imagery classified by both expert biologists and citizen scientists at the Wildlife@home project. Utilizing the feedback loop dramatically reduced population count error rates from previously published work: from +150% to 3.93% on citizen scientist training data and +88% to +5.24% on expert training data. The system developed was then utilized to investigate the effect of altitude on CNN predictions. The training dataset was split into three subsets depending on the altitude of the imagery (75 m,100 m and 120 m). While the lowest altitude was shown to provide the best predictions of the three (+11.46%), the aggregate data set still provided the best results (-3.93%) indicating that there is greater benefit to be gained from a large data set at this scale, and there is potential benefit to having training data from multiple altitudes. This article is an extended version of "Detecting Wildlife in Unmanned Aerial Systems Imagery using Convolutional Neural Networks Trained with an Automated Feedback Loop" published in the proceedings of the 18th International Conference of Computational Science (ICCS 2018) [1]. (C) 2019 Elsevier B.V. All rights reserved.
AD  - Rochester Inst Technol, Dept Software Engn, Rochester, NY 14623 USAAD  - Univ North Dakota, Dept Comp Sci, Grand Forks, ND USAAD  - Univ North Dakota, Dept Biol, Grand Forks, ND USAC3  - Rochester Institute of TechnologyC3  - University of North Dakota Grand ForksC3  - University of North Dakota Grand ForksFU  - North Dakota EPSCoR; Hudson Bay Project; UND College of Arts and Sciences; National Science Foundation [1319700]; Central and Mississippi Flyways; North Dakota Department of Commerce
FX  - Funding was provided by North Dakota EPSCoR, the Hudson Bay Project, Central and Mississippi Flyways, North Dakota Department of Commerce, and the UND College of Arts and Sciences. UAS data collection supported by the Hudson Bay Project. Permissions and in -kind assistance were provided by Parks Canada, Wapusk National Park Management Board, and the community of Churchill, Manitoba. This work has been partially supported by the National Science Foundation under Grant Number 1319700. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
CR  - Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
CR  - Barnas A, 2018, ECOL EVOL, V8, P1328, DOI 10.1002/ece3.3731
CR  - Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
CR  - Bowley C., 2017, THESIS
CR  - Bowley C., 2018, 18 INT C COMP SCI WU
CR  - Bowley C., 2017, 2017 IEEE 12 INT C E
CR  - Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906
CR  - Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
CR  - Bulo S. R., 2017, CVPR, V7
CR  - Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
CR  - Chretien LP, 2016, WILDLIFE SOC B, V40, P181, DOI 10.1002/wsb.629
CR  - COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
CR  - Dollar P., 2009, INTEGRAL CHANNEL FEA
CR  - Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
CR  - FELZENSZWALB PF, 2010, PROC CVPR IEEE, P2241, DOI DOI 10.1109/CVPR.2010.5539906
CR  - Fischer DA, 2012, MON NOT R ASTRON SOC, V419, P2900, DOI 10.1111/j.1365-2966.2011.19932.x
CR  - Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148
CR  - Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
CR  - GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Gomez A., 2016, ARXIV160306169
CR  - Horns JJ, 2018, BIOL CONSERV, V221, P151, DOI 10.1016/j.biocon.2018.02.027
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Krizhevsky A, 2009, LEARNING MULTIPLE LA
CR  - LaRue MA, 2015, WILDLIFE SOC B, V39, P772, DOI 10.1002/wsb.596
CR  - Lin T., 2017, CORR
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
CR  - Lintott CJ, 2008, MON NOT R ASTRON SOC, V389, P1179, DOI 10.1111/j.1365-2966.2008.13689.x
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Loshchilov Ilya, 2015, ARXIV151106343
CR  - Maas A. L., 2013, PROC ICML, V28, P1
CR  - Malik J., 2013, RICH FEATURE HIERARC
CR  - Mattingly M, 2016, P IEEE INT C E-SCI, P223, DOI 10.1109/eScience.2016.7870903
CR  - Nesterov Y., 1983, SOV MATH DOKLADY, V27, P372
CR  - Ng A. Y, 2004, P 21 INT C MACH LEAR
CR  - Phillips T., 2008, TRACKING NESTING SUC
CR  - Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
CR  - Redmon J., 2016, IEEE C COMP VIS PATT
CR  - Redmon J., 2016, CORR
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
CR  - Simo-Serra E., 2014, FRACKING DEEP CONVOL
CR  - Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
CR  - Sung K.-K., 1996, LEARNING EXAMPLE SEL
CR  - Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
CR  - Voss MA, 2010, AM BIOL TEACH, V72, P437, DOI 10.1525/abt.2010.72.7.9
CR  - Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320
CR  - Wood C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1001220
CR  - Xu SX, 2016, ECOL INFORM, V33, P24, DOI 10.1016/j.ecoinf.2016.03.005
CR  - York DG, 2000, ASTRON J, V120, P1579, DOI 10.1086/301513
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2019
VL  - 34
SP  - 102
EP  - 116
DO  - 10.1016/j.jocs.2019.04.010
AN  - WOS:000477917200008
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  53
ER  -

TY  - JOUR
AU  - Akagic, A
AU  - Buza, E
TI  - LW-FIRE: A Lightweight Wildfire Image Classification with a Deep Convolutional Neural Network
T2  - APPLIED SCIENCES-BASEL
LA  - English
KW  - artificial neural networks
KW  - computer vision
KW  - machine learning
KW  - deep learning
KW  - deep neural networks
KW  - image classification
KW  - convolutional neural networks
KW  - DATASET
AB  - Analysis of reports published by the leading national centers for monitoring wildfires and other emergencies revealed that the devastation caused by wildfires has increased by 2.96-fold when compared to a decade earlier. The reports show that the total number of wildfires is declining; however, their impact on the wildlife appears to be more devastating. In recent years, deep neural network models have demonstrated state-of-the-art accuracy on many computer vision tasks. In this paper, we describe the design and implementation of a lightweight wildfire image classification model (LW-FIRE) based on convolutional neural networks. We explore different ways of using the existing dataset to efficiently train a deep convolutional neural network. We also propose a new method for dataset transformation to increase the number of samples in the dataset and improve the accuracy and generalization of the deep learning model. Experimental results show that the proposed model outperforms the state-of-the-art methods, and is suitable for real-time classification of wildfire images.
AD  - Univ Sarajevo, Fac Elect Engn, Sarajevo 71000, Bosnia & HercegC3  - University of SarajevoCR  - Chen XW, 2014, IEEE ACCESS, V2, P514, DOI 10.1109/ACCESS.2014.2325029
CR  - Chenebert A, 2011, IEEE IMAGE PROC, P1741, DOI 10.1109/ICIP.2011.6115796
CR  - Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
CR  - Dunnings Andrew J., 2018, 2018 25th IEEE International Conference on Image Processing (ICIP), P1558, DOI 10.1109/ICIP.2018.8451657
CR  - Dzigal D, 2019, 2019 11TH INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONICS ENGINEERING (ELECO 2019), P595, DOI 10.23919/ELECO47770.2019.8990608
CR  - Ganesh Samarth C. A., 2019, 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), P653, DOI 10.1109/ICMLA.2019.00119
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Jadon A., 2019, ARXIV190511922
CR  - King DB, 2015, ACS SYM SER, V1214, P1
CR  - Lyko K., 2016, NEW HORIZONS DATA DR, P39
CR  - Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
CR  - MacDicken KG, 2015, FOREST ECOL MANAG, V352, P3, DOI 10.1016/j.foreco.2015.02.006
CR  - Min EX, 2018, IEEE ACCESS, V6, P39501, DOI 10.1109/ACCESS.2018.2855437
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Rui Hou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8520, DOI 10.1109/CVPR42600.2020.00855
CR  - Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
CR  - Shuvo SB, 2021, IEEE J BIOMED HEALTH, V25, P2595, DOI 10.1109/JBHI.2020.3048006
CR  - Sousa MJ, 2020, EXPERT SYST APPL, V142, DOI 10.1016/j.eswa.2019.112975
CR  - Steffens CR, 2016, COMM COM INF SC, V619, P135, DOI 10.1007/978-3-319-47247-8_9
CR  - Thomson William, 2020, 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), P136, DOI 10.1109/ICMLA51294.2020.00030
CR  - Toulouse T, 2017, FIRE SAFETY J, V92, P188, DOI 10.1016/j.firesaf.2017.06.012
CR  - Wang XG, 2021, COMPUT VIS IMAGE UND, V206, DOI 10.1016/j.cviu.2021.103188
CR  - Yin CL, 2017, IEEE ACCESS, V5, P21954, DOI 10.1109/ACCESS.2017.2762418
CR  - Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
CR  - Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - MAR
PY  - 2022
VL  - 12
IS  - 5
DO  - 10.3390/app12052646
AN  - WOS:000768842200001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  25
ER  -

TY  - CPAPER
AU  - Pashaei, M
AU  - Starek, MJ
A1  - IEEE
TI  - FULLY CONVOLUTIONAL NEURAL NETWORK FOR LAND COVER MAPPING IN A COASTAL WETLAND WITH HYPERSPATIAL UAS IMAGERY
T2  - 2019 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS 2019)
LA  - English
CP  - IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
KW  - Coastal wetland
KW  - semantic segmentation
KW  - deep learning
KW  - convolutional neural network
KW  - unmanned aircraft system
AB  - Coastal wetlands are among the most productive ecosystems in the world providing numerous valuable services for people and wildlife including recreation, fish and wildlife protection, sediment control and flood prevention. Changes in land cover/use have great impact on the functionality and productivity of wetlands. Therefore, accurate and fast monitoring of land cover/use allow policy makers and wetland users to devise and implement policies and management practices to lessen the side effects of any potential change on wetlands. In this study, we apply deep convolutional neural network (DCNN) to infer complex wetland classes in a semantic segmentation task on hyperspatial resolution UAS images. We examine the capacity of the well-known FCN-VGG architecture for wetland mapping. Results illustrate that fine-tuning the network using transfer learning with UAS hyper-resolution images provide state-of-the art semantic segmentation accuracy of 89.98%.
AD  - Texas A&M Univ, Sch Engn & Comp Sci, Corpus Christi, TX 78412 USAAD  - Conrad Blucher Inst Surveying & Sci, Corpus Christi, TX USAC3  - Texas A&M University SystemCR  - Badrinarayanan V., 2015, ARXIV150507293
CR  - Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
CR  - Donahue J, 2014, PR MACH LEARN RES, V32
CR  - Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
CR  - Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
CR  - Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821
CR  - Paine J.G., 2004, LEAD EDGE, V23, P894, DOI DOI 10.1190/1.1803501
CR  - Pendleton Linwood H, 2010, EC MARKET VALUE COAS
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Stedman S.-M., 2008, STATUS TRENDS WETLAN
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2019
SP  - 6106
EP  - 6109
AN  - WOS:000519270605218
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  10
ER  -

TY  - CPAPER
AU  - Song, YL
AU  - Wang, HP
AU  - Li, S
AU  - Xu, FL
AU  - Liu, JT
A1  - IEEE
TI  - CNN based Wildlife Recognition with Super-pixel Segmentation for Ecological Surveillance
T2  - 2018 IEEE 8TH ANNUAL INTERNATIONAL CONFERENCE ON CYBER TECHNOLOGY IN AUTOMATION, CONTROL, AND INTELLIGENT SYSTEMS (IEEE-CYBER)
LA  - English
CP  - 8th IEEE Annual International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (IEEE-CYBER)
KW  - wildlife monitoring
KW  - super-pixel
KW  - convolutional neural network
KW  - low resolution image
AB  - Recent years, the convolutional neural network have shown to provide excellent results on recognition in different competitions. However, challenges in specific missions still exist. The cluttered backgrounds and rich feature changes of wild environment bring great challenges to the problem of species recognition of wild animals. To address these problems, this paper proposes a novel and effective combination to learn a CNN model. This is achieved by apply simple linear iterative clustering (SLIC) super-pixel segmentation method to unified data dimension during the process of making raw image data (captured by camera-traps) into a dataset. In short, the super pixel-divided images provides the input of the convolutional neural network. In order to verify the application, we conducted a comprehensive performance comparisons between our SLIC-dataset and generally used Resize-dataset over CNN networks. Results proved that our proposed method performs exceptionally well in low-resolution data when it is crucial to take full advantage of the edge information of original images. In addition, we collected and annotated a standard camera-trap dataset of 14 common wildlife species in China, which contains 16,480 training images and 4,120 testing images.
AD  - Nankai Univ, Inst Robot & Automat Informat Syst, Tianjin 300353, Peoples R ChinaAD  - Tianjin Key Lab Intelligent Robot, Tianjin 300353, Peoples R ChinaAD  - Peking Univ, Sch Life Sci, Beijing 100871, Peoples R ChinaC3  - Nankai UniversityC3  - Peking UniversityFU  - China Scholarship Council; Key Program of Natural Science Foundation of Tianjin [15JCZDJC31200]; National Natural Science Foundation of China [61375087]
FX  - This work is supported by China Scholarship Council, National Natural Science Foundation of China (Grant No. 61375087) and Key Program of Natural Science Foundation of Tianjin (Grant No. 15JCZDJC31200).
CR  - Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
CR  - Achanta Radhakrishna, 2010, 149300 EPFL
CR  - Borman Sean, 1998, P 1998 MIDW S CIRC S, P374
CR  - Cheng G., 2016, CVPR
CR  - Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
CR  - Cun L, 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - He K., 2015, CVPR
CR  - He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
CR  - Huang JH, 2010, IEEE SYST J, V4, P198, DOI 10.1109/JSYST.2010.2047294
CR  - Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
CR  - Krizhevsky A, 2010, CONVOLUTIONAL UNPUB, V40, P7
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lee H, 2009, ICML, P8, DOI DOI 10.1145/1553374.1553453
CR  - Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
CR  - Linares OAC, 2017, IET IMAGE PROCESS, V11, P1219, DOI 10.1049/iet-ipr.2016.0072
CR  - Monteiro Sildomar T., 2016, IM PROC ICIP 2016 IE
CR  - Moore A.P., 2008, 2008 IEEE C COMP VIS, P1
CR  - Ren Xiaofeng, 2003, LEARNING CLASSIFICAT
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Tian Luchao, 2017, MULT EXP ICME 2017 I
CR  - Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881
CR  - Zhang PJ, 2014, IEEE C ELEC DEVICES
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2018
SP  - 132
EP  - 137
AN  - WOS:000468941800023
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  25
ER  -

TY  - JOUR
AU  - Zhang, GL
AU  - Wang, M
AU  - Liu, K
TI  - Deep neural networks for global wildfire susceptibility modelling
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Wildfire susceptibility
KW  - Convolutional neural network
KW  - Multilayer perceptron neural networks
KW  - Artificial neural networks
KW  - Interpretability
KW  - FOREST-FIRE
KW  - COUNTY
KW  - CLASSIFICATION
KW  - OPTIMIZATION
KW  - ALGORITHMS
KW  - SYSTEM
KW  - CHINA
AB  - Wildfire susceptibility is of great importance to the prevention and management of global wildfires. Artificial neural networks (ANNs), particularly multilayer perceptrons (MLPs), have been widely used in wildfire susceptibility. Recently, deep neural networks (DNNs) have become state-of-the-art algorithms, especially convolutional neural networks (CNNs). However, the applicability of different ANNs in wildfire susceptibility has not been thoroughly discussed, and the interpretability of CNNs remains problematic. This paper consists of two parts: one part deeply compares and analyses the application of two feedforward neural network models (a CNNs and an MLPs) in global wildfire susceptibility prediction, and the other part explores the interpretability of the CNNs model. By constructing response variables from the Global Fire Atlas (GFA) and monthly wildfire predictors, four MLPs and CNNs architectures (namely, the pixel-based CNN-1D and MLP-1D models and the gridbased CNN-2D and MLP-2D models) are constructed for four seasons from 2003 to 2016. After model training, validation and testing, seasonal global susceptibility maps are constructed, and the seasonal peaks in fire activity and highly fire-prone areas can be adequately reflected. Finally, five statistical measures (i.e., the overall accuracy, recall, precision, F1 score, and kappa coefficient), the receiver operating characteristic (ROC) curve, area under the ROC curve (AUC), and Wilcoxon signed-rank tests (WSRTs), were used to evaluate the prediction performance of the models. The results show that the contextual-based CNN-2D model can make full use of the neighbourhood information and has the highest accuracy, the MLPs model is more suitable for pixel-based classification, and the performance ranking of the four models is CNN-2D > MLP-1D > MLP-2D > CNN-1D. In addition, the interpretability of the CNNs was explored using an improved permutation importance (PI) algorithm and partial dependence plots (PDPs). The PI algorithm shows that explanatory variables such as maximum temperature (Tasmax), soil temperature (SoilTemp), normalized difference vegetation index (NDVI) and accumulated precipitation (AccuPre) have a large impact on the model in the four seasons, and the nine one-way and two two-way PDPs perfectly represent how the predictor variables influence the prediction on average.
AD  - Beijing Normal Univ, State Key Lab Earth Surface Proc & Resource Ecol, Beijing 100875, Peoples R ChinaAD  - Beijing Normal Univ, Key Lab Environm Change & Nat Disaster, Beijing 100875, Peoples R ChinaAD  - Acad Disaster Reduct & Emergency Management, Fac Geog Sci, Beijing 100875, Peoples R ChinaC3  - Beijing Normal UniversityC3  - Beijing Normal UniversityFU  - National Key Research and Development Plan [2018YFC1508802]
FX  - This research was supported by the National Key Research and Development Plan (2017YFC1502902) and National Key Research and Development Plan (2018YFC1508802) . The financial support is highly appreciated.
CR  - Andela N, 2019, EARTH SYST SCI DATA, V11, P529, DOI 10.5194/essd-11-529-2019
CR  - Arpaci A, 2014, APPL GEOGR, V53, P258, DOI 10.1016/j.apgeog.2014.05.015
CR  - Atkinson PM, 1997, INT J REMOTE SENS, V18, P699, DOI 10.1080/014311697218700
CR  - Ball JE, 2017, J APPL REMOTE SENS, V11, DOI 10.1117/1.JRS.11.042609
CR  - Bergen KJ, 2019, SCIENCE, V363, P1299, DOI 10.1126/science.aau0323
CR  - Bergstra J, 2012, J MACH LEARN RES, V13, P281
CR  - Cao YX, 2017, INT J DISAST RISK SC, V8, P164, DOI 10.1007/s13753-017-0129-6
CR  - Bui DT, 2019, J ENVIRON MANAGE, V237, P476, DOI 10.1016/j.jenvman.2019.01.108
CR  - Bui DT, 2017, AGR FOREST METEOROL, V233, P32, DOI 10.1016/j.agrformet.2016.11.002
CR  - Bui DT, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8040347
CR  - Eskandari S, 2020, ECOL INDIC, V118, DOI 10.1016/j.ecolind.2020.106720
CR  - Freeman EA, 2008, ECOL MODEL, V217, P48, DOI 10.1016/j.ecolmodel.2008.05.015
CR  - Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
CR  - Ghorbanzadeh O, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020196
CR  - Giglio L., 2009, BIOGEOSCIENCES DISCU, V6, P11577, DOI [10.5194/bgd-6-11577-2009, DOI 10.5194/BGD-6-11577-2009]
CR  - Giglio L, 2013, J GEOPHYS RES-BIOGEO, V118, P317, DOI 10.1002/jgrg.20042
CR  - Ham YG, 2019, NATURE, V573, P568, DOI 10.1038/s41586-019-1559-7
CR  - Hantson S, 2016, BIOGEOSCIENCES, V13, P3359, DOI 10.5194/bg-13-3359-2016
CR  - Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
CR  - Hong HY, 2019, ECOL INDIC, V101, P878, DOI 10.1016/j.ecolind.2019.01.056
CR  - Hong HY, 2018, SCI TOTAL ENVIRON, V630, P1044, DOI 10.1016/j.scitotenv.2018.02.278
CR  - Hu F, 2015, REMOTE SENS-BASEL, V7, P14680, DOI 10.3390/rs71114680
CR  - Jaafari A, 2019, AGR FOREST METEOROL, V266, P198, DOI 10.1016/j.agrformet.2018.12.015
CR  - Jain P., 2020, ENVIRON REV, V10
CR  - Jung M, 2010, NATURE, V467, P951, DOI 10.1038/nature09396
CR  - LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
CR  - Landschutzer P, 2013, BIOGEOSCIENCES, V10, P7793, DOI 10.5194/bg-10-7793-2013
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Leuenberger M, 2018, ENVIRON MODELL SOFTW, V101, P194, DOI 10.1016/j.envsoft.2017.12.019
CR  - Matsuoka D, 2018, PROG EARTH PLANET SC, V5, DOI 10.1186/s40645-018-0245-y
CR  - Muhammad K, 2018, NEUROCOMPUTING, V288, P30, DOI 10.1016/j.neucom.2017.04.083
CR  - Nguyen NT, 2018, ECOL INFORM, V46, P74, DOI 10.1016/j.ecoinf.2018.05.009
CR  - Pourtaghi ZS, 2016, ECOL INDIC, V64, P72, DOI 10.1016/j.ecolind.2015.12.030
CR  - Roteta E, 2019, REMOTE SENS ENVIRON, V222, P1, DOI 10.1016/j.rse.2018.12.011
CR  - Sachdeva S, 2018, NAT HAZARDS, V92, P1399, DOI 10.1007/s11069-018-3256-5
CR  - Satir O, 2016, GEOMAT NAT HAZ RISK, V7, P1645, DOI 10.1080/19475705.2015.1084541
CR  - Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
CR  - Song YJ, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12142246
CR  - Wang Y, 2019, SCI TOTAL ENVIRON, V666, P975, DOI 10.1016/j.scitotenv.2019.02.263
CR  - WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269
CR  - Yamashita R, 2018, INSIGHTS IMAGING, V9, P611, DOI 10.1007/s13244-018-0639-9
CR  - Yu Y, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16692-w
CR  - Zhang C, 2018, ISPRS J PHOTOGRAMM, V140, P133, DOI 10.1016/j.isprsjprs.2017.07.014
CR  - Zhang GL, 2019, INT J DISAST RISK SC, V10, P386, DOI 10.1007/s13753-019-00233-1
CR  - Zheng Z, 2020, ECOL INDIC, V118, DOI 10.1016/j.ecolind.2020.106772
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - AUG
PY  - 2021
VL  - 127
DO  - 10.1016/j.ecolind.2021.107735
AN  - WOS:000659188500005
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  7
Cited Reference Count:  46
ER  -

TY  - JOUR
AU  - Wightman, PH
AU  - Henrichs, DW
AU  - Collier, BA
AU  - Chamberlain, MJ
TI  - Comparison of methods for automated identification of wild turkey gobbles
T2  - WILDLIFE SOCIETY BULLETIN
LA  - English
KW  - acoustic monitoring
KW  - autonomous call recognition
KW  - convolutional neural network
KW  - gobbling
KW  - Meleagris gallopavo
KW  - Raven Pro
KW  - wild turkey
KW  - AUTONOMOUS RECORDING UNITS
KW  - AUDIO RECORDINGS
KW  - BIRDS
KW  - CLASSIFICATION
KW  - SONGBIRDS
KW  - SOUNDS
KW  - COUNTS
AB  - Autonomous recording units (ARUs) allow for collection of extensive acoustic data sets, while reducing costs and time associated with traditional surveys used to determine gobbling chronology of male wild turkeys (Meleagris gallopavo). A challenge with ARUs is efficiently locating and identifying calls of interest, so autonomous call recognition (ACR) software such as Raven Pro have traditionally been used to identify wild turkey gobbles. However, ACR software often produces high false positive detections, requiring substantive time to verify selections as gobbles. We used ARUs across 3 study sites in the southeastern United States to collect 107,580 hours of ambient sound. We developed a convolutional neural network (CNN) to autonomously identify wild turkey gobbles and compared results of our CNN to results gathered using the commercially available program Raven Pro. After processing of ambient sound, the CNN detected 15,793 more gobbles than Raven Pro, and did so with 5,716,718 fewer selections. Collectively, our CNN improved precision from 0.01 to 0.32 relative to Raven Pro, while decreasing the time required for validation from 4,452 hours to 219. We found precision of our CNN varied across ARUs primarily due to differences in occurrence of ambient sounds similar to gobbles. Thus, we recommend that additional site-specific training data should be considered when developing CNNs. Our results suggest that researchers interested in describing gobbling activity by male wild turkeys should consider developing and applying CNNs for automated call recognition.
AD  - Univ Georgia, Warnell Sch Forestry & Nat Resources, Athens, GA 30602 USAAD  - Texas A&M Univ, Dept Oceanog, College Stn, TX 77843 USAAD  - Louisiana State Univ, Sch Renewable Nat Resources, Agr Ctr, Baton Rouge, LA 70803 USAC3  - University System of GeorgiaC3  - University of GeorgiaC3  - Texas A&M University SystemC3  - Texas A&M University College StationC3  - Louisiana State University SystemC3  - Louisiana State UniversityFU  - Warnell School of Forestry and Natural Resources at the University of Georgia; Louisiana State University Agricultural Center; Georgia Department Of Natural Resources; South Carolina Department of Natural Resources
FX  - Warnell School of Forestry and Natural Resources at the University of Georgia; Louisiana State University Agricultural Center; Georgia Department Of Natural Resources; South Carolina Department of Natural Resources
CR  - Abadi M., 2015, TensorFlow: large-scale machine learning on heterogeneous systems
CR  - Acevedo MA, 2006, WILDLIFE SOC B, V34, P211, DOI 10.2193/0091-7648(2006)34[211:UADRSA]2.0.CO;2
CR  - Bardeli R, 2010, PATTERN RECOGN LETT, V31, P1524, DOI 10.1016/j.patrec.2009.09.014
CR  - Bevill W.V., 1973, P SE ASS GAME FISH C, V27, P62
CR  - Bioacoustics Research Program,, 2014, RAVEN PROINTERACTIVE
CR  - Brandes TS, 2008, BIRD CONSERV INT, V18, pS163, DOI 10.1017/S0959270908000415
CR  - Buckland ST, 2006, AUK, V123, P345, DOI 10.1642/0004-8038(2006)123[345:PSFSRM]2.0.CO;2
CR  - Buxton RT, 2016, ECOL EVOL, V6, P4697, DOI 10.1002/ece3.2242
CR  - Campos-Cerqueira M, 2016, METHODS ECOL EVOL, V7, P1340, DOI 10.1111/2041-210X.12599
CR  - Casalena M.J., 2011, P NATL WILD TURKEY S, V10, P41
CR  - Chamberlain MJ, 2018, WILDLIFE SOC B, V42, P632, DOI 10.1002/wsb.932
CR  - Charif R.A., 2010, RAVEN PRO14 USERS MA
CR  - Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
CR  - Colbert DS, 2015, WILDLIFE SOC B, V39, P757, DOI 10.1002/wsb.577
CR  - Drake KL, 2016, WILDLIFE SOC B, V40, P346, DOI 10.1002/wsb.658
CR  - Furnas BJ, 2015, J WILDLIFE MANAGE, V79, P325, DOI 10.1002/jwmg.821
CR  - Gillespie D, 2013, J ACOUST SOC AM, V134, P2427, DOI 10.1121/1.4816555
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Goyette JL, 2011, J FIELD ORNITHOL, V82, P279, DOI 10.1111/j.1557-9263.2011.00331.x
CR  - Healy W.M., 1992, WILD TURKEY BIOL MAN, P46
CR  - Hutto RL, 2009, J FIELD ORNITHOL, V80, P387, DOI 10.1111/j.1557-9263.2009.00245.x
CR  - Isabelle J.L., 2015, P NATL WILD TURKEY S, V11, P249
CR  - Katz J, 2016, BIOACOUSTICS, V25, P177, DOI 10.1080/09524622.2015.1133320
CR  - King DB, 2015, ACS SYM SER, V1214, P1
CR  - Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
CR  - Krakauer AH, 2008, CONDOR, V110, P1, DOI 10.1525/cond.2008.110.1.1
CR  - Leach EC, 2016, EMU, V116, P305, DOI 10.1071/MU15097
CR  - Miller DA, 1997, J WILDLIFE MANAGE, V61, P840, DOI 10.2307/3802192
CR  - Premoli M, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0244636
CR  - Rempel RS, 2005, J FIELD ORNITHOL, V76, P1, DOI 10.1648/0273-8570-76.1.1
CR  - Ruff ZJ, 2021, ECOL INDIC, V124, DOI 10.1016/j.ecolind.2021.107419
CR  - Ruff ZJ, 2020, REMOTE SENS ECOL CON, V6, P79, DOI 10.1002/rse2.125
CR  - Russo D, 2016, ECOL INDIC, V66, P598, DOI 10.1016/j.ecolind.2016.02.036
CR  - Salzberg SL, 1997, DATA MIN KNOWL DISC, V1, P317, DOI 10.1023/A:1009752403260
CR  - Shonfield J, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00974-120114
CR  - Swiston KA, 2009, J FIELD ORNITHOL, V80, P42, DOI 10.1111/j.1557-9263.2009.00204.x
CR  - Tegeler AK, 2012, WILDLIFE SOC B, V36, P21, DOI 10.1002/wsb.112
CR  - Towsey M, 2012, BIOACOUSTICS, V21, P107, DOI 10.1080/09524622.2011.648753
CR  - Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
CR  - Wakefield CT, 2020, J WILDLIFE MANAGE, V84, P448, DOI 10.1002/jwmg.21804
CR  - Wightman PH, 2019, J WILDLIFE MANAGE, V83, P325, DOI 10.1002/jwmg.21600
CR  - Zhong M, 2020, J ACOUST SOC AM, V147, P1834, DOI 10.1121/10.0000921
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - MAR
PY  - 2022
VL  - 46
IS  - 1
DO  - 10.1002/wsb.1246
AN  - WOS:000754625000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  42
ER  -

TY  - CPAPER
AU  - Nguyen, H
AU  - Maclagan, SJ
AU  - Nguyen, TD
AU  - Nguyen, T
AU  - Flemons, P
AU  - Andrews, K
AU  - Ritchie, EG
AU  - Phung, D
A1  - IEEE
TI  - Animal Recognition and Identification with Deep Convolutional Neural Networks for Automated Wildlife Monitoring
T2  - 2017 IEEE INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS (DSAA)
LA  - English
CP  - 4th IEEE / ACM / ASA International Conference on Data Science and Advanced Analytics (DSAA)
KW  - deep learning
KW  - convolutional neural networks
KW  - large scale image classification
KW  - animal recognition
KW  - wildlife monitoring
KW  - citizen science
KW  - CITIZEN SCIENCE
KW  - TOOL
AB  - Efficient and reliable monitoring of wild animals in their natural habitats is essential to inform conservation and management decisions. Automatic covert cameras or "camera traps" are being an increasingly popular tool for wildlife monitoring due to their effectiveness and reliability in collecting data of wildlife unobtrusively, continuously and in large volume. However, processing such a large volume of images and videos captured from camera traps manually is extremely expensive, time-consuming and also monotonous. This presents a major obstacle to scientists and ecologists to monitor wildlife in an open environment. Leveraging on recent advances in deep learning techniques in computer vision, we propose in this paper a framework to build automated animal recognition in the wild, aiming at an automated wildlife monitoring system. In particular, we use a single-labeled dataset from Wildlife Spotter project, done by citizen scientists, and the state-of-the-art deep convolutional neural network architectures, to train a computational system capable of filtering animal images and identifying species automatically. Our experimental results achieved an accuracy at 96.6% for the task of detecting images containing animal, and 90.4% for identifying the three most common species among the set of images of wild animals taken in South-central Victoria, Australia, demonstrating the feasibility of building fully automated wildlife observation. This, in turn, can therefore speed up research findings, construct more efficient citizen science based monitoring systems and subsequent management decisions, having the potential to make significant impacts to the world of ecology and trap camera images analysis.
AD  - Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, AustraliaAD  - Deakin Univ, Ctr Integrat Ecol, Burwood, AustraliaAD  - Australian Museum Res Inst, Sydney, NSW, AustraliaAD  - ABC Radio Natl, Ultimo, NSW, AustraliaC3  - Deakin UniversityC3  - Deakin UniversityC3  - Australian MuseumFU  - Telstra-Deakin Centre of Excellence in Big Data and Machine Learning
FX  - This work is partially supported by the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning.
CR  - Abadi M, 2016, ABS160304467
CR  - Bishop C. M., 2006, MACH LEARN, V128, P1, DOI DOI 10.1002/9780471740360.EBS0904
CR  - Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
CR  - Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Chollet F., 2015, KERAS
CR  - Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
CR  - Collobert R., 2008, P 25 INT C MACH LEAR, P160, DOI 10.1145/1390156.1390177
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
CR  - Fei-Fei L, 2005, PROC CVPR IEEE, P524
CR  - Gehring J., 2016, ARXIV PREPRINT ARXIV
CR  - Gehring J., 2017, ARXIV E PRINTS
CR  - Godley B. J., 2008, Endangered Species Research, V4, P3, DOI 10.3354/esr00060
CR  - Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
CR  - Gomez A., 2016, ARXIV160306169
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - He Z., 2010, ARXIV PREPRINT ARXIV
CR  - Hulbert IAR, 2001, J APPL ECOL, V38, P869, DOI 10.1046/j.1365-2664.2001.00624.x
CR  - Irwin A, 1995, CITIZEN SCI STUDY PE
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
CR  - O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
CR  - Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
CR  - Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Szewczyk R., 2004, P 2 INT C EMB NETW S, P214, DOI DOI 10.1145/1031495.1031521
CR  - Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
CR  - Vitousek PM, 1997, SCIENCE, V277, P494, DOI 10.1126/science.277.5325.494
CR  - Wang J., 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5540018
CR  - White G.C., 2012, ANAL WILDLIFE RADIO
CR  - Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2017
SP  - 40
EP  - 49
DO  - 10.1109/DSAA.2017.31
AN  - WOS:000454622300005
N1  - Times Cited in Web of Science Core Collection:  49
Total Times Cited:  51
Cited Reference Count:  37
ER  -

TY  - CPAPER
AU  - Kellenberger, B
AU  - Volpi, M
AU  - Tuia, D
A1  - IEEE
TI  - FAST ANIMAL DETECTION IN UAV IMAGES USING CONVOLUTIONAL NEURAL NETWORKS
T2  - 2017 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM (IGARSS)
LA  - English
CP  - IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
KW  - FEATURES
AB  - Illegal wildlife poaching poses one severe threat to the environment. Measures to stem poaching have only been with limited success, mainly due to efforts required to keep track of wildlife stock and animal tracking. Recent developments in remote sensing have led to low-cost Unmanned Aerial Vehicles (UAVs), facilitating quick and repeated image acquisitions over vast areas. In parallel, progress in object detection in computer vision yielded unprecedented performance improvements, partially attributable to algorithms like Convolutional Neural Networks (CNNs). We present an object detection method tailored to detect large animals in UAV images. We achieve a substantial increase in precision over a robust state-of-the-art model on a dataset acquired over the Kuzikus wildlife reserve park in Namibia. Furthermore, our model processes data at over 72 images per second, as opposed 3 for the baseline, allowing for real-time applications.
AD  - Univ Zurich, MultiModal Remote Sensing, Zurich, SwitzerlandC3  - League of European Research Universities - LERUC3  - University of ZurichFU  - SNSF [PP00P2_150593]
FX  - This work has been supported by the SNSF grant PP00P2_150593. The authors would like to acknowledge the SAVMAP project and Micromappers for providing the data and ground truth used in this work.
CR  - Biggs D, 2013, SCIENCE, V339, P1038, DOI 10.1126/science.1229998
CR  - Dalal N., 2005, IEEE COMPUTER SOC C, P886, DOI 10.1109/CVPR.2005.177
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Girshick R., 2015, ICCV, P1440, DOI DOI 10.1109/ICCV.2015.169
CR  - GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
CR  - Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
CR  - Ofli F, 2016, BIG DATA-US, V4, P47, DOI 10.1089/big.2014.0064
CR  - Ramanan Deva, 2010, EUR C COMP VIS, V6314, P1
CR  - Redmon J., 2016, YOU ONLY LOOK ONE UN
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Salberg AB, 2015, INT GEOSCI REMOTE SE, P1893, DOI 10.1109/IGARSS.2015.7326163
CR  - Torralba A, 2004, PROC CVPR IEEE, P762
CR  - Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
CR  - Wittemyer G, 2014, P NATL ACAD SCI USA, V111, P13117, DOI 10.1073/pnas.1403984111
CR  - Zhang F., 2016, IEEE T GEOSCIENCE RE, V54, P1, DOI [10.1109/LGRS.2016.2519241, DOI 10.1109/LGRS.2016.2519241]
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2017
SP  - 866
EP  - 869
AN  - WOS:000426954601006
N1  - Times Cited in Web of Science Core Collection:  18
Total Times Cited:  18
Cited Reference Count:  18
ER  -

TY  - CPAPER
AU  - Curtin, BH
AU  - Matthews, SJ
ED  - Chakrabarti, S
ED  - Saha, HN
TI  - Deep Learning for Inexpensive Image Classification of Wildlife on the Raspberry Pi
T2  - 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON)
LA  - English
CP  - IEEE 10th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference (UEMCON)
KW  - Raspberry Pi
KW  - single board computer
KW  - deep learning
KW  - convolutional neural network
KW  - TensorFlow
KW  - Keras
KW  - SYSTEM
AB  - Animal conservationists need unobtrusive methods of observing and studying wildlife in remote areas. Many commercial options for wildlife observation are expensive, obtrusive, or sub-optimal in remote environments. In this paper, we explore the viability of a Raspberry Pi-based camera system augmented with a deep learning image recognition model for detecting wildlife of interest. Unlike traditional sensor nodes that would have to transmit every captured image, localized image recognition enables only pictures of desired animals to be transferred to the user. For the purposes of this study, we use TensorFlow and Keras to create a convolutional neural network that runs on a Raspberry Pi 3B+. We trained the model on nearly 3,600 images gathered from publicly available image databases that are split into three classes. Our experiments suggest that our system can detect snow leopards with between 74 percent and 97 percent accuracy. We believe that our results show the viability of employing deep learning image recognition models on the Raspberry Pi to create an inexpensive system to observe wildlife.
AD  - US Mil Acad, Dept Elect Engn & Comp Sci, West Point, NY 10996 USAC3  - United States Department of DefenseC3  - United States ArmyC3  - United States Military AcademyFU  - U.S. Army Futures Command, CCDC Armaments
FX  - Funding is provided by U.S. Army Futures Command, CCDC Armaments
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Alejos A., 2018, P 5 ANN S BOOTC HOT
CR  - Ben Thabet A, 2015, I C SCI TECH AUTO CO, P373, DOI 10.1109/STA.2015.7505106
CR  - Bilgin E, 2016, 2016 IEEE LONG ISLAND SYSTEMS, APPLICATIONS AND TECHNOLOGY CONFERENCE (LISAT)
CR  - Byrne JR, 2015, INT CONF COMP SCI ED, P267, DOI 10.1109/ICCSE.2015.7250254
CR  - Caldas-Morgan M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130297
CR  - Caya M. V, 2018 IEEE 10 INT C H, P1
CR  - Chollet, 2015, KERAS
CR  - Cruz FRG, 2016, PROCEEDINGS OF THE 2016 IEEE REGION 10 CONFERENCE (TENCON), P2126, DOI 10.1109/TENCON.2016.7848401
CR  - Curtin B. H., 2019, P 6 ANN S HOT TOP SC
CR  - D.-K. Electroncics, 2019, PIM CAM008
CR  - Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
CR  - de Oliveira DC, 2016, 2016 IEEE 19TH INTERNATIONAL SYMPOSIUM ON REAL-TIME DISTRIBUTED COMPUTING (ISORC 2016), P27, DOI 10.1109/ISORC.2016.14
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Ditmer MA, 2015, CURR BIOL, V25, P2278, DOI 10.1016/j.cub.2015.07.024
CR  - Gray D., 2007, P IEEE INT WORKSHOP, V3
CR  - Gremillet D., 2012, Open Journal of Ecology, V2, P49, DOI 10.4236/oje.2012.22006
CR  - Kawash J., 2016, SIGCSE, V16, P498
CR  - Khandelwal R, 2019, BUILDING POWERFUL IM
CR  - Kluyver T, 2016, POSITIONING AND POWER IN ACADEMIC PUBLISHING: PLAYERS, AGENTS AND AGENDAS, P87, DOI 10.3233/978-1-61499-649-1-87
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Kunik Z, 2017, SIG P ALGO ARCH ARR, P263, DOI 10.23919/SPA.2017.8166876
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Maksimovi M., 2014, DES ISSUES, V3
CR  - Matthews S. J., 2018, MILITARY APPL DATA A, P63
CR  - Matthews SJ, 2018, SIGCSE'18: PROCEEDINGS OF THE 49TH ACM TECHNICAL SYMPOSIUM ON COMPUTER SCIENCE EDUCATION, P92, DOI 10.1145/3159450.3159558
CR  - Njoroge P, 2016, COST EFFECTIVE ACOUS
CR  - P3 International, 2018, KILL WATT MET
CR  - Sajjad M., 2017, FUTURE GENER COMP SY
CR  - Silva C. F., 2017, 2017 IEEE LAT AM C C, P1
CR  - Suchitra, 2016, 2016 3rd International Conference on Signal Processing and Integrated Networks (SPIN), P666, DOI 10.1109/SPIN.2016.7566780
CR  - Wynsberghe A, 2018, SCI ENG ETHICS, V24, P1777, DOI 10.1007/s11948-017-9990-3
CR  - Whytock RC, 2017, METHODS ECOL EVOL, V8, P308, DOI 10.1111/2041-210X.12678
CR  - Zeiler M.D., 2012, ARXIV ARXIV 12125701
CR  - Zhong XY, 2016, ELECTRONICS-SWITZ, V5, DOI 10.3390/electronics5030056
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2019
SP  - 82
EP  - 87
AN  - WOS:000652198600012
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  8
Cited Reference Count:  35
ER  -

TY  - JOUR
AU  - Piazza, G
AU  - Valsecchi, C
AU  - Sottocornola, G
TI  - Deep Learning Applied to SEM Images for Supporting Marine Coralline Algae Classification
T2  - DIVERSITY-BASEL
LA  - English
KW  - machine learning
KW  - CNNs
KW  - SEM images
KW  - coralline algae
KW  - taxonomy
KW  - ultrastructure
KW  - Lithophyllum pseudoracemus
KW  - CONVOLUTIONAL NEURAL-NETWORKS
KW  - PONTIAN ISLANDS
KW  - RHODOPHYTA
KW  - TERRACE
KW  - PALEOECOLOGY
KW  - ASSEMBLAGES
KW  - DIVERSITY
KW  - ATLANTIC
KW  - FACIES
AB  - The classification of coralline algae commonly relies on the morphology of cells and reproductive structures, along with thallus organization, observed through Scanning Electron Microscopy (SEM). Nevertheless, species identification based on morphology often leads to uncertainty, due to their general plasticity. Evolutionary and environmental studies featured coralline algae for their ecological significance in both recent and past Oceans and need to rely on robust taxonomy. Research efforts towards new putative diagnostic tools have recently been focused on cell wall ultrastructure. In this work, we explored a new classification tool for coralline algae, using fine-tuning pretrained Convolutional Neural Networks (CNNs) on SEM images paired to morphological categories, including cell wall ultrastructure. We considered four common Mediterranean species, classified at genus and at the species level (Lithothamnion corallioides, Mesophyllum philippii, Lithophyllum racemus, Lithophyllum pseudoracemus). Our model produced promising results in terms of image classification accuracy given the constraint of a limited dataset and was tested for the identification of two ambiguous samples referred to as L. cf. racemus. Overall, explanatory image analyses suggest a high diagnostic value of calcification patterns, which significantly contributed to class predictions. Thus, CNNs proved to be a valid support to the morphological approach to taxonomy in coralline algae.
AD  - Univ Milano Bicocca, Dept Earth & Environm Sci, I-20126 Milan, ItalyAD  - Free Univ Bozen Bolzano, Dept Informat Sci & Technol, I-39100 Bolzano, ItalyC3  - University of Milano-BicoccaC3  - Free University of Bozen-BolzanoCR  - Abadi M, 2016, ABS160304467
CR  - Abdalla A, 2019, COMPUT ELECTRON AGR, V167, DOI 10.1016/j.compag.2019.105091
CR  - ADEY WH, 1973, GEOL SOC AM BULL, V84, P883, DOI 10.1130/0016-7606(1973)84<883:CCAARI>2.0.CO;2
CR  - ADEY WH, 1970, BOT MAR, V13, P100, DOI 10.1515/botm.1970.13.2.100
CR  - Agarap A.F, 2018, DEEP LEARNING USING
CR  - Auer G, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aay2126
CR  - Ballesteros E, 2006, OCEANOGR MAR BIOL, V44, P123
CR  - Basso D, 1996, FACIES, V35, P275, DOI 10.1007/BF02536965
CR  - Basso D, 1998, PALAEOGEOGR PALAEOCL, V137, P173, DOI 10.1016/S0031-0182(97)00099-0
CR  - Basso D., 1994, RIV ITAL PALEONTOL S, V100, P575, DOI [10.13130/2039-4942/8602, DOI 10.13130/2039-4942/8602]
CR  - Basso D, 2007, NEUES JAHRB GEOL P-A, V244, P173, DOI 10.1127/0077-7749/2007/0244-0173
CR  - Basso D, 2006, GEOL SOC SPEC PUBL, V255, P23, DOI 10.1144/GSL.SP.2006.255.01.03
CR  - Basso D, 2017, COAST RES LIBR, V15, P281, DOI 10.1007/978-3-319-29315-8_11
CR  - Basso D, 2016, AQUAT CONSERV, V26, P549, DOI 10.1002/aqc.2586
CR  - Basso D, 2012, GEODIVERSITAS, V34, P13, DOI 10.5252/g2012n1a2
CR  - Basso Daniela, 1995, Rivista Italiana di Paleontologia e Stratigrafia, V101, P349
CR  - Bracchi VA, 2017, CONT SHELF RES, V144, P10, DOI 10.1016/j.csr.2017.06.005
CR  - Bracchi VA, 2021, BIOGEOSCIENCES, V18, P6061, DOI 10.5194/bg-18-6061-2021
CR  - Bracchi VA, 2016, PALAEOGEOGR PALAEOCL, V454, P101, DOI 10.1016/j.palaeo.2016.04.014
CR  - Bracchi VA, 2014, PALAEOGEOGR PALAEOCL, V414, P296, DOI 10.1016/j.palaeo.2014.09.016
CR  - Cabioch J, 1998, PHYCOLOGIA, V37, P208, DOI 10.2216/i0031-8884-37-3-208.1
CR  - Caragnano A, 2020, PHYCOLOGIA, V59, P584, DOI 10.1080/00318884.2020.1829348
CR  - Carro B, 2014, PHYTOTAXA, V190, P176, DOI 10.11646/phytotaxa.190.1.12
CR  - Cebrian E, 2000, OCEANOL ACTA, V23, P311, DOI 10.1016/S0399-1784(00)00131-6
CR  - Chamberlain, 1994, SEAWEEDS BRIT ISLES
CR  - Coletti G, 2018, GEOBIOS-LYON, V51, P15, DOI 10.1016/j.geobios.2017.12.002
CR  - Darrenougue N, 2014, QUATERNARY SCI REV, V93, P34, DOI 10.1016/j.quascirev.2014.03.005
CR  - De Jode A, 2019, MOL PHYLOGENET EVOL, V137, P104, DOI 10.1016/j.ympev.2019.04.005
CR  - De Lima RP, 2020, PALAIOS, V35, P391, DOI 10.2110/palo.2019.102
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Deng YX, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00480-x
CR  - FLAJS G, 1977, P225
CR  - Foster MS, 2001, J PHYCOL, V37, P659, DOI 10.1046/j.1529-8817.2001.00195.x
CR  - Garbary J.D., 1978, MODERN APPROACHES TA, P205
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Hecht-Nielsen R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P593, DOI 10.1109/IJCNN.1989.118638
CR  - Hsiang AY, 2019, PALEOCEANOGR PALEOCL, V34, P1157, DOI 10.1029/2019PA003612
CR  - 박진현, 2019, [Journal of the Korea Institute Of Information and Communication Engineering, 한국정보통신학회논문지], V23, P39, DOI 10.6109/jkiice.2019.23.1.39
CR  - Kato A, 2011, J PHYCOL, V47, P662, DOI 10.1111/j.1529-8817.2011.00996.x
CR  - Kiel S., 2021, ASSESSING BIVALVE PH, DOI [10.1101/2021.04.08.438943, DOI 10.1101/2021.04.08.438943]
CR  - Kotikalapudi R., CONTRIBUTORS KERAS V
CR  - Liu Q, 2017, LECT NOTES COMPUT SC, V10361, P69, DOI 10.1007/978-3-319-63309-1_7
CR  - Liu XK, 2020, SEDIMENT GEOL, V410, DOI 10.1016/j.sedgeo.2020.105790
CR  - Martin S, 2006, AQUAT BOT, V85, P121, DOI 10.1016/j.aquabot.2006.02.005
CR  - Mitra R, 2019, MAR MICROPALEONTOL, V147, P16, DOI 10.1016/j.marmicro.2019.01.005
CR  - Modarres MH, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13565-z
CR  - Nash MC, 2017, J PHYCOL, V53, P970, DOI 10.1111/jpy.12559
CR  - Pena V, 2015, EUR J PHYCOL, V50, P20, DOI 10.1080/09670262.2014.981294
CR  - Pezzolesi L, 2019, J PHYCOL, V55, P473, DOI 10.1111/jpy.12837
CR  - POTIN P, 1990, HYDROBIOLOGIA, V204, P263, DOI 10.1007/BF00040243
CR  - Rathi D, 2017, 2017 NINTH INTERNATIONAL CONFERENCE ON ADVANCES IN PATTERN RECOGNITION (ICAPR), P344
CR  - Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
CR  - Rosler A, 2016, J PHYCOL, V52, P412, DOI 10.1111/jpy.12404
CR  - Savini A, 2012, GEODIVERSITAS, V34, P77, DOI 10.5252/g2012n1a5
CR  - Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
CR  - Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
CR  - Simonyan K., 2013, INT C LEARN REPR ICL
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Tan P.-N., 2018, INTRO DATA MINING, V2 nd
CR  - Wang LM, 2017, SCI REP-UK, V7, DOI 10.1038/srep41545
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - DEC
PY  - 2021
VL  - 13
IS  - 12
DO  - 10.3390/d13120640
AN  - WOS:000736686100001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  60
ER  -

TY  - JOUR
AU  - Lin, CW
AU  - Hong, SD
AU  - Lin, MX
AU  - Huang, XP
AU  - Liu, JF
TI  - Bird posture recognition based on target keypoints estimation in dual-task convolutional neural networks
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Bird posture recognition
KW  - Keypoint detection
KW  - Multi-task CNNs
KW  - MULTI-LOSS
KW  - IMAGE
KW  - MODEL
AB  - Bird behavior reflects its healthy and the habitat condition in which the bird's posture recognition is the basis of bird behavior research. This paper proposes an end-to-end bird posture recognition based on target keypoints estimation in the dual-task network. The network composes two sub-networks, keypoint feature extraction network and local feature extraction network. In the keypoint feature extraction network, we take the high resolution network (HRNet) as the backbone to detect the bird's keypoints in the high-resolution branch and extract the global features from the low-resolution branch for posture recognition. We design the components generation module in the local feature extraction network, which takes detected keypoints into the clustering algorithm to generate the bird's components. We then utilize the components into convolutional neural networks (CNNs) to extract the local features for posture recognition. Finally, we fuse the global and local features to execute the bird's posture recognition. Our main contributions are fourfold: (1) We have been modified the first point of contribution. We take the bird's keypoints, which have never been used in the bird's posture recognition, to estimate the bird's posture and achieved good results. (2) We propose an end-to-end dual-task network for bird's keypoints detection and posture recognition. (3) The components generation module utilizes the bird's keypoints to crop the bird components and generates the local feature for bird posture recognition with a shallow network. (4) Creating a bird posture image dataset named IMLab-P8-2020 containing eight bird postures. We experimentally evaluated our proposed net on self-collected data set and verified that the proposed net yields better performance and greater accuracy relative to well-known counterparts concerning various metrics.
AD  - Fujian Agr & Forestry Univ, Coll Comp & Informat Sci, Fuzhou 350002, Peoples R ChinaAD  - Fujian Agr & Forestry Univ, Coll Forestry, Fuzhou 350002, Peoples R ChinaAD  - Fujian Agr & Forestry Univ, Forestry Postdoctoral Stn, Fuzhou 350002, Peoples R ChinaAD  - Key Lab Fujian Univ Ecol & Resource Stat, Fuzhou 350002, Peoples R ChinaAD  - Cross Strait Nat Reserve Res Ctr, Fuzhou 350002, Peoples R ChinaAD  - Putian Univ, Coll New Engn Ind, Putian 351100, Peoples R ChinaC3  - Fujian Agriculture & Forestry UniversityC3  - Fujian Agriculture & Forestry UniversityC3  - Fujian Agriculture & Forestry UniversityC3  - Putian UniversityFU  - China Postdoctoral Science Foundation [2018M632565]; Channel Postdoctoral Exchange Funding Scheme; Youth Program of Humanities and Social Sciences Foundation, Ministry of Education of China [18YJCZH093]
FX  - This research was funded by the China Postdoctoral Science Foun-dation under Grant 2018M632565, the Channel Postdoctoral Exchange Funding Scheme, and the Youth Program of Humanities and Social Sciences Foundation, Ministry of Education of China under Grant 18YJCZH093.
CR  - Alaniz AJ, 2021, ECOL INDIC, V126, DOI 10.1016/j.ecolind.2021.107634
CR  - Armanfard N, 2016, IEEE T PATTERN ANAL, V38, P1217, DOI 10.1109/TPAMI.2015.2478471
CR  - Armstrong JA, 2019, SOL PHYS, V294, DOI 10.1007/s11207-019-1473-z
CR  - Artacho B, 2021, IEEE T PATTERN ANAL
CR  - Bech-Hansen M, 2020, BIRD CONSERV INT, V30, P169, DOI 10.1017/S0959270919000364
CR  - Branson S., 2014, BMVC 2014
CR  - Cai X, 2020, IEEE ACCESS, V8, P44493, DOI 10.1109/ACCESS.2020.2978249
CR  - Cao JK, 2019, IEEE I CONF COMP VIS, P9497, DOI 10.1109/ICCV.2019.00959
CR  - Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
CR  - Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
CR  - Duan L, 2017, IEEE IMAGE PROC, P2836, DOI 10.1109/ICIP.2017.8296800
CR  - Fang C, 2021, COMPUT ELECTRON AGR, V180, DOI 10.1016/j.compag.2020.105863
CR  - Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
CR  - Gavali P., 2020, 2020 INT C EM TRENDS, P1
CR  - Gill F.B., 1995, ORNITHOLOGY
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - Huang YP, 2019, IEEE ACCESS, V7, P66980, DOI 10.1109/ACCESS.2019.2918274
CR  - Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
CR  - Jiang-Jiang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10093, DOI 10.1109/CVPR42600.2020.01011
CR  - Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
CR  - Kim ST, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186497
CR  - Kreiss S, 2019, PROC CVPR IEEE, P11969, DOI 10.1109/CVPR.2019.01225
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Li X., 2019, ARXIV PREPRINT ARXIV
CR  - Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
CR  - Li XY, 2019, COMPUT ELECTRON AGR, V164, DOI 10.1016/j.compag.2019.104885
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Lin Zhiwei, 2020, Scientia Silvae Sinicae, V56, P133, DOI 10.11707/j.1001-7488.20200113
CR  - Mathis A, 2018, NAT NEUROSCI, V21, P1281, DOI 10.1038/s41593-018-0209-y
CR  - Meena SD, 2019, IEEE ACCESS, V7, P151783, DOI 10.1109/ACCESS.2019.2947717
CR  - Newell A., 2017, ADV NEURAL INFORM PR, V30
CR  - Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
CR  - Qiang BH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19030718
CR  - Rodriguez I., 2018, HONEYBEE DETECTION P
CR  - Sekercioglu CH, 2006, TRENDS ECOL EVOL, V21, P464, DOI 10.1016/j.tree.2006.05.007
CR  - Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
CR  - Simonyan K, 2015, 3 INT C LEARN REPR I, P1
CR  - Sun X, 2021, ISPRS J PHOTOGRAMM, V173, P50, DOI 10.1016/j.isprsjprs.2020.12.015
CR  - Sur M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0174785
CR  - Tian Y, 2019, NEUROCOMPUTING, V347, P13, DOI 10.1016/j.neucom.2019.01.104
CR  - Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
CR  - Wang B, 2019, MED PHYS, V46, P1707, DOI 10.1002/mp.13416
CR  - Wang Jingdong, 2020, IEEE T PATTERN ANAL, DOI [10.1109/tpami.2020.2983686, DOI 10.1109/TPAMI.2020.2983686, 10.1109/TPAMI.2020.2983686]
CR  - Welinder Peter, 2011, CNSTR2011001 CALTECH
CR  - Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
CR  - Wu D, 2019, NEUROCOMPUTING, V324, P69, DOI 10.1016/j.neucom.2018.03.073
CR  - Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
CR  - Xie M., 2014, E SCI TECHNOLOGY APP, V5, P87
CR  - Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
CR  - Xu CY, 2016, IEEE T CIRC SYST VID, V26, P2273, DOI 10.1109/TCSVT.2015.2477937
CR  - Zhang H., 2020, ARXIV PREPRINT ARXIV
CR  - Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
CR  - Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - FEB
PY  - 2022
VL  - 135
DO  - 10.1016/j.ecolind.2021.108506
AN  - WOS:000761393800007
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Bogucki, R
AU  - Cygan, M
AU  - Khan, CB
AU  - Klimek, M
AU  - Milczek, JK
AU  - Mucha, M
TI  - Applying deep learning to right whale photo identification
T2  - CONSERVATION BIOLOGY
LA  - English
KW  - algorithm
KW  - automated image recognition
KW  - computer vision
KW  - convolutional neural networks
KW  - Kaggle competition
KW  - machine learning
KW  - photo identification
AB  - Photo identification is an important tool for estimating abundance and monitoring population trends over time. However, manually matching photographs to known individuals is time-consuming. Motivated by recent developments in image recognition, we hosted a data science challenge on the crowdsourcing platform Kaggle to automate the identification of endangered North Atlantic right whales (Eubalaena glacialis). The winning solution automatically identified individual whales with 87% accuracy with a series of convolutional neural networks to identify the region of interest on an image, rotate, crop, and create standardized photographs of uniform size and orientation and then identify the correct individual whale from these passport-like photographs. Recent advances in deep learning coupled with this fully automated workflow have yielded impressive results and have the potential to revolutionize traditional methods for the collection of data on the abundance and distribution of wild populations. Presenting these results to a broad audience should further bridge the gap between the data science and conservation science communities.
AD  - Deepsense Ai, Krancowa 5, PL-02493 Warsaw, PolandAD  - Univ Warsaw, Inst Informat, Banacha 2, PL-02097 Warsaw, PolandAD  - NOAA, Natl Ocean & Atmospher Adm, Woods Hole, MA 02543 USAC3  - University of WarsawC3  - National Oceanic Atmospheric Admin (NOAA) - USACR  - Agler B.A., 1990, Reports of the International Whaling Commission Special Issue, P349
CR  - Aizenberg I., 2000, MULTIVALUED UNIVERSA
CR  - [Anonymous], 1990, [No title captured]
CR  - Arzoumanian Z, 2005, J APPL ECOL, V42, P999, DOI 10.1111/j.1365-2664.2005.01117.x
CR  - Beijbom O, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130312
CR  - Bogucki R, 2016, WHICH WHALE IS IT AN
CR  - Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
CR  - Clapham PJ, 2001, 0106 US DEP COMM NE
CR  - Cole TVN, 2007, 0702 US DEP COMM NE
CR  - Crall JP, 2013, P IEEE WORKSH APPL C
CR  - EBERHARDT LL, 1969, J WILDLIFE MANAGE, V33, P28, DOI 10.2307/3799647
CR  - Flukebook, 2018, FLUK HELPS YOU STUD
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Hamilton PK, 1999, CATALOG IDENTIFIED R
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Henry AG, 2016, 1610 US DEP COMM NE
CR  - Hiby Lex, 2001, Journal of Cetacean Research and Management Special Issue, V2, P291
CR  - Katona S.K., 1981, Polar Record, V20, P439
CR  - Khan C, 2016, 1601 US DEP COMM NE
CR  - Kniest E, 2010, MAR MAMMAL SCI, V26, P744, DOI 10.1111/j.1748-7692.2009.00368.x
CR  - Krause S. J., 1986, Rapid Thermal Processing, P145
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
CR  - New England Aquarium, 2017, N ATL RIGHT WHAL CAT
CR  - Ng A. Y, 2009, P 26 INT C MACH LEAR
CR  - Oh KS, 2004, PATTERN RECOGN, V37, P1311, DOI 10.1016/j.patcog.2004.01.013
CR  - OTIS DL, 1978, WILDLIFE MONOGR, P1
CR  - Pace RM, 2017, ECOL EVOL, V7, P8730, DOI 10.1002/ece3.3406
CR  - PAYNE R, 1976, NATL GEOGR, V149, P322
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Russell S. J., 1995, ARTIFICIAL INTELLIGE
CR  - Seber George Arthur Frederick, 1982, ESTIMATION ANIMAL AB
CR  - Silber GK, 2015, PEERJ, V3, DOI 10.7717/peerj.866
CR  - Silber GK, 2014, PEERJ, V2, DOI 10.7717/peerj.399
CR  - Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
CR  - Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
CR  - Van der Hoop JM, 2013, CONSERV BIOL, V27, P121, DOI 10.1111/j.1523-1739.2012.01934.x
CR  - Vanderlaan Angelia S. M., 2008, Endangered Species Research, V4, P283, DOI 10.3354/esr00083
CR  - Waring GT, 2016, 238 NMFS NE NOAA
CR  - Wiley D, 2013, P MARINE SAFETY  FAL, V2013, P10
CR  - Xing FY, 2018, IEEE T NEUR NET LEAR, V29, P4550, DOI 10.1109/TNNLS.2017.2766168
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - JUN
PY  - 2019
VL  - 33
IS  - 3
SP  - 676
EP  - 684
DO  - 10.1111/cobi.13226
AN  - WOS:000467327300019
N1  - Times Cited in Web of Science Core Collection:  19
Total Times Cited:  21
Cited Reference Count:  43
ER  -

TY  - CPAPER
AU  - Dai, W
AU  - Wang, HP
AU  - Song, YL
AU  - Xin, YW
A1  - IEEE
TI  - Performance Optimization of Wildlife Recognition with Distributed Learning in Ecological Surveillance
T2  - 2019 9TH IEEE ANNUAL INTERNATIONAL CONFERENCE ON CYBER TECHNOLOGY IN AUTOMATION, CONTROL, AND INTELLIGENT SYSTEMS (IEEE-CYBER 2019)
LA  - English
CP  - 9th IEEE Annual International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (IEEE-CYBER)
KW  - distributed training
KW  - convolutional neural network
KW  - wildlife recognition
AB  - Recently, deep learning technology has been widely used in ecological environment monitoring, but there are still huge challenges in specific tasks. For example, data sets have increased over the past few years, and in a CPU environment, training is quite time consuming. In order to solve these problems, we bring up a novel and effective method to improve training speed. In this paper, we make use of multi-GPUs to achieve distributed parallel acceleration Small and shallow networks are not suitable for distributed training, because the computation of each parameter in this network is much higher than that of multi-layer perception or automatic encoder architecture, we use convolutional neural network with parameter sharing as the training network. In this paper, the main adopted method is to compare the performance of training wildlife recognition in single GPU and multi-GPUs environments by using distributed deep learning framework TensorFlow. The experimental results show that multi-GPUs which adopt distributed architecture can significantly accelerate training time consumption than single GPU. The results of this experiment also provides strong support for our follow-up work.
AD  - Nankai Univ, Coll Artificial Intelligence, Tianjin 300353, Peoples R ChinaAD  - Tianjin Key Lab Intelligent Robot, Tianjin 300353, Peoples R ChinaAD  - Nankai Univ, Res Lab Machine Learning & Pervas Comp, Tianjin 300353, Peoples R ChinaC3  - Nankai UniversityC3  - Nankai UniversityFU  - National Natural Science Foundation of China [61375087]; Key Program of Natural Science Foundation of Tianjin [15JCZDJC31200]; Technology Research and Development Program of Tianjin [18ZXZNGX00340]
FX  - This work is supported by National Natural Science Foundation of China (Grant No. 61375087), Key Program of Natural Science Foundation of Tianjin (Grant No. 15JCZDJC31200) and Technology Research and Development Program of Tianjin (Grants No. 18ZXZNGX00340).
CR  - Agarwal A., 2011, ADV NEURAL INFORM PR, P873
CR  - Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
CR  - Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
CR  - CENA G, 2010, IEEE INT C EMERG
CR  - Chen J., 2016, ARXIV160400981
CR  - Ciresan D. Claudiu, 2010, ARXIV10030358
CR  - Coates A., 2011, P 14 INT C ART INT S, P215
CR  - Collobert R., 2008, P 25 INT C MACH LEAR, P160, DOI 10.1145/1390156.1390177
CR  - Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090
CR  - Franco H., 1994, COMPUTER SPEECH LANG, V8
CR  - GiraldoZuluaga J.-H., 2017 INT C TOOLS ART
CR  - Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
CR  - Kingsbury B., 2012, INTERSPEECH
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Langford J., 2009, ARXIV09110491
CR  - RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
CR  - Sainath TN, 2013, IEEE T AUDIO SPEECH, V21, P2267, DOI 10.1109/TASL.2013.2284378
CR  - Vesely K, 2010, LECT NOTES ARTIF INT, V6231, P439, DOI 10.1007/978-3-642-15760-8_56
CR  - Wiesler S., 2013, INTERSPEECH
CR  - Xue J, 2013, INTERSPEECH, P2364
CR  - Zhang C, 2016, ARXIV PREPRINT ARXIV
CR  - Zhang X., ICASSP 2014
CR  - Zhou P., 2013, ICASSP
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2019
SP  - 347
EP  - 352
AN  - WOS:000569550300060
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  23
ER  -

TY  - JOUR
AU  - Younis, S
AU  - Schmidt, M
AU  - Weiland, C
AU  - Dressler, S
AU  - Seeger, B
AU  - Hickler, T
TI  - Detection and annotation of plant organs from digitised herbarium scans using deep learning
T2  - BIODIVERSITY DATA JOURNAL
LA  - English
KW  - herbarium specimens
KW  - plant organ detection
KW  - deep learning
KW  - convolutional neural networks
KW  - object detection and localisation
KW  - image annotation
KW  - digitisation
KW  - SPECIMENS
AB  - As herbarium specimens are increasingly becoming digitised and accessible in online repositories, advanced computer vision techniques are being used to extract information from them. The presence of certain plant organs on herbarium sheets is useful information in various scientific contexts and automatic recognition of these organs will help mobilise such information. In our study, we use deep learning to detect plant organs on digitised herbarium specimens with Faster R-CNN. For our experiment, we manually annotated hundreds of herbarium scans with thousands of bounding boxes for six types of plant organs and used them for training and evaluating the plant organ detection model. The model worked particularly well on leaves and stems, while flowers were also present in large numbers in the sheets, but were not equally well recognised.
AD  - Senckenberg Biodivers & Climate Res Ctr SBiK, Frankfurt, GermanyAD  - Philipps Univ Marburg, Dept Math & Comp Sci, Marburg, GermanyAD  - Palmengarten Stadt Frankfurt, Frankfurt, GermanyAD  - Senckenberg Res Inst, Frankfurt, GermanyAD  - Nat Hist Museum, Frankfurt, GermanyC3  - Senckenberg Biodiversitat & Klima- Forschungszentrum (BiK-F)C3  - Philipps University MarburgC3  - Senckenberg Gesellschaft fur Naturforschung (SGN)FU  - DFG [316452578]; NVIDIA Corporation
FX  - TH, SY, MS and SD received funding from the DFG Project "Mobilization of trait data from digital image files by deep learning approaches" (grant 316452578). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the TITAN Xp GPU to CW used for this research. Digitisation of the Senckenberg specimens used in this study has taken place in the frame of the Global Plants Initiative.
CR  - Bargoti Suchet, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3626, DOI 10.1109/ICRA.2017.7989417
CR  - Borges L, 2020, SCHRODINGERS PHENOTY, DOI DOI 10.1101/2020.03.31.018812
CR  - Borsch T., 2020, RES IDEAS OUTCOMES, V6, pe50675, DOI [10.3897/rio.6.e50675, DOI 10.3897/RIO.6.E50675]
CR  - Carranza-Rojas J, 2018, MULTIMED SYST APPL, P151, DOI 10.1007/978-3-319-76445-0_9
CR  - Carranza-Rojas J, 2017, BMC EVOL BIOL, V17, P1, DOI 10.1186/s12862-017-1014-z
CR  - Chagnoux S, 2020, VASCULAR PLANTS COLL, DOI [10.15468/nc6rxy, DOI 10.15468/NC6RXY]
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Figueiredo E, 2014, TAXON, V63, P187, DOI 10.12705/631.18
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Hani N, 2020, J FIELD ROBOT, V37, P263, DOI 10.1002/rob.21902
CR  - Heberling JM, 2019, APPL PLANT SCI, V7, DOI 10.1002/aps3.1223
CR  - Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
CR  - Jiang Y, 2019, PLANT METHODS, V15, DOI 10.1186/s13007-019-0528-3
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Lang PLM, 2019, NEW PHYTOL, V221, P110, DOI 10.1111/nph.15401
CR  - Le Bras G, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.16
CR  - LeCun Y., 1995, HDB BRAIN THEORY NEU, P276
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
CR  - Mai XC, 2018, IEEE INT CONF ROBOT, P7166
CR  - Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
CR  - Ott T, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11351
CR  - Otte V., 2011, TAXON, V60, P617
CR  - Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Sa I, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16081222
CR  - Senckenberg, 2020, HERBARIUM SENCKENBER, DOI [10.15468/ucmdjy, DOI 10.15468/UCMDJY]
CR  - Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
CR  - Stein M, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111915
CR  - Sun J, 2018, AGRICULTURE-BASEL, V8, DOI 10.3390/agriculture8120196
CR  - Thiers B, 2020, WORLDS HERBARIA 2019
CR  - Tzutalin, 2015, LABELLMG
CR  - Veenstra Anneke A., 2012, Muelleria, V30, P59
CR  - Waldchen J, 2019, BIOL UNSERER Z, V49, P99, DOI [10.1002/biuz.201970211, DOI 10.1002/BIUZ.201970211]
CR  - Wang YC, 2016, INT J COMPUT SCI NET, V16, P21
CR  - Weaver WN, 2020, APPL PLANT SCI, V8, DOI 10.1002/aps3.11367
CR  - Willis CG, 2017, TRENDS ECOL EVOL, V32, P531, DOI 10.1016/j.tree.2017.03.015
CR  - Wu Y, 2019, DETECTRON2
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Younis S, 2020, PLANT ORGAN DETECTIO
CR  - Younis S, 2018, BOT LETT, V165, P377, DOI 10.1080/23818107.2018.1446357
CR  - Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
CR  - Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
PU  - PENSOFT PUBLISHERS
PI  - SOFIA
PA  - 12 PROF GEORGI ZLATARSKI ST, SOFIA, 1700, BULGARIA
DA  - DEC 10
PY  - 2020
VL  - 8
DO  - 10.3897/BDJ.8.e57090
AN  - WOS:000596935800001
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  8
Cited Reference Count:  44
ER  -

TY  - JOUR
AU  - Verma, GK
AU  - Gupta, P
TI  - Wild Animal Detection from Highly Cluttered Images Using Deep Convolutional Neural Network
T2  - INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE AND APPLICATIONS
LA  - English
KW  - Wild animal detection
KW  - convolutional neural network
KW  - VGGNet
KW  - ResNet
KW  - SVM
KW  - ensemble tree
KW  - KNN
KW  - natural scenes
AB  - Monitoring wild animals became easy due to camera trap network, a technique to explore wildlife using automatically triggered camera on the presence of wild animal and yields a large volume of multimedia data. Wild animal detection is a dynamic research field since the last several decades. In this paper, we propose a wild animal detection system to monitor wildlife and detect wild animals from highly cluttered natural images. The data acquired from the camera-trap network comprises of scenes that are highly cluttered that poses a challenge for detection of wild animals bringing about low recognition rates and high false discovery rates. To deal with the issue, we have utilized a camera trap database that provides candidate regions utilizing multilevel graph cut in the spatiotemporal area. The regions are utilized to make a validation stage that recognizes whether animals are present or not in a scene. These features from cluttered images are extracted using Deep Convolutional Neural Network (CNN). We have implemented the system using two prominent CNN models namely VGGNet and ResNet, on standard camera trap database. Finally, the CNN features fed to some of the best in class machine learning techniques for classification. Our outcomes demonstrate that our proposed system is superior compared to existing systems reported in the literature.
AD  - Natl Inst Technol Kurukshetra, Dept Comp Engn, Kurukshetra 136119, Haryana, IndiaC3  - National Institute of Technology (NIT System)C3  - National Institute of Technology KurukshetraCR  - Chacon-Murguia MI, 2012, IEEE T IND ELECTRON, V59, P3286, DOI 10.1109/TIE.2011.2106093
CR  - Chatfield K., 2014, P BRIT MACH VIS C, P114
CR  - Chauhan A.K., 2013, INT J ADV RES COMPUT, V3
CR  - Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
CR  - Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
CR  - GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Gupta P, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTING, COMMUNICATION AND AUTOMATION (ICCCA), P104, DOI 10.1109/CCAA.2017.8229781
CR  - Hongfei Yu W. L., 2014, IET COMPUT VIS, V9, P13
CR  - Kays R., 2014, P N AM CONSERVATION, P80
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lee WT, 2009, PROC CVPR IEEE, P1590, DOI 10.1109/CVPRW.2009.5206521
CR  - Mahadevan V., 2008, P IEEE C COMP VIS PA, P1
CR  - Mammeri A, 2014, IEEE ICC, P1854, DOI 10.1109/ICC.2014.6883593
CR  - Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
CR  - Rakibe Rupali S., 2013, INT J SCI RES PUBLIC, V3, P2250
CR  - Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Sermanet P., 2013, P INT C LEARN REPR
CR  - Szegedy C., 2014, ARXIV14121441
CR  - Szegedy C., 2013, P ADV NEURAL INFORM
CR  - Thakore D.G., 2012, INT J SOFT COMPUTING, V2, P44
CR  - Tilak S., 2011, INT J RES REV WIRELE, V1, P19
CR  - Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
CR  - Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
CR  - Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
CR  - Zhang Z, 2015, IEEE IMAGE PROC, P2830, DOI 10.1109/ICIP.2015.7351319
PU  - WORLD SCIENTIFIC PUBL CO PTE LTD
PI  - SINGAPORE
PA  - 5 TOH TUCK LINK, SINGAPORE 596224, SINGAPORE
DA  - DEC
PY  - 2018
VL  - 17
IS  - 4
DO  - 10.1142/S1469026818500219
AN  - WOS:000456261200004
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  27
ER  -

TY  - JOUR
AU  - Yao, L
AU  - Liu, T
AU  - Qin, J
AU  - Lu, N
AU  - Zhou, CH
TI  - Tree counting with high spatial-resolution satellite imagery based on deep neural networks
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Forestry
KW  - Deep learning algorithms
KW  - Remote sensing
KW  - Density estimation
KW  - Tree counting
KW  - DELINEATION
AB  - Forest inventory at single-tree level is of great importance to modern forest management. The inventory contains two critical parameters about trees, including their numbers and spatial locations. Traditional methods to catalogue single trees are laborious, while deep neural networks enable to discover the multi-scale features hidden in images and thus make it possible to count trees with remote sensing imagery. In this study, four different tree counting networks, which were constructed by remodeling four different classical deep convolutional neural networks, were evaluated to determine their abilities to grasp the relationship between remote sensing images and tree locations for automatic tree counting end-to-end. To this end, a tree counting dataset was constructed with remote sensing images of 0.8-m spatial resolution in distinct regions. This dataset consisted of 24 GF-II images and the corresponding manually annotated locations of trees based on these images. Thereafter, a large number of experiments were conducted to examine the performance of these networks in regards to tree counting. The results demonstrated that all networks could achieve the competitive performance (above 0.91) in terms of the determination coefficient (R-2) between the ground truth and the estimated values. The average accuracy of the Encoder-Decoder Network (one of the four networks) was greater than 91.58% and its R-2 was equal to 0.97, achieving the best performance. It has been found that the deep learning is an efficient and effective means for tree counting task.
AD  - Chinese Acad Sci, Inst Geog Sci & Nat Resources Res, State Key Lab Resources & Environm Informat Syst, Beijing, Peoples R ChinaAD  - Southern Marine Sci & Engn Guangdong Lab, Guangzhou, Peoples R ChinaAD  - China Univ Geosci, Beijing, Peoples R ChinaAD  - Jiangsu Ctr Collaborat Innovat Geog Informat Reso, Nanjing, Peoples R ChinaAD  - Univ Chinese Acad Sci, Beijing, Peoples R ChinaC3  - Chinese Academy of SciencesC3  - Institute of Geographic Sciences & Natural Resources Research, CASC3  - China University of GeosciencesC3  - Chinese Academy of SciencesC3  - University of Chinese Academy of Sciences, CASFU  - National Natural Science Foundation of China [41771380]; Key Special Project for Introduced Talents Team of Southern Marine Science and Engineering Guangdong Laboratory (Guangzhou) [GML2019ZD0301]; National Postdoctoral Program for Innovative Talents, China [BX20200100]; National Data Sharing Infrastructure of Earth System Science
FX  - This study was jointly supported by the National Natural Science Foundation of China (Grant No. 41771380), Key Special Project for Introduced Talents Team of Southern Marine Science and Engineering Guangdong Laboratory (Guangzhou) (GML2019ZD0301) the National Postdoctoral Program for Innovative Talents (BX20200100), China, as well as the National Data Sharing Infrastructure of Earth System Science (http://www.geodata.cn/).
CR  - Anderson CB, 2018, ECOL LETT, V21, P1572, DOI 10.1111/ele.13106
CR  - Boominathan L, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P640, DOI 10.1145/2964284.2967300
CR  - Cao SX, 2011, AMBIO, V40, P828, DOI 10.1007/s13280-011-0150-8
CR  - Caughlin TT, 2016, ECOL APPL, V26, P2367, DOI 10.1002/eap.1436
CR  - Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
CR  - Crowther TW, 2015, NATURE, V525, P201, DOI 10.1038/nature14967
CR  - Culvenor DS, 2002, COMPUT GEOSCI-UK, V28, P33, DOI 10.1016/S0098-3004(00)00110-2
CR  - Deng G., 2009, RES INDIVIDUAL TREE
CR  - FAO, 2016, STATE WORLDS FORESTS
CR  - Gomes M.F., 2016, 23 ISPRS C PRAG CZEC
CR  - Hanson MA, 2012, SCIENCE, V335, P851, DOI [10.1126/science.1244693, 10.1126/science.1215904]
CR  - Khan S., 2018, INT ARCH PHOTOGRAMM, VXLII- 5, P801
CR  - Kong WY, 2019, INT J REMOTE SENS, V40, P8528, DOI 10.1080/01431161.2019.1615653
CR  - Koon Cheang E., 2017, ARXIV E PRINTS ARXIV
CR  - Lempitsky Victor, 2010, ADV NEURAL INFORM PR, P1324
CR  - Li WJ, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9010022
CR  - Pan YD, 2011, SCIENCE, V333, P988, DOI 10.1126/science.1201609
CR  - Pouliot DA, 2002, REMOTE SENS ENVIRON, V82, P322, DOI 10.1016/S0034-4257(02)00050-0
CR  - Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
CR  - Simonyan K., 2014, ARXIV14091556, DOI 10.1109/ACPR.2015.7486599 2-s2.0-84978884248
CR  - Sindagi VA, 2018, PATTERN RECOGN LETT, V107, P3, DOI 10.1016/j.patrec.2017.07.007
CR  - Wagner FH, 2018, ISPRS J PHOTOGRAMM, V145, P362, DOI 10.1016/j.isprsjprs.2018.09.013
CR  - Wang YR, 2019, INT J REMOTE SENS, V40, P7356, DOI 10.1080/01431161.2018.1513669
CR  - Weinstein BG, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111309
CR  - Wu JT, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11060691
CR  - Xie WD, 2018, COMP M BIO BIO E-IV, V6, P283, DOI 10.1080/21681163.2016.1149104
CR  - Xie YQ, 2018, IEEE DATA MINING, P1344, DOI 10.1109/ICDM.2018.00183
CR  - Zhang XJ, 2019, 2019 IEEE 18TH INTERNATIONAL SYMPOSIUM ON NETWORK COMPUTING AND APPLICATIONS (NCA), P229
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - JUN
PY  - 2021
VL  - 125
DO  - 10.1016/j.ecolind.2021.107591
AN  - WOS:000637798700004
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  28
ER  -

TY  - JOUR
AU  - Ghosh, R
AU  - Kumar, A
TI  - A hybrid deep learning model by combining convolutional neural network and recurrent neural network to detect forest fire
T2  - MULTIMEDIA TOOLS AND APPLICATIONS
LA  - English
KW  - Forest fire
KW  - Deep learning
KW  - Convolutional neural network
KW  - Recurrent neural network
KW  - REAL-TIME FIRE
KW  - FLAME DETECTION
KW  - COLOR
KW  - RECOGNITION
AB  - Forest fire poses a serious threat to wildlife, environment, and all mankind. This threat has prompted the development of various intelligent and computer vision based systems to detect forest fire. This article proposes a novel hybrid deep learning model to detect forest fire. This model uses a combination of convolutional neural network (CNN) and recurrent neural network (RNN) for feature extraction and two fully connected layers for final classification. The final feature map obtained from the CNN has been flattened and then fed as an input to the RNN. CNN extracts various low level as well as high level features, whereas RNN extracts various dependent and sequential features. The use of both CNN and RNN for feature extraction is proposed in this article for the first time in the literature of forest fire detection. The performance of the proposed system has been evaluated on two publicly available fire datasets-Mivia lab dataset and Kaggle fire dataset. Experimental results demonstrate that the proposed model is able to achieve very high classification accuracy and outperforms the existing state-of-the-art results in this regard.
AD  - Natl Inst Technol Patna, Dept CSE, Patna, Bihar, IndiaC3  - National Institute of Technology (NIT System)C3  - National Institute of Technology PatnaCR  - Barmpoutis P, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193177
CR  - Borges PVK, 2010, IEEE T CIRC SYST VID, V20, P721, DOI 10.1109/TCSVT.2010.2045813
CR  - Celik T, 2007, J VIS COMMUN IMAGE R, V18, P176, DOI 10.1016/j.jvcir.2006.12.003
CR  - Cruz H, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16060893
CR  - Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
CR  - Foggia P, 2015, MIVIA LAB DATASET
CR  - Foggia P, 2015, IEEE T CIRC SYST VID, V25, P1545, DOI 10.1109/TCSVT.2015.2392531
CR  - Ghosh R, 2019, PATTERN RECOGN, V92, P203, DOI 10.1016/j.patcog.2019.03.030
CR  - Gomes P, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58821
CR  - Gong YJ, 2016, IEEE T CYBERNETICS, V46, P2277, DOI 10.1109/TCYB.2015.2475174
CR  - Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
CR  - Ho CC, 2009, MEAS SCI TECHNOL, V20, DOI 10.1088/0957-0233/20/4/045502
CR  - Khatami A, 2017, EXPERT SYST APPL, V68, P69, DOI 10.1016/j.eswa.2016.09.021
CR  - Kim YH, 2014, INT J DISTRIB SENS N, DOI 10.1155/2014/923609
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Larsen A, 2021, J EXPO SCI ENV EPID, V31, P170, DOI 10.1038/s41370-020-0246-y
CR  - Li ZL, 2018, IEEE T IND INFORM, V14, P1146, DOI 10.1109/TII.2017.2768530
CR  - Liu CB, 2004, INT C PATT RECOG, P134, DOI 10.1109/HPD.2004.1346686
CR  - Mahmoud MAI, 2018, MATH PROBL ENG, V2018, DOI 10.1155/2018/7612487
CR  - Mao J, 2015, P ICLR
CR  - Muhammad K, 2019, IEEE T SYST MAN CY-S, V49, P1419, DOI 10.1109/TSMC.2018.2830099
CR  - Park M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12223715
CR  - Saied A., 2018, FIRE DATASET
CR  - Saripalli S, 2003, IEEE T ROBOTIC AUTOM, V19, P371, DOI 10.1109/TRA.2003.810239
CR  - Sousa MJ, 2020, EXPERT SYST APPL, V142, DOI 10.1016/j.eswa.2019.112975
CR  - Sudhakar S, 2020, COMPUT COMMUN, V149, P1, DOI 10.1016/j.comcom.2019.10.007
CR  - Sun ZW, 2018, J NETW COMPUT APPL, V112, P29, DOI 10.1016/j.jnca.2018.03.023
CR  - Toreyin BU, 2006, PATTERN RECOGN LETT, V27, P49, DOI 10.1016/j.patrec.2005.06.015
CR  - Yuan C, 2016, INT CONF UNMAN AIRCR, P1200, DOI 10.1109/ICUAS.2016.7502546
CR  - Zhang QJ, 2016, ADV SOC SCI EDUC HUM, V47, P568
CR  - Zhao Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18030712
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
DO  - 10.1007/s11042-022-13068-8
AN  - WOS:000787139200002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Swarup, P
AU  - Chen, P
AU  - Hou, R
AU  - Que, PJ
AU  - Liu, P
AU  - Kong, AWK
TI  - Giant panda behaviour recognition using images
T2  - GLOBAL ECOLOGY AND CONSERVATION
LA  - English
KW  - Giant panda
KW  - Animal behaviour recognition
KW  - Deep learning
KW  - Convolutional neural network
KW  - Wildlife ecology
AB  - Monitoring giant panda (Ailuropoda melanoleuca) behaviour is critical for their conservation and understanding their health conditions. Currently, captive giant panda behaviour is usually monitored by their caregivers. In previous studies, researchers observed panda behaviours for short time spans over a period. However, both caregivers and researchers cannot monitor them 24-h using traditional methods of observation. In other words, animal behaviour data are difficult to collect over long periods and are prone to errors when recorded manually. Some researchers have used wearable devices such as accelerometer ear tags and collar-mounted units with a global position system (GPS) receiver and con tactless devices such as depth cameras and video cameras for understanding behaviour of other animals such as primates and American white pelicans. However, the giant panda, an icon of endangered species conservation, is almost completely neglected in these studies. To monitor giant panda behaviour effectively, a fully automated giant panda behaviour recognition method based on Faster R-CNN and two modified ResNet was created. The Faster R-CNN network was able to detect panda bodies and panda faces in images. One of the modified ResNet was trained to classify their behaviour into five classes, walking, sitting, resting, climbing, and eating and the other to recognise whether the panda's eyes and mouth were opened or closed. Experiments were conducted on 10,804 images collected from over 218 pandas in various environments and illumination conditions. The experimental results were very encouraging and achieved an overall accuracy of 90% for the five panda behaviours and an overall accuracy of 84% for the subtle panda facial motions. The proposed method provides an effective way to monitor giant panda behaviour in captivity. (c) 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
AD  - Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, SingaporeAD  - Chengdu Res Base Giant Panda Breeding, Chengdu 610086, Peoples R ChinaAD  - Sichuan Key Lab Conservat Biol Endangered Wildlif, Chengdu 610086, Peoples R ChinaAD  - Sichuan Acad Giant Panda, Chengdu 610086, Peoples R ChinaC3  - Nanyang Technological University & National Institute of Education (NIE) SingaporeC3  - Nanyang Technological UniversityFU  - Chengdu Research Base of Giant Panda Breeding [CPB2018-02, 2020CPB-C09, 2021CPB-C01, 2021CPB-B06]; Nanyang Technological University, Singapore is under the project Development of a Computational Method for Giant Panda Identification from Images [CPB2018-02]
FX  - This work was supported by the Chengdu Research Base of Giant Panda Breeding [NO. CPB2018-02; NO. 2020CPB-C09; NO.2021CPB-C01; NO.2021CPB-B06]. The research done in the Nanyang Technological University, Singapore is under the project Development of a Computational Method for Giant Panda Identification from Images NO. CPB2018-02. We are grateful to James Ayala Gonzalez for his suggestions on the writing of our paper.
CR  - [Anonymous], 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.511
CR  - Chen J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P911
CR  - Chen J, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P814
CR  - Chen P, 2020, ECOL EVOL, V10, P3561, DOI 10.1002/ece3.6152
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Ding RZ, 2020, INT CONF ACOUST SPEE, P2108, DOI 10.1109/ICASSP40776.2020.9052960
CR  - Dutta A, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2276, DOI 10.1145/3343031.3350535
CR  - Fogarty ES, 2020, ANIM REPROD SCI, V216, DOI 10.1016/j.anireprosci.2020.106345
CR  - Hansen RL, 2010, ZOO BIOL, V29, P470, DOI 10.1002/zoo.20280
CR  - Hou J, 2020, BIOL CONSERV, V242, DOI 10.1016/j.biocon.2020.108414
CR  - Janssen DL, 2006, GIANT PANDAS: BIOLOGY, VETERINARY MEDICINE AND MANAGEMENT, P59, DOI 10.1017/CBO9780511542244.005
CR  - Juan Chen, 2012, 2012 International Conference on Computational Problem-Solving (ICCP), P358, DOI 10.1109/ICCPS.2012.6384309
CR  - Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
CR  - Labuguen R, 2019, 2019 JOINT 8TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2019 3RD INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR) WITH INTERNATIONAL CONFERENCE ON ACTIVITY AND BEHAVIOR COMPUTING (ABC), P297, DOI 10.1109/ICIEV.2019.8858533
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
CR  - Martin-Wintle MS, 2017, BIOL CONSERV, V207, P27, DOI 10.1016/j.biocon.2017.01.010
CR  - Matkowski WM, 2019, IEEE IMAGE PROC, P1680, DOI 10.1109/ICIP.2019.8803125
CR  - Mu X, 2019, NUMBER CAPTIVE PANDA
CR  - Nie YG, 2015, SCIENCE, V349, P171, DOI 10.1126/science.aab2413
CR  - Pons P, 2017, EXPERT SYST APPL, V86, P235, DOI 10.1016/j.eswa.2017.05.063
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Riaboff L, 2020, COMPUT ELECTRON AGR, V169, DOI 10.1016/j.compag.2019.105179
CR  - Swaisgood RR, 2018, ANIM CONSERV, V21, P474, DOI 10.1111/acv.12418
CR  - Wang C., 1990, RES PROGR BIOL GIANT, P263
CR  - Wang GM, 2019, ECOL INFORM, V49, P69, DOI 10.1016/j.ecoinf.2018.12.002
CR  - Wang HN, 2019, IEEE INT CONF COMP V, P279, DOI 10.1109/ICCVW.2019.00037
CR  - Wwf, 2020, GIANT PAND
CR  - Yang QM, 2018, COMPUT ELECTRON AGR, V155, P453, DOI 10.1016/j.compag.2018.11.002
CR  - Yang ZS, 2018, BIOL CONSERV, V217, P181, DOI 10.1016/j.biocon.2017.08.012
CR  - Zhang GQ, 2004, ZOO BIOL, V23, P15, DOI 10.1002/zoo.10118
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - APR
PY  - 2021
VL  - 26
DO  - 10.1016/j.gecco.2021.e01510
AN  - WOS:000641413000008
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Baek, SS
AU  - Pyo, J
AU  - Pachepsky, Y
AU  - Park, Y
AU  - Ligaray, M
AU  - Ahn, CY
AU  - Kim, YH
AU  - Chun, JA
AU  - Cho, KH
TI  - Identification and enumeration of cyanobacteria species using a deep neural network
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Fast R-CNN
KW  - CNN
KW  - Identification
KW  - Cell counting
KW  - Cyanobacteria
KW  - TOXIN-PRODUCING CYANOBACTERIA
KW  - VARIABILITY
KW  - HEALTH
KW  - IMPACT
KW  - WATER
AB  - Cell classification and cell counting are essential for the detection, monitoring, forecasting, and management of harmful algae populations. Conventional methods of algae classification and cell counting are known to be time-consuming, labor-intensive, and subjective, depending on the expertise of the observers. The objectives of this study were to classify and quantify five cyanobacteria using the deep learning techniques of a fast regional convolutional neural network (R-CNN) and convolutional neural network (CNN). Water samples taken from the Haman weir of Nakdong River and Baekje weir of the Geum River were observed under the optical microscope. The images captured by the microscope were used to classify cyanobacteria species using the fast R-CNN model. Post-processing of the classified images generated by the model reduced the noises of the cell features, thereby improving the accuracy of the CNN model in quantifying cyanobacteria cells. The distinctive morphological features of the five species were extracted by the fast R-CNN model. This model was able to achieve a reasonable agreement with the manual classification results, yielding average precision (AP) values of 0.929, 0.973, 0.829, 0.890, and 0.890 for Microcystis aeruginosa, Microcystis wesenbergii, Dolichospermum, Oscillatoria, and Aphanizomenon, respectively. The CNN model for the Microcystis species obtained an R-2 value of 0.775 and RMSE value of 26 cells for training, and an R-2 of 0.854 and RMSE of 23 cells for validation. A minor underestimation and overestimation for a population with < 50 cells and > 250 cells were observed, respectively, which are due to the overlapping of cells and the presence of blurry regions in the input images. In conclusion, this study was able to demonstrate the reliable performance of cyanobacteria classification and cell counting using deep learning approaches.
AD  - Ulsan Natl Inst Sci & Technol, Sch Urban & Environm Engn, 50 UNIST Gil, Ulsan 689798, South KoreaAD  - USDA ARS, Environm Microbial & Food Safety Lab, Beltsville, MD USAAD  - Konkuk Univ, Sch Civil & Environm Engn, Seoul 05029, South KoreaAD  - Korea Res Inst Biosci & Biotechnol KRIBB, Environm Biotechnol Res Ctr, 111 Gwahangno, Daejeon 305806, South KoreaAD  - Hanyang Univ, Dept Life Sci, Seoul 04763, South KoreaAD  - APEC Climate Ctr, Climate Analyt Dept, 12 Centum 7 Ro, Busan 612020, South KoreaC3  - Ulsan National Institute of Science & Technology (UNIST)C3  - United States Department of Agriculture (USDA)C3  - Konkuk UniversityC3  - Korea Research Institute of Bioscience & Biotechnology (KRIBB)C3  - Hanyang UniversityFU  - Basic Core Technology Development Program for the Oceans and the Polar Regions of the National Research Foundation (NRF) - Ministry of Science, ICT & Future Planning [NRF-2016M1A5A1027457]; Basic Science Research Program through the National Research Foundation of Korea (NRF) - Ministry of Education [2017R1D1A1B04033074]
FX  - This research was supported by the Basic Core Technology Development Program for the Oceans and the Polar Regions of the National Research Foundation (NRF) funded by the Ministry of Science, ICT & Future Planning (NRF-2016M1A5A1027457) and by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (No. 2017R1D1A1B04033074).
CR  - Alverson A. J., 2003, J APPL PHYCOL, V39, P1
CR  - [Anonymous], 2015, PATTERN ANAL MACH IN, V38, P814, DOI [10.1109/TPAMI. 2015.2465908, DOI 10.1109/TPAMI.2015.2465908]
CR  - Bartram J., 1999, TOXIC CYANOBACTERIA
CR  - Biegala IC, 2002, J PHYCOL, V38, P404, DOI 10.1046/j.1529-8817.2002.01045.x
CR  - Carmichael WW, 2001, HUM ECOL RISK ASSESS, V7, P1393, DOI 10.1080/20018091095087
CR  - Cheung MY, 2013, J MICROBIOL, V51, P1, DOI 10.1007/s12275-013-2549-3
CR  - Ciresan DC, 2012, 2012 INT JOINT C NEU, DOI [10.1109/IJCNN.2012.6252544, DOI 10.1109/IJCNN.2012.6252544]
CR  - Correa I, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P20, DOI 10.1109/ICMLA.2017.0-183
CR  - Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
CR  - Davies JM, 2003, J ENVIRON MANAGE, V68, P273, DOI 10.1016/S0301-4797(03)00070-7
CR  - Davis J, 2006, P 23 INT C MACH LEAR, DOI DOI 10.1145/1143844.1143874
CR  - Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
CR  - Girshick R. B., 2015, ABS150408083 CORR
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Hinton G. E., 2012, IMPROVING NEURAL NET
CR  - Holt J.G., 1994, BERGEYS MANUAL DETER, Vninth, P377
CR  - James G., 2013, INTRO STAT LEARNING, P176
CR  - Jiang HZ, 2017, IEEE INT CONF AUTOMA, P650, DOI [10.1109/MWSYM.2017.8058653, 10.1109/FG.2017.82]
CR  - Joung SH, 2006, J MICROBIOL, V44, P562
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Kalchbrenner N., 2014, ARXIV14042188
CR  - Kangur Kulli, 2005, Proceedings of the Estonian Academy of Sciences Biology Ecology, V54, P67
CR  - Khan, 2003, PAKISTAN J BIOL SCI, V6, P1046, DOI DOI 10.3923/PJBS.2003.1046.1050
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Le THN, 2016, IEEE COMPUT SOC CONF, P46, DOI 10.1109/CVPRW.2016.13
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Lee MC, 2017, INFORM PROCESS LETT, V128, P14, DOI 10.1016/j.ipl.2017.07.010
CR  - Lehmussola A, 2007, IEEE T MED IMAGING, V26, P1010, DOI 10.1109/TMI.2007.896925
CR  - Leonard JA, 2005, HYDROBIOLOGIA, V537, P89, DOI 10.1007/s10750-004-2483-9
CR  - Li X., 2015, C OCEANS 2015 MTS IE
CR  - Li XP, 2017, APPL OPTICS, V56, P6520, DOI 10.1364/AO.56.006520
CR  - Macario IPE, 2015, HYDROBIOLOGIA, V757, P155, DOI 10.1007/s10750-015-2248-7
CR  - Marchall K.C., 1982, ADV MICROBIAL ECOLOG
CR  - Ministry of Environment (MOE), 2011, PLAN IMPL ALG WARN S
CR  - Nagi J., 2011, 2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA 2011), P342, DOI 10.1109/ICSIPA.2011.6144164
CR  - Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342
CR  - OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
CR  - Otsuka S, 2000, J GEN APPL MICROBIOL, V46, P39, DOI 10.2323/jgam.46.39
CR  - Park J, 2019, ENVIRON ENG RES, V24, P397, DOI 10.4491/eer.2018.266
CR  - Pedraza A, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7050460
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Rajaniemi P, 2005, INT J SYST EVOL MICR, V55, P11, DOI 10.1099/ijs.0.63276-0
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Repka S, 1997, FRESHWATER BIOL, V38, P675, DOI 10.1046/j.1365-2427.1997.00236.x
CR  - Ron K., 1998, MACH LEARN, V30, P271, DOI DOI 10.1023/A:1017181826899
CR  - Sasaki Y., 2007, TEACH TUTOR MAT, P1
CR  - Soille P., 1999, MORPHOLOGICAL IMAGE, P173, DOI [10.1007/978-3-662-05088-0, DOI 10.1007/978-3-662-05088-0]
CR  - Srivastava A, 2015, BIOMED RES INT, V2015, DOI 10.1155/2015/584696
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - Steffensen DA, 2008, ADV EXP MED BIOL, V619, P855, DOI 10.1007/978-0-387-75865-7_37
CR  - Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - AUG
PY  - 2020
VL  - 115
DO  - 10.1016/j.ecolind.2020.106395
AN  - WOS:000561438700014
N1  - Times Cited in Web of Science Core Collection:  10
Total Times Cited:  10
Cited Reference Count:  52
ER  -

TY  - JOUR
AU  - Desgarnier, L
AU  - Mouillot, D
AU  - Vigliola, L
AU  - Chaumont, M
AU  - Mannocci, L
TI  - Putting eagle rays on the map by coupling aerial video-surveys and deep learning
T2  - BIOLOGICAL CONSERVATION
LA  - English
KW  - Automated species detection
KW  - Convolutional neural networks
KW  - Coral reefs
KW  - Elasmobranchs
KW  - New-Caledonia
KW  - NEURAL-NETWORKS
KW  - SCALE MOVEMENT
KW  - SHARKS
KW  - REEF
KW  - IDENTIFICATION
KW  - ACCURATE
KW  - SEA
AB  - Reliable and efficient techniques are urgently needed to monitor elasmobranch populations that face increasing threats worldwide. Aerial video-surveys provide precise and verifiable observations for the rapid assessment of species distribution and abundance in coral reefs, but the manual processing of videos is a major bottleneck for timely conservation applications. In this study, we applied deep learning for the automated detection and mapping of vulnerable eagle rays from aerial videos. A light aircraft dedicated to touristic flights allowed us to collect 42 h of aerial video footage over a shallow coral lagoon in New Caledonia (Southwest Pacific). We extracted the videos at a rate of one image per second before annotating them, yielding 314 images with eagle rays. We then trained a convolutional neural network with 80% of the eagle ray images and evaluated its ac-curacy on the remaining 20% (independent data sets). Our deep learning model detected 92% of the annotated eagle rays in a diversity of habitats and acquisition conditions across the studied coral lagoon. Our study offers a potential breakthrough for the monitoring of ray populations in coral reef ecosystems by providing a fast and accurate alternative to the manual processing of aerial videos. Our deep learning approach can be extended to the detection of other elasmobranchs and applied to systematic aerial surveys to not only detect individuals but also estimate species density in coral reef habitats.
AD  - Univ Montpellier, IFREMER, CNRS, MARBEC,IRD, Montpellier, FranceAD  - Inst Univ France, Paris, FranceAD  - Univ Reunion, Univ Nouvelle Caledonie, IFREMER, CNRS,ENTROPIE,IRD, Noumea, New Caledonia, FranceAD  - Univ Montpellier, CNRS, LIRMM, Montpellier, FranceAD  - Univ Nimes, Nimes, FranceC3  - Centre National de la Recherche Scientifique (CNRS)C3  - IfremerC3  - Institut de Recherche pour le Developpement (IRD)C3  - Universite de MontpellierC3  - Institut Universitaire de FranceC3  - Centre National de la Recherche Scientifique (CNRS)C3  - IfremerC3  - Institut de Recherche pour le Developpement (IRD)C3  - University of La ReunionC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Universite Paul-ValeryC3  - Universite Perpignan Via DomitiaC3  - Universite de MontpellierC3  - Universite de NimesFU  - European Union [845178]
FX  - We are grateful to Jugurtha Ifticen, Lola Romant, Nacim Guellati, Gwendal Quimbre and Matis Toitot for their help with visualisation of ULM videos and annotation of images. We thank Air Paradise for their collaboration in collecting ULM video sequences in New Caledonia. This project received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 845178 (`MEGAFAUNA').
CR  - Abadi M, 2016, ABS160304467
CR  - Ajemian MJ, 2014, ENVIRON BIOL FISH, V97, P1067, DOI 10.1007/s10641-014-0296-x
CR  - Ajemian MJ, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040227
CR  - Boussarie G, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aap9661
CR  - Ceccarelli DM, 2013, ADV MAR BIOL, V66, P213, DOI 10.1016/B978-0-12-408096-6.00004-3
CR  - Chen Z, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010139
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Colefax AP, 2018, ICES J MAR SCI, V75, P1, DOI 10.1093/icesjms/fsx100
CR  - DeGroot BC, 2020, ENDANGER SPECIES RES, V42, P109, DOI 10.3354/esr01047
CR  - Ditria EM, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.00429
CR  - Dujon AM, 2021, REMOTE SENS ECOL CON, V7, P341, DOI 10.1002/rse2.205
CR  - Dulvy N.K., 2020, AETOBATUS NARINARI I, DOI [10.2305/IUCN.UK.2021-2.RLTS.T42564343A201613657. en, DOI 10.2305/IUCN.UK.2021-2.RLTS.T42564343A201613657.EN]
CR  - Dulvy NK, 2021, CURR BIOL, V31, P4773, DOI 10.1016/j.cub.2021.08.062
CR  - Dwyer RG, 2020, CURR BIOL, V30, P480, DOI 10.1016/j.cub.2019.12.005
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Fellows I., 2019, OPENSTREETMAP ACCESS
CR  - Fricke R, 2011, STUTTG BEITR NATURKU, V123
CR  - Froese R., 2021, WORLD WIDE WEB ELECT
CR  - Fust P, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108380
CR  - Gorkin R, 2020, DRONES-BASEL, V4, DOI 10.3390/drones4020018
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P1490, DOI 10.1111/2041-210X.13246
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
CR  - Guirado E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50795-9
CR  - Hodgson A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079556
CR  - Jabado RW, 2018, FISH FISH, V19, P1043, DOI 10.1111/faf.12311
CR  - Jalal A, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101088
CR  - Juhel J.-B, 2017, REEF ACCESSIBILITY I, P11
CR  - Kelaher B, 2019, J UNMANNED VEHICLE S, V32
CR  - Kelaher BP, 2020, MAR FRESHWATER RES, V71, P68, DOI 10.1071/MF18375
CR  - Kiszka JJ, 2016, MAR ECOL PROG SER, V560, P237, DOI 10.3354/meps11945
CR  - Last PR, 2010, DESCRIPTIONS NEW SHA
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lin T.-Y., 2015, ARXIV160304467 CS
CR  - MacNeil MA, 2020, NATURE, V583, P801, DOI 10.1038/s41586-020-2519-y
CR  - Mannocci L, 2022, CONSERV BIOL, V36, DOI 10.1111/cobi.13798
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Pacoureau N, 2021, NATURE, V589, P567, DOI 10.1038/s41586-020-03173-9
CR  - Padubidri C, 2021, ANIM BIOTELEM, V9, DOI 10.1186/s40317-021-00247-x
CR  - Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
CR  - Raoult V, 2020, DRONES-BASEL, V4, DOI 10.3390/drones4040064
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rieucau G, 2018, J FISH BIOL, V93, P119, DOI 10.1111/jfb.13645
CR  - Rigby C.L., 2020, AETOMYLAEUS MACULATU
CR  - Rizzari JR, 2014, MAR BIOL, V161, P2847, DOI 10.1007/s00227-014-2550-3
CR  - Sarle W.S, 1995, COMPUTER SCI
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Schofield G, 2017, MAR ECOL PROG SER, V575, P153, DOI 10.3354/meps12193
CR  - Sellas AB, 2015, J HERED, V106, P266, DOI 10.1093/jhered/esv011
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - Sykora-Bodie ST, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-17719-x
CR  - Torney CJ, 2019, METHODS ECOL EVOL, V10, P779, DOI 10.1111/2041-210X.13165
CR  - Unel FO, 2019, IEEE COMPUT SOC CONF, P582, DOI 10.1109/CVPRW.2019.00084
CR  - Villon S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67573-7
CR  - Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
CR  - Ward-Paige C.A, 2017, GLOB ENV CHANG, V16
CR  - Wickham H., 2020, GGPLOT2 CREATE ELEGA
CR  - Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
CR  - Yan HF, 2021, SCI ADV, V7, DOI 10.1126/sciadv.abb6026
CR  - Zoph Barret, 2019, ARXIV PREPRINT ARXIV
PU  - ELSEVIER SCI LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND
DA  - MAR
PY  - 2022
VL  - 267
DO  - 10.1016/j.biocon.2022.109494
AN  - WOS:000788067900008
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  59
ER  -

TY  - JOUR
AU  - Silva, LC
AU  - Padua, MBS
AU  - Ogusuku, LM
AU  - Albertini, MK
AU  - Pimentel, R
AU  - Backes, AR
TI  - Wild boar recognition using convolutional neural networks
T2  - CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE
LA  - English
KW  - convolutional neural networks
KW  - image classification
KW  - wildlife monitoring
AB  - Wild boar (Sus scrofa) is a destructive species of swine. They spread diseases, represent a threat to native species, and destroy natural habitats by destabilizing river banks, thus reducing water flow. The monitoring of populations of wild boars is central to the execution and evaluation of methods to control them. To address this issue, in this article, we retrain and apply four convolutional neural networks (CNNs; AlexNet, VGG-16, Inception-v3, and ResNet-50) to classify different species of "bush pigs" in real-world footage: two native species of the Brazilian fauna, collared peccary (Pecari tajacu) and white-lipped peccary (Tayassu pecari), and one invasive species, wild boar (S. scrofa). Results show that CNN can be used to classify animals with very similar behavior and appearance and that ResNet-50 outperforms all compared CNN in terms of accuracy (98.33%) and the lowest probability of false positives (i.e., native species classified as wild boar).
AD  - Univ Fed Uberlandia, Sch Comp Sci, Ave Joao Naves de Avila 2-121, BR-38400902 Uberlandia, MG, BrazilAD  - State Forest Inst, Triangle Reg Unit, Uberlandia, MG, BrazilC3  - Universidade Federal de UberlandiaFU  - Conselho Nacional de Desenvolvimento Cientifico e Tecnologico [301715/2018-1]; Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior [001]
FX  - Conselho Nacional de Desenvolvimento Cientifico e Tecnologico, Grant/Award Number: 301715/2018-1; Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior, Grant/Award Number: 001
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Alsadi EMTA, 2019, INT J RECENT TECHNOL, V8, P151
CR  - Barbedo JGA, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245436
CR  - Daxini N., 2015, INT J INNOVATIVE RES, V3, P1
CR  - Doutel-Ribas C, 2019, PESQUI AGROPECU BRAS, V54, DOI [10.1590/S1678-3921.pab2019.v54.00241, 10.1590/s1678-3921.pab2019.v54.00241]
CR  - Favorskaya M, 2019, PROCEDIA COMPUT SCI, V159, P933, DOI 10.1016/j.procs.2019.09.260
CR  - Guo YH, 2020, IET IMAGE PROCESS, V14, P585, DOI 10.1049/iet-ipr.2019.1042
CR  - Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
CR  - He K., 2015, CVPR
CR  - Jiang QY, 2017, PERSPECT SUSTAIN GRO, P1, DOI 10.1007/978-3-319-43663-0_1
CR  - Kellenberger B, 2019, IEEE T GEOSCI REMOTE, V57, P9524, DOI 10.1109/TGRS.2019.2927393
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Kumar Y. H., 2014, SIGNAL IMAGE PROCESS, V5, P55, DOI [10.5121/sipij.2014.5406, DOI 10.5121/SIPIJ.2014.5406]
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Marsot M, 2020, COMPUT ELECTRON AGR, V173, DOI 10.1016/j.compag.2020.105386
CR  - Mauri L, 2019, EARTH SURF PROC LAND, V44, P2085, DOI 10.1002/esp.4623
CR  - Nores C, 2008, WILDLIFE BIOL, V14, P44, DOI 10.2981/0909-6396(2008)14[44:WBSSMB]2.0.CO;2
CR  - Oliveira dCHS, 2017, TECH REP
CR  - Podgorski T, 2018, J WILDLIFE MANAGE, V82, P1210, DOI 10.1002/jwmg.21480
CR  - Qureshi Q., 2009, IND J SCI T ENV TECH, V2, P170, DOI DOI 10.3923/jest.2009.170.178
CR  - Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
CR  - Seymour AC, 2017, SCI REP-UK, V7, DOI 10.1038/srep45127
CR  - Sharma SU, 2017, THESIS
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Wawerla J, 2009, MACH VISION APPL, V20, P303, DOI 10.1007/s00138-008-0128-0
CR  - Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
CR  - Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - NOV 25
PY  - 2021
VL  - 33
IS  - 22
DO  - 10.1002/cpe.6010
AN  - WOS:000569393500001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Al-Qurran, R
AU  - Al-Ayyoub, M
AU  - Shatnawi, A
A1  - IEEE
TI  - Plant Classification in the Wild: A Transfer Learning Approach
T2  - 2018 19TH INTERNATIONAL ARAB CONFERENCE ON INFORMATION TECHNOLOGY (ACIT)
LA  - English
CP  - 19th International Arab Conference on Information Technology (ACIT)
KW  - Deep Learning
KW  - Convolutional Neural Networks
KW  - Transfer Learning
KW  - Data Augmentation
KW  - Plant classification
AB  - Datasets specialized in wildlife usually contain imbalanced classes of natural wild images such as, for instance, plant images, which are acquired from the surrounding environment with natural scene background. Deep neural networks have proven their efficiency in classifying such datasets. However, such an approach requires a workaround to approximately balance the classes in order to prevent the occurrence of overfitting during the training phase of the neural network. Many approaches exist to overcome this problem includes over-sampling, under sampling, generating synthetic samples, data augmentation, etc. The iNaturalist species classification and detection dataset represents a good example of vastly imbalanced datasets. It contains 13 superclasses. This work focuses on the Plantae superclass and builds a Convolutional Neural Network to distinguish a subset of the subclasses of Plantae. Our model benefits from cutting-edge techniques such as transfer learning and data augmentation to obtain a reasonably high level of accuracy (78.76%).
AD  - Jordan Univ Sci & Technol, Irbid, JordanC3  - Jordan University of Science & TechnologyCR  - Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
CR  - CHAURASIA G, 2017, P IEEE INT C COMP VI, P5315, DOI DOI 10.1109/ICCV.2017.567
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Goodfellow I., 2016, DEEP LEARNING, V1
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839
CR  - Pound MP, 2017, IEEE INT CONF COMP V, P2055, DOI 10.1109/ICCVW.2017.241
CR  - Reyes A.K., 2015, CLEF WORKING NOTES
CR  - Shatnawi A, 2018, INT CONF INFORM COMM, P72, DOI 10.1109/IACS.2018.8355444
CR  - Sun Yu, 2017, COMPUTATIONAL INTELL, V2017
CR  - Toma A, 2017, CLEF WORKING NOTES
CR  - Ubbens JR, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01190
CR  - Van Horn G., 2017, ARXIV170706642
CR  - Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120
CR  - Yalcin H., 2016, P 5 INT C AGR AGR JU, P1
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2018
SP  - 148
EP  - 152
AN  - WOS:000469047400024
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  15
ER  -

TY  - JOUR
AU  - Zizka, A
AU  - Andermann, T
AU  - Silvestro, D
TI  - IUCNN - Deep learning approaches to approximate species' extinction risk
T2  - DIVERSITY AND DISTRIBUTIONS
LA  - English
KW  - automated assessment
KW  - conservation assessment
KW  - IUCN
KW  - machine learning
KW  - red list
KW  - R-package
KW  - R PACKAGE
KW  - CONSERVATION
KW  - BIODIVERSITY
KW  - ASSESSMENTS
KW  - DIVERSITY
KW  - REPTILES
KW  - THREAT
KW  - BIAS
AB  - Aim: The Red List (RL) from the International Union for the Conservation of Nature is the most comprehensive global quantification of extinction risk, and widely used in applied conservation as well as in biogeographic and ecological research. Yet, due to the time-consuming assessment process, the RL is biased taxonomically and geographically, which limits its application on large scales, in particular for underdocumented areas such as the tropics, or understudied taxa, such as most plants and invertebrates. Here, we present IUCNN, an R-package implementing deep learning models to predict species RL status from publicly available geographic occurrence records (and other data if available).
   Innovation: We implement a user-friendly workflow to train and validate neural network models, and use them to predict species RL status. IUCNN contains specific functions for extinction risk prediction in the RL framework, including a regression-based approach to account for the ordinal nature of RL categories, a Bayesian approach for improved uncertainty quantification and a convolutional neural network to predict species RL status based on their raw geographic occurrences. Most analyses run with few lines of code, not requiring users to have prior experience with neural network models. We demonstrate the use of IUCNN on an empirical dataset of similar to 14,000 orchid species, for which IUCNN models can predict extinction risk within minutes, while outperforming comparable methods based on species occurrence information.
   Main conclusions: IUCNN harnesses innovative methodology to estimate the RL status of large numbers of species. By providing estimates of the number and identity of threatened species in custom geographic or taxonomic datasets, IUCNN enables large-scale automated assessments of the extinction risk of species so far not well represented on the official RL.
AD  - Univ Leipzig, German Ctr Integrat Biodivers Res Halle Jena Leip, Leipzig, GermanyAD  - Philipps Univ Marburg, Dept Biol, Marburg, GermanyAD  - Univ Gothenburg, Dept Biol & Environm Sci, Gothenburg, SwedenAD  - Gothenburg Global Biodivers Ctr, Gothenburg, SwedenAD  - Univ Fribourg, Dept Biol, Fribourg, SwitzerlandAD  - Swiss Inst Bioinformat, Lausanne, SwitzerlandC3  - Leipzig UniversityC3  - Philipps University MarburgC3  - University of GothenburgC3  - University of GothenburgC3  - University of FribourgC3  - Swiss Institute of BioinformaticsFU  - Deutsche Forschungsgemeinschaft [DFG FZT 118]; Vetenskapsradet [2019-04739]; Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung [FN-1749, PCEFP3_187012]
FX  - Deutsche Forschungsgemeinschaft, Grant/Award Number: DFG FZT 118; Vetenskapsradet, Grant/Award Number: VR and 2019--04739; Schweizerischer Nationalfonds zur Forderung der Wissenschaftlichen Forschung, Grant/Award Number: FN--1749 and PCEFP3_187012
CR  - Abadi M., 2015, TensorFlow: large-scale machine learning on heterogeneous systems
CR  - Aiello-Lammens ME, 2015, ECOGRAPHY, V38, P541, DOI 10.1111/ecog.01132
CR  - Andermann T, 2021, ECOGRAPHY, V44, P162, DOI 10.1111/ecog.05110
CR  - Arle E, 2021, METHODS ECOL EVOL, V12, P1609, DOI 10.1111/2041-210X.13629
CR  - Bachman S, 2011, ZOOKEYS, P117, DOI 10.3897/zookeys.150.2109
CR  - Bachman SP, 2019, BIOL CONSERV, V234, P45, DOI 10.1016/j.biocon.2019.03.002
CR  - Betts J, 2020, CONSERV BIOL, V34, P632, DOI 10.1111/cobi.13454
CR  - Bland LM, 2017, ANIM CONSERV, V20, P532, DOI 10.1111/acv.12350
CR  - Bland LM, 2017, CONSERV BIOL, V31, P531, DOI 10.1111/cobi.12850
CR  - Bohm M, 2016, GLOBAL ECOL BIOGEOGR, V25, P391, DOI 10.1111/geb.12419
CR  - Bruelheide H, 2020, DIVERS DISTRIB, V26, P782, DOI 10.1111/ddi.13058
CR  - Butchart SHM, 2010, BIOL CONSERV, V143, P239, DOI 10.1016/j.biocon.2009.10.008
CR  - Cayuela L, 2012, METHODS ECOL EVOL, V3, P1078, DOI 10.1111/j.2041-210X.2012.00232.x
CR  - Coll M, 2015, GLOBAL ECOL BIOGEOGR, V24, P226, DOI 10.1111/geb.12250
CR  - Dauby G, 2017, ECOL EVOL, V7, P11292, DOI 10.1002/ece3.3704
CR  - Donaldson MR, 2016, FACETS, V1, P105, DOI 10.1139/facets-2016-0011
CR  - Fick SE, 2017, INT J CLIMATOL, V37, P4302, DOI 10.1002/joc.5086
CR  - Freiberg M, 2020, SCI DATA, V7, DOI 10.1038/s41597-020-00702-z
CR  - Gal Y, 2016, PR MACH LEARN RES, V48
CR  - Global Biodiversity Information Facility (www.gbif.org), 2019, 26 AUG 2019 GBIF OCC, DOI [10.15468/dl.4bijtu, DOI 10.15468/DL.4BIJTU]
CR  - Gonz?lez-Su?rez, 2021, FRONTIERS BIOGEOGRAP, V13, DOI DOI 10.21425/F5FBG53025
CR  - Gonzalez-del-Pliego P, 2019, CURR BIOL, V29, P1557, DOI 10.1016/j.cub.2019.04.005
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Hughes AC, 2021, ECOGRAPHY, V44, P1259, DOI 10.1111/ecog.05926
CR  - IUCN, 2012, IUCN RED LIST CATEGO, V2nd edition
CR  - IUCN, 2018, NUMB THREAT SPEC MAJ
CR  - IUCN Standards and Petitions Subcommittee, 2017, GUIDELINES USING IUC
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Lang M, 2017, R J, V9, P437
CR  - Lughadha EN, 2020, PLANTS PEOPLE PLANET, V2, P389, DOI 10.1002/ppp3.10146
CR  - Lughadha EN, 2019, PHILOS T R SOC B, V374, DOI 10.1098/rstb.2017.0402
CR  - Maldonado C, 2015, GLOBAL ECOL BIOGEOGR, V24, P973, DOI 10.1111/geb.12326
CR  - Monroe MJ, 2019, BIOL LETTERS, V15, DOI 10.1098/rsbl.2019.0633
CR  - Oliveira BF, 2020, GLOBAL ECOL BIOGEOGR, V29, P309, DOI 10.1111/geb.13031
CR  - Olson DM, 2001, BIOSCIENCE, V51, P933, DOI 10.1641/0006-3568(2001)051[0933:TEOTWA]2.0.CO;2
CR  - Parsons ECM, 2016, FRONT MAR SCI, V3, DOI 10.3389/fmars.2016.00193
CR  - Pebesma E, 2018, R J, V10, P439
CR  - Pelletier TA, 2018, P NATL ACAD SCI USA, V115, P13027, DOI 10.1073/pnas.1804098115
CR  - Pescott, 2021, OCCASSESS R PACKAGE, DOI DOI 10.1101/2021.04.19.440441
CR  - Pincheira-Donoso D, 2021, GLOBAL ECOL BIOGEOGR, V30, P1299, DOI 10.1111/geb.13287
CR  - Polaina E, 2018, GLOBAL ECOL BIOGEOGR, V27, P647, DOI 10.1111/geb.12725
CR  - Pollock LJ, 2020, TRENDS ECOL EVOL, V35, P1119, DOI 10.1016/j.tree.2020.08.015
CR  - R Core Team, 2019, R LANG ENV STAT COMP
CR  - Rapacciuolo G, 2019, GLOBAL ECOL BIOGEOGR, V28, P54, DOI 10.1111/geb.12848
CR  - Richards C, 2021, GLOBAL ECOL BIOGEOGR, V30, P973, DOI 10.1111/geb.13279
CR  - Rivers MC, 2011, BIOL CONSERV, V144, P2541, DOI 10.1016/j.biocon.2011.07.014
CR  - Rolland J, 2016, GLOBAL ECOL BIOGEOGR, V25, P1252, DOI 10.1111/geb.12482
CR  - Rondinini C, 2014, CONSERV LETT, V7, P126, DOI 10.1111/conl.12040
CR  - Schmidt M, 2017, PHYTOTAXA, V304, P1, DOI 10.11646/phytotaxa.304.1.1
CR  - Smiley TM, 2020, GLOBAL ECOL BIOGEOGR, V29, P516, DOI 10.1111/geb.13050
CR  - Tingley R, 2016, GLOBAL ECOL BIOGEOGR, V25, P1050, DOI 10.1111/geb.12462
CR  - Topel M, 2017, SYST BIOL, V66, P145, DOI 10.1093/sysbio/syw064
CR  - Varela S, 2014, ECOGRAPHY, V37, P1084, DOI 10.1111/j.1600-0587.2013.00441.x
CR  - Venter O, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.67
CR  - Walker BE, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00520
CR  - Walsh JC, 2012, GLOBAL ECOL BIOGEOGR, V21, P841, DOI 10.1111/j.1466-8238.2011.00724.x
CR  - Wickham H., 2021, R PACKAGES, V2nd ed.
CR  - Wickham H, 2011, R J, V3, P5
CR  - Wickham H, 2009, USE R, P1, DOI 10.1007/978-0-387-98141-3_1
CR  - Xie Y., 2020, GEN PURPOSE PACKAGE
CR  - Zizka A, 2021, ECOGRAPHY, DOI 10.1111/ecog.05557
CR  - Zizka A, 2021, J BIOGEOGR, V48, P2715, DOI 10.1111/jbi.14256
CR  - Zizka A, 2021, ECOGRAPHY, V44, P25, DOI 10.1111/ecog.05102
CR  - Zizka A, 2020, PEERJ, V8, DOI 10.7717/peerj.9916
CR  - Zizka A, 2021, CONSERV BIOL, V35, P897, DOI 10.1111/cobi.13616
CR  - Zizka A, 2019, METHODS ECOL EVOL, V10, P744, DOI 10.1111/2041-210X.13152
CR  - Zotz G, 2021, ECOLOGY, V102, DOI 10.1002/ecy.3326
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - FEB
PY  - 2022
VL  - 28
IS  - 2
SP  - 227
EP  - 241
DO  - 10.1111/ddi.13450
AN  - WOS:000730609300001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  68
ER  -

TY  - JOUR
AU  - Lopez-Jimenez, E
AU  - Vasquez-Gomez, JI
AU  - Sanchez-Acevedo, MA
AU  - Herrera-Lozada, JC
AU  - Uriarte-Arcia, AV
TI  - Columnar cactus recognition in aerial images using a deep learning approach
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Deep learning
KW  - Cactus
KW  - Arid land
KW  - Environmental conservation
KW  - Drones
KW  - TREE SPECIES CLASSIFICATION
KW  - SHAPE-FEATURES
KW  - UAV
KW  - FORESTS
AB  - Tehuacan-Cuicatlan Valley is a semi-arid zone in the south of Mexico. It was inscribed in the World Heritage List by the UNESCO in 2018. This unique area has wide biodiversity including several endemic plants. Unfortunately, human activity is constantly affecting the area. A way to preserve a protected area is to carry out autonomous surveillance of the area. A first step to reach this autonomy is to automatically detect and recognize elements in the area. In this work, we present a deep learning based approach for columnar cactus recognition, specifically, the Neobuxbaumia tetetzo species, endemic of the Valley. An image dataset was generated for this study by our research team, containing more than 10,000 image examples. The proposed approach uses this dataset to train a modified LeNet-5 Convolutional Neural Network. Experimental results have shown a high recognition accuracy, 0.95 for the validation set, validating the use of the approach for columnar cactus recognition.
AD  - Consejo Nacl Ciencia & Tecnol CONACYT, Mexico City, DF, MexicoAD  - CIDETEC, IPN, Mexico City, DF, MexicoAD  - Univ Canada UNCA, Teotitlan, Oaxaca, MexicoC3  - Instituto Politecnico Nacional - MexicoFU  - Consejo Nacional de Ciencia y Tecnologia (CONACYT), Mexico [1507 (Catedra 1507)]
FX  - The authors thank to Eduardo Armas Garcia, Rafael Cano Martinez and Luis Cresencio Mota Carrera for their help during the dataset labeling. This work was partially supported by Consejo Nacional de Ciencia y Tecnologia (CONACYT), Mexico, grant 1507 (Catedra 1507).
CR  - Adam E, 2010, WETL ECOL MANAG, V18, P281, DOI 10.1007/s11273-009-9169-z
CR  - Alonzo M, 2014, REMOTE SENS ENVIRON, V148, P70, DOI 10.1016/j.rse.2014.03.018
CR  - Cabreira TM, 2018, IEEE ROBOT AUTOM LET, V3, P3662, DOI 10.1109/LRA.2018.2854967
CR  - Cao L, 2016, INT J APPL EARTH OBS, V49, P39, DOI 10.1016/j.jag.2016.01.007
CR  - Carrillo LRG, 2011, J INTELL ROBOT SYST, V61, P103, DOI 10.1007/s10846-010-9472-1
CR  - Chaki J, 2015, PATTERN RECOGN LETT, V58, P61, DOI 10.1016/j.patrec.2015.02.010
CR  - Comba L, 2015, COMPUT ELECTRON AGR, V114, P78, DOI 10.1016/j.compag.2015.03.011
CR  - Cope JS, 2012, EXPERT SYST APPL, V39, P7562, DOI 10.1016/j.eswa.2012.01.073
CR  - Dalponte M, 2013, IEEE T GEOSCI REMOTE, V51, P2632, DOI 10.1109/TGRS.2012.2216272
CR  - Diederik P. Kingma, 2015, INT C LEARN REPR ICL
CR  - Ghazi MM, 2017, NEUROCOMPUTING, V235, P228, DOI 10.1016/j.neucom.2017.01.018
CR  - Goodfellow I., 2016, DEEP LEARNING, V1
CR  - Grinblat GL, 2016, COMPUT ELECTRON AGR, V127, P418, DOI 10.1016/j.compag.2016.07.003
CR  - HUNT D., 2006, NEW CACTUS LEXICON D
CR  - Ishida T, 2018, COMPUT ELECTRON AGR, V144, P80, DOI 10.1016/j.compag.2017.11.027
CR  - Kerkech M, 2018, COMPUT ELECTRON AGR, V155, P237, DOI 10.1016/j.compag.2018.10.006
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lee SH, 2015, IEEE IMAGE PROC, P452, DOI 10.1109/ICIP.2015.7350839
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Munisami T, 2015, PROCEDIA COMPUT SCI, V58, P740, DOI 10.1016/j.procs.2015.08.095
CR  - Naresh YG, 2016, NEUROCOMPUTING, V173, P1789, DOI 10.1016/j.neucom.2015.08.090
CR  - Nex F, 2014, APPL GEOMAT, V6, P1, DOI 10.1007/s12518-013-0120-x
CR  - Sladojevic S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3289801
CR  - Smith C. E., 1965, AGR TEHUACAN VALLEY, V31
CR  - Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
CR  - Tapia B. P. V., 2009, TECH REP
CR  - Unesco, 2019, TEH CUIC VALL OR HAB
CR  - Valiente-Banuet A., 2000, B SOC BOT MEX, V67, P25, DOI [10.17129/botsci.1625, DOI 10.17129/BOTSCI.1625]
CR  - Vasquez-Gomez J. I., 2017, INT C MECH EL AUT EN
CR  - Vasquez-Gomez J. I., 2019, CACTUS AERIAL PHOTOS
CR  - Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
CR  - Xiao QG, 2018, ECOL INFORM, V48, P117, DOI 10.1016/j.ecoinf.2018.09.001
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - JUL
PY  - 2019
VL  - 52
SP  - 131
EP  - 138
DO  - 10.1016/j.ecoinf.2019.05.005
AN  - WOS:000472984800014
N1  - Times Cited in Web of Science Core Collection:  14
Total Times Cited:  16
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Tabak, MA
AU  - Norouzzadeh, MS
AU  - Wolfson, DW
AU  - Sweeney, SJ
AU  - Vercauteren, KC
AU  - Snow, NP
AU  - Halseth, JM
AU  - Di Salvo, PA
AU  - Lewis, JS
AU  - White, MD
AU  - Teton, B
AU  - Beasley, JC
AU  - Schlichting, PE
AU  - Boughton, RK
AU  - Wight, B
AU  - Newkirk, ES
AU  - Ivan, JS
AU  - Odell, EA
AU  - Brook, RK
AU  - Lukacs, PM
AU  - Moeller, AK
AU  - Mandeville, EG
AU  - Clune, J
AU  - Miller, RS
TI  - Machine learning to classify animal species in camera trap images: Applications in ecology
T2  - METHODS IN ECOLOGY AND EVOLUTION
LA  - English
KW  - artificial intelligence
KW  - camera trap
KW  - convolutional neural network
KW  - deep neural networks
KW  - image classification
KW  - machine learning
KW  - r package
KW  - remote sensing
AB  - Motion-activated cameras ("camera traps") are increasingly used in ecological and management studies for remotely observing wildlife and are amongst the most powerful tools for wildlife research. However, studies involving camera traps result in millions of images that need to be analysed, typically by visually observing each image, in order to extract data that can be used in ecological analyses. We trained machine learning models using convolutional neural networks with the ResNet-18 architecture and 3,367,383 images to automatically classify wildlife species from camera trap images obtained from five states across the United States. We tested our model on an independent subset of images not seen during training from the United States and on an out-of-sample (or "out-of-distribution" in the machine learning literature) dataset of ungulate images from Canada. We also tested the ability of our model to distinguish empty images from those with animals in another out-of-sample dataset from Tanzania, containing a faunal community that was novel to the model. The trained model classified approximately 2,000 images per minute on a laptop computer with 16 gigabytes of RAM. The trained model achieved 98% accuracy at identifying species in the United States, the highest accuracy of such a model to date. Out-of-sample validation from Canada achieved 82% accuracy and correctly identified 94% of images containing an animal in the dataset from Tanzania. We provide an r package (Machine Learning for Wildlife Image Classification) that allows the users to (a) use the trained model presented here and (b) train their own model using classified images of wildlife from their studies. The use of machine learning to rapidly and accurately classify wildlife in camera trap images can facilitate non-invasive sampling designs in ecological studies by reducing the burden of manually analysing images. Our r package makes these methods accessible to ecologists.
AD  - USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO 80526 USAAD  - Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USAAD  - Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USAAD  - USDA, Natl Wildlife Res Ctr, Ft Collins, CO USAAD  - Arizona State Univ, Coll Integrat Sci & Arts, Mesa, AZ USAAD  - Tejon Ranch Conservancy, Lebec, CA USAAD  - Univ Georgia, Savannah River Ecol Lab, Warnell Sch Forestry & Nat Resources, Aiken, SC USAAD  - Univ Florida, Wildlife Ecol & Conservat, Range Cattle Res & Educ Ctr, Ona, FL USAAD  - Colorado Pk & Wildlife, Ft Collins, CO USAAD  - Univ Saskatchewan, Dept Anim & Poultry Sci, Saskatoon, SK, CanadaAD  - Univ Montana, WA Franke Coll Forestry & Conservat, Dept Ecosyst & Conservat Sci, Wildlife Biol Program, Missoula, MT 59812 USAAD  - Univ Wyoming, Dept Bot, Laramie, WY 82071 USAC3  - United States Department of Agriculture (USDA)C3  - University of WyomingC3  - University of WyomingC3  - United States Department of Agriculture (USDA)C3  - Arizona State UniversityC3  - United States Department of Energy (DOE)C3  - Savannah River Ecology LaboratoryC3  - University System of GeorgiaC3  - University of GeorgiaC3  - State University System of FloridaC3  - University of FloridaC3  - University of SaskatchewanC3  - University of Montana SystemC3  - University of MontanaC3  - University of WyomingFU  - U.S. Department of Energy [DE-EM0004391]; USDA Animal and Plant Health Inspection Service, National Wildlife Research Center and Center for Epidemiology and Animal Health; Canadian Natural Science and Engineering Research Council; University of Saskatchewan; Idaho Department of Game and Fish
FX  - U.S. Department of Energy, Grant/Award Number: DE-EM0004391; USDA Animal and Plant Health Inspection Service, National Wildlife Research Center and Center for Epidemiology and Animal Health; Colorado Parks and Wildlife; Canadian Natural Science and Engineering Research Council; University of Saskatchewan; Idaho Department of Game and Fish
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Advanced Research Computing Center, 2012, MOUNT MOR IBM SYST 1
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Howe EJ, 2017, METHODS ECOL EVOL, V8, P1558, DOI 10.1111/2041-210X.12790
CR  - Kelly MJ, 2008, J MAMMAL, V89, P408, DOI 10.1644/06-MAMM-A-424R.1
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
CR  - Rovero F., 2013, HYSTRIX ITALIAN J MA, V24, P585
CR  - Scott AB, 2018, AVIAN DIS, V62, P65, DOI 10.1637/11761-101917-Reg.1
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - APR
PY  - 2019
VL  - 10
IS  - 4
SP  - 585
EP  - 590
DO  - 10.1111/2041-210X.13120
AN  - WOS:000463036400013
N1  - Times Cited in Web of Science Core Collection:  124
Total Times Cited:  125
Cited Reference Count:  17
ER  -

TY  - JOUR
AU  - Jia, L
AU  - Tian, Y
AU  - Zhang, JG
TI  - Domain-Aware Neural Architecture Search for Classifying Animals in Camera Trap Images
T2  - ANIMALS
LA  - English
KW  - camera trap images
KW  - convolutional neural network
KW  - neural architecture search
AB  - Simple Summary Camera traps acquire visual data in a non-disturbing and round-the-clock manner, so they are popular for ecological researchers observing wildlife. Each camera trap may record thousands of images of diverse species and bring about millions of images that need to be classified. Many methods have been proposed to classify camera trap images, but almost all methods rely on very deep convolutional neural networks that require intensive computational resources. Such resources may be unavailable and become formidable in cases where the surveillance area is large or becomes greatly expanded. We turn our attention to camera traps organized as groups, where each group produces images that are processed by the edge device with lightweight networks tailored for images produced by the group. To achieve this goal, we propose a method to automatically design networks deployable for edge devices with respect to given images. With the proposed method, researchers without any experience in designing neural networks can develop networks applicable for edge devices. Thus, camera trap images can be processed in a distributed manner through edge devices, lowering the costs of transferring and processing data accumulated at camera traps. Camera traps provide a feasible way for ecological researchers to observe wildlife, and they often produce millions of images of diverse species requiring classification. This classification can be automated via edge devices installed with convolutional neural networks, but networks may need to be customized per device because edge devices are highly heterogeneous and resource-limited. This can be addressed by a neural architecture search capable of automatically designing networks. However, search methods are usually developed based on benchmark datasets differing widely from camera trap images in many aspects including data distributions and aspect ratios. Therefore, we designed a novel search method conducted directly on camera trap images with lowered resolutions and maintained aspect ratios; the search is guided by a loss function whose hyper parameter is theoretically derived for finding lightweight networks. The search was applied to two datasets and led to lightweight networks tested on an edge device named NVIDIA Jetson X2. The resulting accuracies were competitive in comparison. Conclusively, researchers without knowledge of designing networks can obtain networks optimized for edge devices and thus establish or expand surveillance areas in a cost-effective way.
AD  - Beijing Forestry Univ, Sch Technol, Beijing 100083, Peoples R ChinaAD  - Changzhou Univ, Sch Microelect & Control Engn, Changzhou 213164, Peoples R ChinaC3  - Beijing Forestry UniversityC3  - Changzhou UniversityCR  - Bergstra J, 2012, J MACH LEARN RES, V13, P281
CR  - Castelblanco L.P., 2017, P 9 INT C MACH VIS N, P1
CR  - Chen JS, 2019, P IEEE, V107, P1655, DOI 10.1109/JPROC.2019.2921977
CR  - Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Dong XT, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3023706
CR  - Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
CR  - Falkner S., 2018, P 35 INT C MACH LEAR, P1
CR  - Follmann P., 2018, Pattern Recognition and Image Analysis, V28, P605, DOI 10.1134/S1054661818040107
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Jaafra Y, 2019, IMAGE VISION COMPUT, V89, P57, DOI 10.1016/j.imavis.2019.06.005
CR  - Janzen M, 2017, ENVIRON MONIT ASSESS, V189, DOI 10.1007/s10661-017-6206-x
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Krizhevsky A, 2009, LEARNING MULTIPLE LA
CR  - Li XL, 2018, SPRINGERBRIEF MATH, P1, DOI [10.1109/ICDCS.2018.00011, 10.1007/978-3-319-89617-5_1]
CR  - Li YH, 2018, CHIN CONTR CONF, P9021, DOI 10.23919/ChiCC.2018.8483963
CR  - Lin M, 2014, ICLR, P1
CR  - Loshchilov I., 2017, P INT C LEARN REPR
CR  - Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
CR  - Moore HA, 2020, WILDLIFE RES, V47, P326, DOI 10.1071/WR19159
CR  - Nair V., 2010, P 27 INT C MACH LEAR
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Novotny D., 2016, P BRIT MACH VIS C YO, P115
CR  - Pham H., 2018, P INT C MACH LEARN S, P4095
CR  - Randler C, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10112178
CR  - Real E, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P4780
CR  - Reddi S. J., 2018, P INT C LEARN REPR I, P1
CR  - Ren PZ, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3447582
CR  - Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Shang Y, 2005, ELECTRICAL ENGINEERING HANDBOOK, P367, DOI 10.1016/B978-012170960-0/50031-1
CR  - Sutskever I., 2013, INT C MACH LEARN ICM, DOI [DOI 10.1007/S00287-015-0911-Z, 10.5555/3042817.3043064]
CR  - Tabak MA, 2020, ECOL EVOL, V10, P10374, DOI 10.1002/ece3.6692
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tan MX, 2019, PR MACH LEARN RES, V97
CR  - Tekeli U, 2019, TURK J ELECTR ENG CO, V27, P2395, DOI 10.3906/elk-1808-130
CR  - Ulker B, 2020, PROCEEDINGS OF THE 23RD INTERNATIONAL WORKSHOP ON SOFTWARE AND COMPILERS FOR EMBEDDED SYSTEMS (SCOPES 2020), P48, DOI 10.1145/3378678.3391882
CR  - Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
CR  - Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
CR  - Whytock RC, 2021, METHODS ECOL EVOL, V12, P1080, DOI 10.1111/2041-210X.13576
CR  - WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1023/A:1022672621406
CR  - Xie S., 2017, P IEEE C COMP VIS PA, P1063
CR  - Xing YX, 2018, 2018 IEEE INTERNATIONAL SYMPOSIUM ON LOCAL AND METROPOLITAN AREA NETWORKS (LANMAN), P13, DOI 10.1109/LANMAN.2018.8475056
CR  - Yates R.C., 1974, CURVES THEIR PROPERT, V1st, P237
CR  - Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
CR  - Zagoruyko S., 2016, BMVC
CR  - Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
CR  - Zhong Z., 2019, THESIS U CHINESE ACA
CR  - Zhou Y, 2020, PROCEEDINGS OF 2020 IEEE 5TH INFORMATION TECHNOLOGY AND MECHATRONICS ENGINEERING CONFERENCE (ITOEC 2020), P1713, DOI 10.1109/ITOEC49072.2020.9141847
CR  - Zhu CB, 2017, IEEE INT CONF COMP V, P2860, DOI 10.1109/ICCVW.2017.337
CR  - Zoph B., 2017, INT C LEARNING REPRE, P1
CR  - Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
CR  - Zualkernan IA, 2020, 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT), P111, DOI 10.1109/GCAIOT51063.2020.9345858
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - FEB
PY  - 2022
VL  - 12
IS  - 4
DO  - 10.3390/ani12040437
AN  - WOS:000766440300001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  58
ER  -

TY  - JOUR
AU  - Cardoso, AS
AU  - Renna, F
AU  - Moreno-Llorca, R
AU  - Alcaraz-Segura, D
AU  - Tabik, S
AU  - Ladle, RJ
AU  - Vaz, AS
TI  - Classifying the content of social media images to support cultural ecosystem service assessments using deep learning models
T2  - ECOSYSTEM SERVICES
LA  - English
KW  - Computer vision
KW  - Convolutional neural networks
KW  - Culturomics
KW  - iEcology
KW  - Nature contributions to people
KW  - Transfer learning
KW  - PROTECTED AREA
KW  - CONSERVATION
KW  - IDENTIFICATION
KW  - PERCEPTIONS
KW  - BUTTERFLIES
KW  - FRAMEWORK
AB  - Crowdsourced social media data has become popular for assessing cultural ecosystem services (CES). Nevertheless, social media data analyses in the context of CES can be time consuming and costly, particularly when based on the manual classification of images or texts shared by people. The potential of deep learning for automating the analysis of crowdsourced social media content is still being explored in CES research. Here, we use freely available deep learning models, i.e., Convolutional Neural Networks, for automating the classification of natural and human (e.g., species and human structures) elements relevant to CES from Flickr and Wikiloc images. Our approach is developed for Peneda-Ger <^>es (Portugal) and then applied to Sierra Nevada (Spain). For Peneda-Ger <^>es, image classification showed promising results (F1-score ca. 80%), highlighting a preference for aesthetics appreciation by social media users. In Sierra Nevada, even though model performance decreased, it was still satisfactory (F1-score ca. 60%), indicating a predominance of people's pursuit for cultural heritage and spiritual enrichment. Our study shows great potential from deep learning to assist in the automated classification of human-nature interactions and elements from social media content and, by extension, for supporting researchers and stakeholders to decode CES distributions, benefits, and values.
AD  - CIBIO, Ctr Invest Biodiversidade Recursos Gen, InBIO Lab Associado, Campus Vairao, P-4485661 Porto, PortugalAD  - Univ Porto, Dept Biol, Fac Ciencias, P-4099002 Porto, PortugalAD  - CIBIO, BIOPOLIS Program Genom, Biodivers & Land Planning, Campus Vairao, P-4485661 Vairao, PortugalAD  - Univ Porto, Inst Telecomunicacoes, Fac Ciencias, Rua Campo Alegre, Porto, PortugalAD  - Univ Granada, Andalusian Inter Univ Inst Earth Syst Res IISTA, iEcolab, Avda Mediterraneo S N, Granada 18006, SpainAD  - Univ Granada, Fac Ciencias, Dpto Botan, Av Fuentenueva S N, Granada 18003, SpainAD  - Univ Almeria, Andalusian Ctr Assessment & Monitoring Global, Crta San Urbano S-N, Almeria 04120, SpainAD  - Univ Granada, Andalusian Res Inst Data Sci & Computat Intellige, Granada 18071, SpainAD  - Univ Granada, Andalusian Res Inst Data Sci & Computat Intellige, Dept Comp Sci & Artificial Intelligence, DaSCI, Granada 18071, SpainAD  - Univ Fed Alagoas, Inst Biol Sci & Hlth, Maceio, Alagoas, BrazilC3  - Universidade do PortoC3  - Universidade do PortoC3  - University of GranadaC3  - University of GranadaC3  - Universidad de AlmeriaC3  - University of GranadaC3  - University of GranadaC3  - Universidade Federal de AlagoasFU  - FCT -Portuguese Foundation for Science and Technology [2020.01175, CEECIND/CP1601/CT0009]; Spanish Ministry of Science and Innovation; Regional Ministry of Environment of Andalucia; European Union's ERDF; University of Granada [A-TIC-458-UGR18]; European Union [854248]; Ministerio de Ciencia, Innovaci 'on y Universidades (Spain) [FJC2018-038131-I]
FX  - ASC is supported by the FCT -Portuguese Foundation for Science and Technology through the 2021 PhD Research Studentships [grant reference 2021.05426.BD]. ASV, DAS, FR, RML and ST acknowledge support from the EarthCul Project (reference PID2020-118041GB-I00), funded by the Spanish Ministry of Science and Innovation. FR acknowledges national funds from the FCT -Portuguese Foundation for Science and Technology through the program Stimulus for Scientific Employment Individual Support [contract reference CEECIND/01970/2017. RMLL is supported by the collaboration agreement between the Regional Ministry of Environment of Andalucia and the University of Granada for the project "Global Change Observatory of Sierra Nevada". ST acknowledges support from European Union's ERDF and University of Granada through project DeepL-ISCO (A-TIC-458-UGR18). RJL is supported by European Union's Horizon 2020 research and innovation programme grant 854248. ASV acknowledges support from the Ministerio de Ciencia, Innovaci ' on y Universidades (Spain) through the 2018 Juan de la Cierva-Formaci ' on program [contract reference FJC2018-038131-I] and from the FCT -Portuguese Foundation for Science and Technology through the program Stimulus for Scientific Employment -Individual Support [contract reference 2020.01175.CEECIND/CP1601/CT0009]. This paper contributes to the GEO BON working group on Ecosystem Services.
CR  - Almryad AS, 2020, ENG SCI TECHNOL, V23, P189, DOI 10.1016/j.jestch.2020.01.006
CR  - Blicharska M, 2017, ECOSYST SERV, V23, P55, DOI 10.1016/j.ecoser.2016.11.014
CR  - Bragagnolo C, 2016, CONSERV SOC, V14, P163, DOI 10.4103/0972-4923.191161
CR  - Bubalo M, 2019, LANDSCAPE URBAN PLAN, V184, P101, DOI 10.1016/j.landurbplan.2019.01.001
CR  - Cai GY, 2015, LECT NOTES ARTIF INT, V9362, P159, DOI 10.1007/978-3-319-25207-0_14
CR  - Chan KMA, 2012, ECOL ECON, V74, P8, DOI 10.1016/j.ecolecon.2011.11.011
CR  - Cheng X, 2019, ECOSYST SERV, V37, DOI 10.1016/j.ecoser.2019.100925
CR  - Chollet F., 2015, KERAS
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Di Minin E, 2015, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00063
CR  - Dramsch J.S., 2018, SEG INT EXP 88 ANN M, P2036, DOI [10.1190/segam2018- 2996783.1, DOI 10.1190/SEGAM2018-2996783.1]
CR  - Ferreira AC, 2020, METHODS ECOL EVOL, V11, P1072, DOI 10.1111/2041-210X.13436
CR  - Fish R, 2016, ECOSYST SERV, V21, P208, DOI 10.1016/j.ecoser.2016.09.002
CR  - Fu JL, 2017, APSIPA TRANS SIGNAL, V6, DOI 10.1017/ATSIP.2017.12
CR  - Gliozzo G, 2016, ECOL SOC, V21, DOI 10.5751/ES-08436-210306
CR  - Goodness J, 2016, ECOL INDIC, V70, P597, DOI 10.1016/j.ecolind.2016.02.031
CR  - Gosal AS, 2020, ECOL INDIC, V117, DOI 10.1016/j.ecolind.2020.106638
CR  - Gosal AS, 2019, ECOSYST SERV, V38, DOI 10.1016/j.ecoser.2019.100958
CR  - Haines-Young R., 2018, COMMON INT CLASSIFIC
CR  - Hausmann A, 2018, CONSERV LETT, V11, DOI 10.1111/conl.12343
CR  - Havinga I, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-99282-0
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hsu H., 2005, ENCY BIOSTATISTICS, V6, DOI [10.1002/9780471462422.eoct969, DOI 10.1002/9780471462422.EOCT969, 10.1002/0470011815.b2a15112, DOI 10.1002/0470011815.B2A15112]
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - James K, 2020, METHODS ECOL EVOL, V11, P1509, DOI 10.1111/2041-210X.13473
CR  - Jaric I, 2020, TRENDS ECOL EVOL, V35, P630, DOI 10.1016/j.tree.2020.03.003
CR  - Jepson PR, 2017, BIOL CONSERV, V212, P183, DOI 10.1016/j.biocon.2017.03.032
CR  - King DB, 2015, ACS SYM SER, V1214, P1
CR  - Koblet O, 2020, LANDSCAPE URBAN PLAN, V197, DOI 10.1016/j.landurbplan.2020.103757
CR  - Ladle RJ, 2017, FRONT ECOL ENVIRON, V15, P290, DOI 10.1002/fee.1506
CR  - Ladle RJ, 2016, FRONT ECOL ENVIRON, V14, P270, DOI 10.1002/fee.1260
CR  - Langlois J, 2021, ECOL INDIC, V129, DOI 10.1016/j.ecolind.2021.107935
CR  - Li J., 2018, ADV NEURAL INFORM PR, P1586
CR  - Lusch B, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07210-0
CR  - Moreno-Llorca R, 2020, SCI TOTAL ENVIRON, V737, DOI 10.1016/j.scitotenv.2020.140067
CR  - Na B., 2020, ADV SCI TECHNOLOGY E, V5, P476
CR  - Nekola JC, 1999, J BIOGEOGR, V26, P867, DOI 10.1046/j.1365-2699.1999.00305.x
CR  - Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
CR  - Retka J, 2019, OCEAN COAST MANAGE, V176, P40, DOI 10.1016/j.ocecoaman.2019.04.018
CR  - Richards DR, 2018, ECOSYST SERV, V31, P318, DOI 10.1016/j.ecoser.2017.09.004
CR  - Richards DR, 2015, ECOL INDIC, V53, P187, DOI 10.1016/j.ecolind.2015.01.034
CR  - Riechers M, 2016, ECOSYST SERV, V17, P33, DOI 10.1016/j.ecoser.2015.11.007
CR  - Ros-Candeira A, 2020, NAT CONSERV-BULGARIA, P1, DOI 10.3897/natureconservation.38.38325
CR  - Santarem F, 2015, TOUR MANAG PERSPECT, V16, P190, DOI 10.1016/j.tmp.2015.07.019
CR  - Seresinhe CI, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.170170
CR  - Silva WA, 2021, APPL INTELL, V51, P396, DOI 10.1007/s10489-020-01805-1
CR  - Sitaula C, 2020, IEEE IJCNN
CR  - Vaz AS, 2020, CONSERV LETT, V13, DOI 10.1111/conl.12704
CR  - Srivastava S, 2018, PROCEEDINGS OF THE 2ND ACM SIGSPATIAL INTERNATIONAL WORKSHOP ON AI FOR GEOGRAPHIC KNOWLEDGE DISCOVERY (GEOAI 2018), P43, DOI 10.1145/3281548.3281559
CR  - Szegedy C, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4278
CR  - Tan MX, 2019, PR MACH LEARN RES, V97
CR  - Tenkanen H, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-18007-4
CR  - Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335
CR  - Theivaprakasham H, 2021, J ASIA-PAC ENTOMOL, V24, P329, DOI 10.1016/j.aspen.2020.11.015
CR  - Thorat P., 2020, P INT C COMP SCI APP, P343, DOI [10.1007/978-981-15-0790-8_33, DOI 10.1007/978-981-15-0790-8_33]
CR  - Toivonen T, 2019, BIOL CONSERV, V233, P298, DOI 10.1016/j.biocon.2019.01.023
CR  - van der Wal R, 2015, AMBIO, V44, pS517, DOI 10.1007/s13280-015-0701-5
CR  - Vigl LE, 2021, PEOPLE NAT, V3, P673, DOI 10.1002/pan3.10199
CR  - Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
CR  - Wartmann FM, 2021, LANDSCAPE ECOL, V36, P2347, DOI 10.1007/s10980-020-01181-8
CR  - Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011
CR  - Willcock S, 2018, ECOSYST SERV, V33, P165, DOI 10.1016/j.ecoser.2018.04.004
CR  - Ying X, 2019, J PHYS CONF SER, V1168, DOI 10.1088/1742-6596/1168/2/022022
CR  - Yoshimura N, 2017, ECOSYST SERV, V24, P68, DOI 10.1016/j.ecoser.2017.02.009
CR  - Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - APR
PY  - 2022
VL  - 54
DO  - 10.1016/j.ecoser.2022.101410
AN  - WOS:000777737000005
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  65
ER  -

TY  - JOUR
AU  - Yang, LH
AU  - Yan, J
AU  - Li, HR
AU  - Cao, XY
AU  - Ge, BJ
AU  - Qi, ZC
AU  - Yan, XL
TI  - Real-Time Classification of Invasive Plant Seeds Based on Improved YOLOv5 with Attention Mechanism
T2  - DIVERSITY-BASEL
LA  - English
KW  - weed seeds
KW  - seed identification
KW  - target detection
KW  - convolutional neural network
KW  - YOLOv5
AB  - Seeds of exotic plants transported with imported goods pose a risk of alien species invasion in cross-border transportation and logistics. It is critical to develop stringent inspection and quarantine protocols with active management to control the import and export accompanied by exotic seeds. As a result, a method for promptly identifying exotic plant seeds is urgently needed. In this study, we built a database containing 3000 images of seeds of 12 invasive plants and proposed an improved YOLOv5 target detection algorithm that incorporates a channel attention mechanism. Given that certain seeds in the same family and genus are very similar in appearance and are thus difficult to differentiate, we improved the size model of the initial anchor box to converge better; moreover, we introduce three attention modules, SENet, CBAM, and ECA-Net, to enhance the extraction of globally important features while suppressing the weakening of irrelevant features, thereby effectively solving the problem of automated inspection of similar species. Experiments on an invasive alien plant seed data set reveal that the improved network model fused with ECA-Net requires only a small increase in parameters when compared to the original YOLOv5 network model and achieved greater classification and detection accuracy without affecting detection speed.
AD  - Shanghai Chenshan Bot Garden, Eastern China Conservat Ctr Wild Endangered Plant, Shanghai 201602, Peoples R ChinaAD  - Zhejiang Sci Tech Univ, Zhejiang Prov Key Lab Plant Secondary Metab & Reg, Coll Life Sci & Med, Hangzhou 310018, Peoples R ChinaAD  - Zhejiang Sci Tech Univ, Shaoxing Acad Biomed, Shaoxing 312366, Peoples R ChinaC3  - Chinese Academy of SciencesC3  - Zhejiang Sci-Tech UniversityC3  - Zhejiang Sci-Tech UniversityFU  - Special Fund for Scientific Research of Shanghai Landscaping & City Appearance Administrative Bureau [G212405, G222403, ZWGX1902]; Special Fundamental Work of the Ministry of Science and Technology [2014FY120400]; Natural Science Foundation of Zhejiang Province [LY21C030008]; Open Fund of Shaoxing Academy of Biomedicine of Zhejiang Sci-Tech University [SXAB202020]
FX  - This research was funded by the Special Fund for Scientific Research of Shanghai Landscaping & City Appearance Administrative Bureau, grant numbers G212405, G222403; National Wild Plant Germplasm Resource Center, grant number ZWGX1902; Special Fundamental Work of the Ministry of Science and Technology, grant number 2014FY120400; the Natural Science Foundation of Zhejiang Province, grant number LY21C030008; and the Open Fund of Shaoxing Academy of Biomedicine of Zhejiang Sci-Tech University, grant number SXAB202020.
CR  - Bochkovskiy A., 2020, ARXIV
CR  - Chtioui Y, 1996, J SCI FOOD AGR, V71, P433, DOI [10.1002/(SICI)1097-0010(199608)71:4<433::AID-JSFA596>3.3.CO;2-2, 10.1002/(SICI)1097-0010(199608)71:4&lt;433::AID-JSFA596&gt;3.0.CO;2-B]
CR  - Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
CR  - Glorot X., 2011, AISTATS, P315, DOI DOI 10.1.1.208.6449
CR  - Hamerly G, 2004, ADV NEUR IN, V16, P281
CR  - He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
CR  - He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23
CR  - Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
CR  - Javanmardi S, 2021, J STORED PROD RES, V92, DOI 10.1016/j.jspr.2021.101800
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Kundu Nidhi, 2021, DSMLAI '21': Proceedings of the International Conference on Data Science, Machine Learning and Artificial Intelligence, P153, DOI 10.1145/3484824.3484913
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Loddo A, 2021, COMPUT ELECTRON AGR, V187, DOI 10.1016/j.compag.2021.106269
CR  - Luo T., 2021, INF PROCESS AGR, P2214, DOI [10.1016/j.inpa.2021.10.002, DOI 10.1016/J.INPA.2021.10.002]
CR  - Qilong Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11531, DOI 10.1109/CVPR42600.2020.01155
CR  - Redmon J., 2018, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.690
CR  - Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
CR  - Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
CR  - Winther O., 2015, ARXIV150905329
CR  - Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
CR  - Xu HG, 2006, BIODIVERS CONSERV, V15, P2893, DOI 10.1007/s10531-005-2575-5
CR  - Yan B, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13091619
CR  - Zhao JQ, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13163095
CR  - Zhu LL, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13183776
CR  - Zuo Y, 2017, J BIOSAF, V26, P307
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - APR
PY  - 2022
VL  - 14
IS  - 4
DO  - 10.3390/d14040254
AN  - WOS:000787413800001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Xie, JJ
AU  - Li, AQ
AU  - Zhang, JG
AU  - Cheng, ZA
TI  - An Integrated Wildlife Recognition Model Based on Multi-Branch Aggregation and Squeeze-And-Excitation Network
T2  - APPLIED SCIENCES-BASEL
LA  - English
KW  - wildlife recognition
KW  - SE-ResNeXt
KW  - deep convolutional neural network
AB  - Infrared camera trapping, which helps capture large volumes of wildlife images, is a widely-used, non-intrusive monitoring method in wildlife surveillance. This method can greatly reduce the workload of zoologists through automatic image identification. To achieve higher accuracy in wildlife recognition, the integrated model based on multi-branch aggregation and Squeeze-and-Excitation network is introduced. This model adopts multi-branch aggregation transformation to extract features, and uses Squeeze-and-Excitation block to adaptively recalibrate channel-wise feature responses based on explicit self-mapped interdependencies between channels. The efficacy of the integrated model is tested on two datasets: the Snapshot Serengeti dataset and our own dataset. From experimental results on the Snapshot Serengeti dataset, the integrated model applies to the recognition of 26 wildlife species, with the highest accuracies in Top-1 (when the correct class is the most probable class) and Top-5 (when the correct class is within the five most probable classes) at 95.3% and 98.8%, respectively. Compared with the ROI-CNN algorithm and ResNet (Deep Residual Network), on our own dataset, the integrated model, shows a maximum improvement of 4.4% in recognition accuracy.
AD  - Beijing Forestry Univ, Sch Technol, Beijing 100083, Peoples R ChinaAD  - Adm Forestry Equipment & Automat, Key Lab State Forestry & Grassland, Beijing 100083, Peoples R ChinaC3  - Beijing Forestry UniversityFU  - National Natural Science Foundation of China [31670553]; Natural Science Foundation of Beijing Municipality [6192019]; Fundamental Research Funds for the Central Universities [2016ZCQ08]; Baidu
FX  - This work is supported by the National Natural Science Foundation of China under Grant No. 31670553, the Natural Science Foundation of Beijing Municipality under Grant No. 6192019. and Fundamental Research Funds for the Central Universities under Grant No. 2016ZCQ08. The authors also thank Baidu for its financial support for the "Paddlepaddle-basedWildlife Identification and Classification System" project.
CR  - Chen S., 2017, MOD MANUF TECHNOL EQ, V3, P64
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - He K, 2016, ARXIV160305027, DOI DOI 10.1109/CVPR.2016.90
CR  - Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
CR  - Jiang J, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9091829
CR  - Kamencay P, 2016, 2016 ELEKTRO 11TH INTERNATIONAL CONFERENCE, P62, DOI 10.1109/ELEKTRO.2016.7512036
CR  - Kone I, 2018, LECT NOTES COMPUT SC, V10882, P796, DOI 10.1007/978-3-319-93000-8_90
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - LIU W, 2018, J BEIJING FOR UNIV, V40, P124, DOI DOI 10.3969/J.ISSN.1674-0858.2018.06.1
CR  - Manousakis Nikolaos M., 2016, 2016 Power Systems Computation Conference (PSCC), P1, DOI 10.1109/PSCC.2016.7540813
CR  - Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
CR  - McAllister P, 2018, COMPUT BIOL MED, V95, P217, DOI 10.1016/j.compbiomed.2018.02.008
CR  - Na L., 2011, NATURE MONITORING WI
CR  - Nair V., 2010, P 27 INT C MACH LEAR
CR  - Nigati K., 2019, J AGR MACH, V50, P217
CR  - O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
CR  - Owoeye K., 2018, P WORKSH MOD DEC MAK
CR  - Pawara P., 2016, P 2016 IEEE S SER CO, P1, DOI 10.1109/ SSCI.2016.7850111
CR  - Peng M, 2018, IEEE INT CONF AUTOMA, P790, DOI 10.1109/FG.2018.00127
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Szegedy C., 2015, 2015 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2015.7298594
CR  - Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
CR  - Tang YX, 2016, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2016.233
CR  - Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
CR  - Yunlong Y., 2018, COMPUT INTEL NEUROSC, V2018
CR  - Zhang K, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/8214975
CR  - Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - JUL 2
PY  - 2019
VL  - 9
IS  - 14
DO  - 10.3390/app9142794
AN  - WOS:000479026900022
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  28
ER  -

TY  - CPAPER
AU  - Ferreira, BA
AU  - de Villiers, JP
AU  - de Freitas, A
A1  - IEEE
TI  - Dashcam based wildlife detection and classification using fused data sets of digital photographic and simulated imagery
T2  - PROCEEDINGS OF 2020 23RD INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2020)
LA  - English
CP  - 23rd International Conference on Information Fusion (FUSION)
KW  - convolutional neural network
KW  - wildlife detection
KW  - transfer learning
KW  - sim-to-real
KW  - automatic data collection
AB  - In this paper, data from a simulated data set were fused with a significantly smaller measured data set for the purpose of training a deep neural network to identify animals from the footage of a dash cam. The trained networks were used to detect wildlife in an environment similar to game reserves in South Africa. To enable the automatic collection of data for the experiment, a simulated environment was created to simulate four classes of wildlife found in South Africa: buffalo, elephants, rhino and zebra. The network structure for the detector network selected was an adapted version of the tiny YOLOv3 network.
   It was discovered that using transfer learning and fine tuning resulted in two models with higher accuracy of 82.59% and 86.64% mAP@0.5 respectively than models where no transfer learning was used. The results were achieved when tested on a testing set of digital photographic images. These networks, initialised using transfer learning, were also faster and easier to train than training using a combined data set of photographic and simulated images from scratch. The simulated environment can however not replace real-life data, as was proven by an accuracy of no better than chance for the model trained using only simulated data.
AD  - Univ Pretoria, Dept Elect Elect & Comp Engn, Pretoria, South AfricaAD  - CSIR, Pretoria, South AfricaC3  - University of PretoriaC3  - Council for Scientific & Industrial Research (CSIR) - South AfricaCR  - AlexeyAB, 2019, WINDOWS LINUX VERSIO
CR  - [Anonymous], INT J COMPUT VISION
CR  - Babbar R, 2019, MACH LEARN, V108, P1329, DOI 10.1007/s10994-019-05791-5
CR  - Bird J. J., 2020, 10 INT C INT SYST
CR  - Ferreira b, 2020, AFRICAN WILDLIFE
CR  - Ferreira B, 2020, WILDLIFE SIMULATOR
CR  - Ferreira B, 2020, SIMULATED AFRICAN WI
CR  - Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
CR  - Gopalakrishnan K, 2017, CONSTR BUILD MATER, V157, P322, DOI 10.1016/j.conbuildmat.2017.09.110
CR  - Kim J., 2017, P IEEE C COMP VIS PA, P30
CR  - Koltun V, 2017, ARXIV PREPRINT ARXIV
CR  - Kovaceva J, 2020, ACCIDENT ANAL PREV, V136, DOI 10.1016/j.aap.2019.105352
CR  - Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
CR  - Lee Kangwook, 2017, ICML WORKSH MACH LEA
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Muller M, 2018, INT J COMPUT VISION, V126, P902, DOI 10.1007/s11263-018-1073-7
CR  - Pharr M., 2016, PHYS BASED RENDERING
CR  - Rajpura Param S., 2017, OBJECT DETECTION USI
CR  - Redmon J., 2018, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.690
CR  - Redmon J., 2016, DARKNET OPEN SOURCE
CR  - Redmon J., 2016, 2016 IEEE C COMP VIS, P779
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
CR  - Su Y.-C., 2015, TRANSFER LEARNING VI
CR  - Wang L, 2020, SUSTAIN CITIES SOC, V54, DOI 10.1016/j.scs.2019.102002
CR  - Wang M, 2018, NEUROCOMPUTING, V312, P135, DOI 10.1016/j.neucom.2018.05.083
CR  - Wong KI, 2013, APPL SOFT COMPUT, V13, P4428, DOI 10.1016/j.asoc.2013.06.006
CR  - Wu H, 2020, COMPUT CHEM ENG, V135, DOI 10.1016/j.compchemeng.2020.106731
CR  - Yosinski J, 2014, ADV NEUR IN, V27
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2020
SP  - 248
EP  - 255
AN  - WOS:000659928700034
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  29
ER  -

TY  - JOUR
AU  - Deng, X
AU  - Kim, IT
AU  - Shen, C
TI  - Research on Convolutional Neural Network-Based Virtual Reality Platform Framework for the Intangible Cultural Heritage Conservation of China Hainan Li Nationality: Boat-Shaped House as an Example
T2  - MATHEMATICAL PROBLEMS IN ENGINEERING
LA  - English
AB  - Hainan is located at the southernmost tip of China, since ancient times it has always occupied an important position on the Silk Road. Hainan culture is dominated by minority and marine cultures and has a rich intangible cultural heritage. Hainan has always been committed to the development and utilization of its wide cultural heritage, and the development direction is mainly based on live display and folk activities. In May 2020, the Chinese government announced the establishment of the Hainan Free Trade Port Policy and System, the establishment of a Hainan International Free Trade Zone, and the development of tourism, modern services, and high-tech industry. All these put forward higher requirements for the protection of Hainan's cultural heritage, not just traditional ways to protect and promote, but also to use the dividends of current scientific and technological development to keep up with the times to protect and promote. The integration of digital technology will be the development direction of cultural heritage and intangible cultural heritage. This paper enwnerates and analyzes other cases and academic directions of intangible cultural heritage, combined with the present situation of intangible cultural heritage in Hainan. It also analyzes the predicament of handiwork inheritance in Hainan intangible cultural heritage, expounds the structure, humanistic connotation, and construction skills of Li nationality ship house, and summarizes the role of a novel deep learning convolutional neural network- (CNN-) based virtual reality framework of intangible cultural heritage conservation in promoting the intangible cultural heritage of traditional skills. It also puts forward the scheme and heritage conservation virtual reality content construction and provides the process of building a virtual reality platform for the intangible cultural heritage of ship-shaped houses, which as an example can be used as a reference for intangible cultural heritage researchers in other areas. At the same time, it fills the gap for the artificial intelligence-based digitization of the intangible cultural heritage.
AD  - Hainan Univ, Coll Fine Art & Design, Haikou, Hainan, Peoples R ChinaAD  - Chosun Univ, Art & Sports Coll, Dept Art, Gwangju, South KoreaAD  - Hainan Univ, State Key Lab Marine Resource Utilizat, Haikou, Hainan, Peoples R ChinaC3  - Hainan UniversityC3  - Chosun UniversityC3  - Hainan UniversityFU  - Key Research and Development projects of Hainan Province, "Digital Research on the Intangible Cultural Heritage of Hainan Li Nationality under Virtual Reality Technology" [ZDYF2019017]
FX  - This paper was supported by the Key Research and Development projects of Hainan Province, "Digital Research on the Intangible Cultural Heritage of Hainan Li Nationality under Virtual Reality Technology," project number: ZDYF2019017.
CR  - Doulamis A, 2017, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISIGRAPP 2017), VOL 5, P451, DOI 10.5220/0006347304510460
CR  - Grau O., 2002, VIRTUAL ART ILLUSION, DOI [10.7551/mitpress/7104.001.0001, DOI 10.7551/MITPRESS/7104.001.0001]
CR  - Guo J., P 2009 11 IEEE INT C, DOI [10.1109/CADCG.2009.5246882, DOI 10.1109/CADCG.2009.5246882]
CR  - Hedman P, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130828
CR  - JunLi Li, 2020, Journal of Physics: Conference Series, V1533, DOI 10.1088/1742-6596/1533/3/032011
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Leimgruber W, 2010, J FOLKLORE RES, V47, P161, DOI 10.2979/JFR.2010.47.1-2.161
CR  - Massing K, 2018, INT J HERIT STUD, V24, P66, DOI 10.1080/13527258.2017.1362571
CR  - McLay A, 2012, INT J SOCIOTECHNOLOG, V2, P37, DOI [10.4018/978-1-4666-0200-7.ch010, DOI 10.4018/978-1-4666-0200-7.CH010]
CR  - Portman ME, 2015, COMPUT ENVIRON URBAN, V54, P376, DOI 10.1016/j.compenvurbsys.2015.05.001
CR  - Rhee T, 2017, IEEE T VIS COMPUT GR, V23, P1302, DOI 10.1109/TVCG.2017.2657178
CR  - Schroers C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3225150
CR  - Soccini AM, 2017, P IEEE VIRT REAL ANN, P413, DOI 10.1109/VR.2017.7892352
CR  - Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
CR  - Tamborrino R, 2017, J INST CONSERV, V40, P168, DOI 10.1080/19455224.2017.1321562
CR  - Teo T, 2016, INTERACT LEARN ENVIR, V24, P745, DOI 10.1080/10494820.2014.917110
CR  - Wang M, 2020, COMPUT VIS MEDIA, V6, P3, DOI 10.1007/s41095-020-0162-z
CR  - Wang M, 2018, IEEE T IMAGE PROCESS, V27, P5854, DOI 10.1109/TIP.2018.2859628
CR  - Wang Q., P 2018 INT C INT TRA, DOI [10.1109/ICITBS.2018.00082, DOI 10.1109/ICITBS.2018.00082]
CR  - Ye JY, 2018, SIGNAL PROCESS-IMAGE, V68, P258, DOI 10.1016/j.image.2018.04.016
CR  - Zhang Y, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0366-7
CR  - Zhang Y, 2021, IEEE T VIS COMPUT GR, V27, P3198, DOI 10.1109/TVCG.2020.2965097
CR  - Zhao ZH, 2017, 2017 INTERNATIONAL CONFERENCE ON ROBOTS & INTELLIGENT SYSTEM (ICRIS), P135, DOI 10.1109/ICRIS.2017.41
CR  - Zhu Z, 2018, IEEE T IMAGE PROCESS, V27, P2952, DOI 10.1109/TIP.2018.2808766
PU  - HINDAWI LTD
PI  - LONDON
PA  - ADAM HOUSE, 3RD FLR, 1 FITZROY SQ, LONDON, W1T 5HF, ENGLAND
DA  - MAR 28
PY  - 2021
VL  - 2021
DO  - 10.1155/2021/5538434
AN  - WOS:000637367500006
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  24
ER  -

TY  - JOUR
AU  - Martinez, LA
AU  - Mortelmans, J
AU  - Dillen, N
AU  - Debusschere, E
AU  - Deneudt, K
TI  - LifeWatch observatory data: phytoplankton observations in the Belgian Part of the North Sea
T2  - BIODIVERSITY DATA JOURNAL
LA  - English
KW  - phytoplankton
KW  - Belgium
KW  - marine
KW  - LifeWatch Belgium
KW  - FlowCAM
KW  - image recognition
KW  - HARMFUL ALGAL BLOOMS
KW  - COMMUNITY COMPOSITION
KW  - LONG-TERM
KW  - PLANKTON
KW  - IMPACT
AB  - Background
   This paper describes a phytoplankton data series generated through systematic observations in the Belgian Part of the North Sea (BPNS). Phytoplankton samples were collected during multidisciplinary sampling campaigns, visiting nine nearshore stations with monthly frequency and an additional eight offshore stations on a seasonal basis.
   New information
   The data series contain taxon-specific phytoplankton densities determined by analysis with the Flow Cytometer And Microscope (FlowCAM (R)) and associated image-based classification. The classification is performed by two separate semi-automated classification systems, followed by manual validation by taxonomic experts. To date, 637,819 biological particles have been collected and identified, yielding a large dataset of validated phytoplankton images. The collection and processing of the 2017-2018 dataset are described, along with its data curation, quality control and data storage. In addition, the classification of images using image classification algorithms, based on convolutional neural networks (CNN) from 2019 onwards, is also described. Data are published in a standardised format together with environmental parameters, accompanied by extensive metadata descriptions and finally labelled with digital identifiers for traceability. The data are published under a CC-BY 4.0 licence, allowing the use of the data under the condition of providing the reference to the source.
AD  - Flanders Marine Inst VLIZ, Wandelaarkaai 7, Oostende, BelgiumAD  - Univ Ghent, Dept Biol, Lab Protistol & Aquat Ecol, Ghent, BelgiumC3  - Ghent UniversityFU  - Research Foundation Flanders (FWO)
FX  - Funding for the data collection and management is provided by the Research Foundation Flanders (FWO) in the framework of the Flemish contribution to LifeWatch, which is a landmark European Research Infrastructures on the European Strategy Forum on Research (ESFRI) roadmap. Scientists and RV Simon Stevin crew joining the LifeWatch sampling campaigns are acknowledged for their practical support. The authors thank the Flemish Ministry of Mobility and Public Works (VLOOT) for operating the RV Simon Stevin and facilitating the surveys. We thank the reviewer for the very helpful comments.
CR  - Alvarez E, 2014, J PLANKTON RES, V36, P170, DOI 10.1093/plankt/fbt069
CR  - Alvarez E, 2012, J PLANKTON RES, V34, P454, DOI 10.1093/plankt/fbs017
CR  - Alvarez E, 2011, J PLANKTON RES, V33, P1119, DOI 10.1093/plankt/fbr012
CR  - Anderson DM, 2012, ANNU REV MAR SCI, V4, P143, DOI 10.1146/annurev-marine-120308-081121
CR  - [Anonymous], 2020, WORLD REGISTER MARIN
CR  - Benfield MC, 2007, OCEANOGRAPHY, V20, P172, DOI 10.5670/oceanog.2007.63
CR  - Breton E, 2006, LIMNOL OCEANOGR, V51, P1401, DOI 10.4319/lo.2006.51.3.1401
CR  - Buesseler KO, 2007, SCIENCE, V316, P567, DOI 10.1126/science.1137959
CR  - Camoying MG, 2016, LIMNOL OCEANOGR-METH, V14, P305, DOI 10.1002/lom3.10090
CR  - De Pooter D, 2017, BIODIVERS DATA J, V5, DOI 10.3897/BDJ.5.e10989
CR  - Edwards M, 2001, ICES J MAR SCI, V58, P39, DOI 10.1006/jmsc.2000.0987
CR  - Edwards M, 2010, TRENDS ECOL EVOL, V25, P602, DOI 10.1016/j.tree.2010.07.007
CR  - Emeis KC, 2015, J MARINE SYST, V141, P18, DOI 10.1016/j.jmarsys.2014.03.012
CR  - Field CB, 1998, SCIENCE, V281, P237, DOI 10.1126/science.281.5374.237
CR  - Gasparini S, 2000, J SEA RES, V43, P345, DOI 10.1016/S1385-1101(00)00016-2
CR  - Graham MD, 2018, LIMNOL OCEANOGR-METH, V16, P669, DOI 10.1002/lom3.10274
CR  - HALLEGRAEFF GM, 1993, PHYCOLOGIA, V32, P79, DOI 10.2216/i0031-8884-32-2-79.1
CR  - Haraguchi L, 2018, FRONT MAR SCI, V5, DOI 10.3389/fmars.2018.00272
CR  - Hutchins DA, 2017, NAT MICROBIOL, V2, DOI 10.1038/nmicrobiol.2017.58
CR  - Kraberg A., 2010, COASTAL PHYTOPLANKTO
CR  - Lacroix G, 2004, J SEA RES, V52, P149, DOI 10.1016/j.seares.2004.01.003
CR  - Lancelot C, 1987, OCEANOGRAPHIC LIT RE, V34, DOI [10.1016/0198-0254(87)90379-7, DOI 10.1016/0198-0254(87)90379-7]
CR  - Lee AJ, 1980, PHYS CHEM OCEANOGR B, P467, DOI 10.1016/s0422-9894(08)71359-x
CR  - Lloret L, 2018, BIODIVERS INF SCI ST, V2, DOI [10.3897/biss.2.25762, DOI 10.3897/BISS.2.25762]
CR  - LUND J. W. G., 1958, HYDROBIOLOGIA, V11, P143, DOI 10.1007/BF00007865
CR  - MARGALEF R, 1978, OCEANOL ACTA, V1, P493
CR  - Muelbert JH, 2019, FRONT MAR SCI, V6, DOI 10.3389/fmars.2019.00527
CR  - Muylaert K, 2006, J SEA RES, V55, P253, DOI 10.1016/j.seares.2005.12.002
CR  - Muylaert K, 2009, ESTUAR COAST SHELF S, V82, P335, DOI 10.1016/j.ecss.2009.01.024
CR  - Nohe A, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.126
CR  - Poulton NJ, 2010, MICROSCOPIC MOL METH, V47
CR  - Richardson AJ, 2004, SCIENCE, V305, P1609, DOI 10.1126/science.1100958
CR  - Sieracki CK, 1998, MAR ECOL PROG SER, V168, P285, DOI 10.3354/meps168285
CR  - Suikkanen S, 2007, ESTUAR COAST SHELF S, V71, P580, DOI 10.1016/j.ecss.2006.09.004
CR  - Tomas C.R., 1997, IDENTIFYING MARINE P
CR  - Wells ML, 2020, HARMFUL ALGAE, V91, DOI 10.1016/j.hal.2019.101632
CR  - Zingone A, 2015, ESTUAR COAST SHELF S, V162, P151, DOI 10.1016/j.ecss.2015.05.024
PU  - PENSOFT PUBLISHERS
PI  - SOFIA
PA  - 12 PROF GEORGI ZLATARSKI ST, SOFIA, 1700, BULGARIA
DA  - DEC 16
PY  - 2020
VL  - 8
DO  - 10.3897/BDJ.8.e57236
AN  - WOS:000600452700001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  37
ER  -

TY  - JOUR
AU  - Mannocci, L
AU  - Villon, S
AU  - Chaumont, M
AU  - Guellati, N
AU  - Mouquet, N
AU  - Iovan, C
AU  - Vigliola, L
AU  - Mouillot, D
TI  - Leveraging social media and deep learning to detect rare megafauna in video surveys
T2  - CONSERVATION BIOLOGY
LA  - English
KW  - convolutional neural networks
KW  - endangered megafauna
KW  - internet ecology
KW  - monitoring
KW  - species detection
KW  - deteccion de especies
KW  - ecologia de internet
KW  - megafauna en peligro
KW  - monitoreo
KW  - redes neurales convolucionales
AB  - Deep learning has become a key tool for the automated monitoring of animal populations with video surveys. However, obtaining large numbers of images to train such models is a major challenge for rare and elusive species because field video surveys provide few sightings. We designed a method that takes advantage of videos accumulated on social media for training deep-learning models to detect rare megafauna species in the field. We trained convolutional neural networks (CNNs) with social media images and tested them on images collected from field surveys. We applied our method to aerial video surveys of dugongs (Dugong dugon) in New Caledonia (southwestern Pacific). CNNs trained with 1303 social media images yielded 25% false positives and 38% false negatives when tested on independent field video surveys. Incorporating a small number of images from New Caledonia (equivalent to 12% of social media images) in the training data set resulted in a nearly 50% decrease in false negatives. Our results highlight how and the extent to which images collected on social media can offer a solid basis for training deep-learning models for rare megafauna detection and that the incorporation of a few images from the study site further boosts detection accuracy. Our method provides a new generation of deep-learning models that can be used to rapidly and accurately process field video surveys for the monitoring of rare megafauna.
AD  - Univ Montpellier, IFREMER, CNRS, MARBEC,IRD, Pl Eugene Bataillon,Bat 24,CC093, F-34095 Montpellier 5, FranceAD  - Univ La Reunion, Univ Nouvelle Caledonie, Ctr IRD Noumea, Lab Excellence LABEX Corail,ENTROPIE IRD,CNRS,Ifr, Noumea, New CaledoniaAD  - Univ Montpellier, CNRS, LIRMM, Montpellier, FranceAD  - Univ Nimes, Nimes, FranceAD  - FRB CESAB, Montpellier, FranceAD  - Inst Univ France, Paris, FranceC3  - Centre National de la Recherche Scientifique (CNRS)C3  - IfremerC3  - Institut de Recherche pour le Developpement (IRD)C3  - Universite de MontpellierC3  - Institut de Recherche pour le Developpement (IRD)C3  - Universite Nouvelle CaledonieC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Universite Paul-ValeryC3  - Universite Perpignan Via DomitiaC3  - Universite de MontpellierC3  - Universite de NimesC3  - Institut Universitaire de FranceFU  - European Union's Horizon 2020 research and innovation program under the Marie Skodowska-Curie grant [845178]; Monaco Explorations; Centre for the Synthesis and Analysis of Biodiversity (CESAB) of the Foundation for Research on Biodiversity (FRB)
FX  - We are indebted to all data owners for allowing us to reuse their dugongs' videos posted on social media websites. Data owners include Nautica Environmental Associates LLC (O. Farrell drone pilot) for dugong footage in the Gulf of Arabia, and Burapha University and the National Science and Technology Development Agency for dugong footage in Thailand. We are grateful to L. D., L. R., G. Q., and M. T. for their help with social media search, visualization of ULM videos, and annotation of images. We thank Air Paradise for their collaboration in collecting ULM video sequences in New Caledonia. This project received funding from the European Union's Horizon 2020 research and innovation program under the Marie Skodowska-Curie grant agreement 845178 (MEGAFAUNA). Collection of video data was funded by Monaco Explorations. This study benefited from the PELAGIC group funded by the Centre for the Synthesis and Analysis of Biodiversity (CESAB) of the Foundation for Research on Biodiversity (FRB).
CR  - Abadi Martin, 2016, arXiv
CR  - Borowicz A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212532
CR  - Buckland ST, 2012, J APPL ECOL, V49, P960, DOI 10.1111/j.1365-2664.2012.02150.x
CR  - Ceballos G, 2020, P NATL ACAD SCI USA, V117, P13596, DOI 10.1073/pnas.1922686117
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Cleguer C, 2015, BIOL CONSERV, V184, P154, DOI 10.1016/j.biocon.2015.01.007
CR  - Courchamp F, 2018, PLOS BIOL, V16, DOI 10.1371/journal.pbio.2003997
CR  - Doll?r, 2015, MICROSOFT COCO COMM
CR  - Ducarme F., 2013, BIOSCIENCES MASTER R, V10, P1, DOI DOI 10.1080/08927936.2015.1052279
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Enquist BJ, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-14369-y
CR  - Ferreira AC, 2020, METHODS ECOL EVOL, V11, P1072, DOI 10.1111/2041-210X.13436
CR  - Fiori L, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9060543
CR  - Garrigue C, 2008, MAR MAMMAL SCI, V24, P81, DOI 10.1111/j.1748-7692.2007.00173.x
CR  - Geremia C, 2019, P NATL ACAD SCI USA, V116, P25707, DOI 10.1073/pnas.1913783116
CR  - Ghermandi A, 2019, GLOBAL ENVIRON CHANG, V55, P36, DOI 10.1016/j.gloenvcha.2019.02.003
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
CR  - Gregr EJ, 2020, SCIENCE, V368, P1243, DOI 10.1126/science.aay5342
CR  - Hammerschlag N, 2019, TRENDS ECOL EVOL, V34, P369, DOI 10.1016/j.tree.2019.01.005
CR  - He K., 2015, CVPR
CR  - Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
CR  - Jaric I, 2020, PLOS BIOL, V18, DOI 10.1371/journal.pbio.3000935
CR  - Jaric I, 2020, TRENDS ECOL EVOL, V35, P630, DOI 10.1016/j.tree.2020.03.003
CR  - Letessier TB, 2019, PLOS BIOL, V17, DOI 10.1371/journal.pbio.3000366
CR  - Lyons MB, 2019, METHODS ECOL EVOL, V10, P1024, DOI 10.1111/2041-210X.13194
CR  - MacNeil MA, 2020, NATURE, V583, P801, DOI 10.1038/s41586-020-2519-y
CR  - Mariani G, 2020, SCI ADV, V6, DOI 10.1126/sciadv.abb4848
CR  - Marsh H., 2019, DUGONG DUGON AMENDED
CR  - McCauley DJ, 2015, SCIENCE, V347, DOI 10.1126/science.1255641
CR  - Moleon M, 2020, P ROY SOC B-BIOL SCI, V287, DOI 10.1098/rspb.2019.2643
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Pacoureau N, 2021, NATURE, V589, P567, DOI 10.1038/s41586-020-03173-9
CR  - Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Ripple WJ, 2019, CONSERV LETT, V12, DOI 10.1111/conl.12627
CR  - Sarle W. S., 1995, Computing Science and Statistics. Vol.27. Proceedings of the 27th Symposium on the Interface. Statistics and Manufacturing with Subthemes in Environmental Statistics, Graphics and Imaging, P352
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335
CR  - Toivonen T, 2019, BIOL CONSERV, V233, P298, DOI 10.1016/j.biocon.2019.01.023
CR  - Unel FO, 2019, IEEE COMPUT SOC CONF, P582, DOI 10.1109/CVPRW.2019.00084
CR  - Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
CR  - Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
CR  - Villon S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67573-7
CR  - Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
CR  - West Jeremy, 2007, SPRING RES PRESENTAT
CR  - Wong TT, 2015, PATTERN RECOGN, V48, P2839, DOI 10.1016/j.patcog.2015.03.009
CR  - Zoph Barret, 2019, ARXIV PREPRINT ARXIV
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - FEB
PY  - 2022
VL  - 36
IS  - 1
DO  - 10.1111/cobi.13798
AN  - WOS:000681599900001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  49
ER  -

TY  - CPAPER
AU  - Hsu, HW
AU  - Lee, YC
AU  - Ding, JJ
AU  - Chang, RY
A1  - IEEE
TI  - Dolphin Recognition with Adaptive Hybrid Saliency Detection for Deep Learning Based on DenseNet Recognition
T2  - 2018 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2018)
LA  - English
CP  - 14th IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)
KW  - photo-identification
KW  - marine vertebrate
KW  - convolutional neural networks
KW  - computer vision
KW  - saliency map
AB  - a Dolphin identification is important for wildlife conservation. Since identifying dolphins from thousands of images manually takes tremendous time, it is important to develop an automatic dolphin identification algorithm. In this paper, a high accurate deep learning based dolphin identification algorithm is proposed. We presented an advanced approach, called hybrid saliency method, for feature extraction and efficiently integrate several wellknown techniques to make dolphins distinguishable. With the proposed techniques, we can avoid the background part (e.g. the sea water) to affect the identification results, which is usually a problem of most convolutional neural network based methods. Simulations show that the proposed algorithm can well identify a dolphin in most cases and it can achieve the accuracy rate of 85% even if there are 40 dolphins to be distinguished.
AD  - Natl Taiwan Univ, Grad Inst Commun Engn, Taipei, TaiwanAD  - Acad Sinica, Res Ctr Informat Technol Innovat, Taipei, TaiwanC3  - National Taiwan UniversityC3  - Academia Sinica - TaiwanCR  - Gilman A., 2016, 2016 INT C IM VIS CO, P1
CR  - Gilman A, 2013, INT CONF IMAG VIS, P388, DOI 10.1109/IVCNZ.2013.6727046
CR  - Harel J., 2006, P 19 INT C NEURAL IN, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073
CR  - Hillman G.R., 2003, Aquatic Mammals, V29, P117, DOI 10.1578/016754203101023960
CR  - Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - Kabani A., 2016, MATH APPL, V5, P155
CR  - Kabani A., 2017, 2017 IEEE 30 CAN C E, P1
CR  - Karczmarski Leszek, 1998, Aquatic Mammals, V24, P143
CR  - Kim J, 2016, IEEE T IMAGE PROCESS, V25, P9, DOI 10.1109/TIP.2015.2495122
CR  - Luo Z., 2017, IEEE Computer Vision and Pattern Recognition, P6609
CR  - Polzounov A., 2016, ARXIV160405605
CR  - Simonyan K., 2013, INT C LEARN REPR ICL
CR  - Wang JY, 2000, J MAMMAL, V81, P1157, DOI 10.1644/1545-1542(2000)081<1157:DITEMO>2.0.CO;2
CR  - Weideman HJ, 2017, IEEE INT CONF COMP V, P2831, DOI 10.1109/ICCVW.2017.334
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2018
SP  - 455
EP  - 458
AN  - WOS:000458319800111
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  15
ER  -

TY  - JOUR
AU  - Szenicer, A
AU  - Reinwald, M
AU  - Moseley, B
AU  - Nissen-Meyer, T
AU  - Muteti, ZM
AU  - Oduor, S
AU  - McDermott-Roberts, A
AU  - Baydin, AG
AU  - Mortimer, B
TI  - Seismic savanna: machine learning for classifying wildlife and behaviours using ground-based vibration field recordings
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - African elephant
KW  - machine learning
KW  - seismic waves
KW  - wildlife monitoring
KW  - ELEPHANTS
KW  - AUGMENTATION
KW  - DISCRIMINATION
AB  - We develop a machine learning approach to detect and discriminate elephants from other species, and to recognise important behaviours such as running and rumbling, based only on seismic data generated by the animals. We demonstrate our approach using data acquired in the Kenyan savanna, consisting of 8000 h seismic recordings and 250 k camera trap pictures. Our classifiers, different convolutional neural networks trained on seismograms and spectrograms, achieved 80%-90% balanced accuracy in detecting elephants up to 100 m away, and over 90% balanced accuracy in recognising running and rumbling behaviours from the seismic data. We release the dataset used in this study: SeisSavanna represents a unique collection of seismic signals with the associated wildlife species and behaviour. Our results suggest that seismic data offer substantial benefits for monitoring wildlife, and we propose to further develop our methods using dense arrays that could result in a seismic shift for wildlife monitoring.
AD  - Univ Oxford, Dept Earth Sci, South Parks Rd, Oxford OX1 3AN, EnglandAD  - Univ Oxford, Dept Zool, Oxford, EnglandAD  - Univ Oxford, Dept Comp Sci, Oxford, EnglandAD  - Mpala Res Ctr, Nanyuki, KenyaAD  - Univ Oxford, Dept Engn Sci, Oxford, EnglandC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - League of European Research Universities - LERUC3  - University of OxfordFU  - National Geographic [NGS50019R-18]; Royal Society [URF R1 191033]; John Fell Oxford University Press Research Fund; Royal Commission for the Exhibition of 1851; Centre for Doctoral Training in Autonomous Intelligent Machines and Systems at the University of Oxford, Oxford, UK; UK Engineering and Physical Sciences Research Council; EPSRC/MURI grant [EP/N019474/1]; Lawrence Berkeley National Lab
FX  - We would like to express our gratitude to all the staff at the Mpala Research Centre for assisting with fieldwork operations and for creating a warm and welcoming environment, in particular Dino Martins and Cosmas Nzomo. We thank Frank Pope and staff at Save the Elephants for help getting research permits and Kenyan Wildlife Service affiliations, and Paula Koelemeijer for help with sensor deployment. A. Szenicer thanks the anonymous donor of his PhD grant. We thank National Geographic (NGS50019R-18), Royal Society (URF R1 191033), John Fell Oxford University Press Research Fund, Royal Commission for the Exhibition of 1851 for funding. This research has been supported by the Centre for Doctoral Training in Autonomous Intelligent Machines and Systems at the University of Oxford, Oxford, UK, and the UK Engineering and Physical Sciences Research Council. A. G. Baydin is supported by EPSRC/MURI grant EP/N019474/1 and by Lawrence Berkeley National Lab.
CR  - Barnosky AD, 2004, SCIENCE, V306, P70, DOI 10.1126/science.1101476
CR  - Bellwood DR, 2004, NATURE, V429, P827, DOI 10.1038/nature02691
CR  - Bianco MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50381-z
CR  - Blake S, 2001, AFR J ECOL, V39, P178, DOI 10.1046/j.1365-2028.2001.00296.x
CR  - Brown Tom B., 2020, ARXIV PREPRINT ARXIV
CR  - Ceballos G, 2020, P NATL ACAD SCI USA, V117, P13596, DOI 10.1073/pnas.1922686117
CR  - Chase MJ, 2016, PEERJ, V4, DOI 10.7717/peerj.2354
CR  - Clemente J, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON SMART COMPUTING (SMARTCOMP 2019), P417, DOI 10.1109/SMARTCOMP.2019.00081
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Douglas-Hamilton I., 1987, Oryx, V6, P11
CR  - Duporge I, 2021, REMOTE SENS ECOL CON, V7, P369, DOI 10.1002/rse2.195
CR  - DZIEWONSKI AM, 1977, J GEOPHYS RES, V82, P239, DOI 10.1029/JB082i002p00239
CR  - Falcin A, 2021, J VOLCANOL GEOTH RES, V411, DOI 10.1016/j.jvolgeores.2020.107151
CR  - Frid-Adar M, 2018, NEUROCOMPUTING, V321, P321, DOI 10.1016/j.neucom.2018.09.013
CR  - Galanti Valeria, 2000, Hystrix, V11, P27
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Harris C., 1988, P ALVEY VISION C AVC, P147
CR  - Hensman, 2021, FRONT CONSERV SCI, V1, P10, DOI DOI 10.3389/FCOSC.2020.630967
CR  - Hosseini K, 2020, GEOPHYS J INT, V220, P96, DOI 10.1093/gji/ggz394
CR  - Iandola F. N., 2016, ARXIV PREPRINT ARXIV
CR  - Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
CR  - Leonid TT, 2021, J AMB INTEL HUM COMP, V12, P5269, DOI 10.1007/s12652-020-02005-y
CR  - Li ZF, 2018, GEOPHYS RES LETT, V45, P4773, DOI 10.1029/2018GL077870
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Mangewa LJ, 2019, SUSTAINABILITY-BASEL, V11, DOI 10.3390/su11216116
CR  - Meier MA, 2019, J GEOPHYS RES-SOL EA, V124, P788, DOI 10.1029/2018JB016661
CR  - Mortimer B, 2018, CURR BIOL, V28, pR547, DOI 10.1016/j.cub.2018.03.062
CR  - Moseley B, 2020, SOLID EARTH, V11, P1527, DOI 10.5194/se-11-1527-2020
CR  - Mousavi SM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17591-w
CR  - Mousavi SM, 2020, GEOPHYS RES LETT, V47, DOI 10.1029/2019GL085976
CR  - Ngene SM, 2010, AFR J ECOL, V48, P386, DOI 10.1111/j.1365-2028.2009.01125.x
CR  - Poole Joyce H., 2011, P125
CR  - Randler C, 2018, ECOL EVOL, V8, P7151, DOI 10.1002/ece3.4240
CR  - Reinwald M, 2021, J R SOC INTERFACE, V18, DOI 10.1098/rsif.2021.0264
CR  - Ren L, 2008, J R SOC INTERFACE, V5, P195, DOI 10.1098/rsif.2007.1095
CR  - REW R, 1990, IEEE COMPUT GRAPH, V10, P76, DOI 10.1109/38.56302
CR  - Rost S, 2002, REV GEOPHYS, V40, DOI 10.1029/2000RG000100
CR  - Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
CR  - Scheele B, 2019, SCIENCE, V363, P1459, DOI 10.1126/science.aav0379
CR  - Senior AW, 2020, NATURE, V577, P706, DOI 10.1038/s41586-019-1923-7
CR  - Shamout FE, 2020, IEEE J BIOMED HEALTH, V24, P437, DOI 10.1109/JBHI.2019.2937803
CR  - Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
CR  - Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
CR  - Smit J, 2019, ORYX, V53, P368, DOI 10.1017/S0030605317000345
CR  - Springenberg J.T., 2014, ARXIV ABS14126806
CR  - Sugumar SJ, 2013, CURR SCI INDIA, V104, P1515
CR  - Sukumar R., 2003, LIVING ELEPHANTS EVO
CR  - Szenicer A, 2020, GEOPHYS J INT, V223, P1247, DOI 10.1093/gji/ggaa349
CR  - Szenicer A, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw6548
CR  - Taylor, 2021, LOXODONTA AFRICANA I
CR  - Taylor, 2016, OCCASIONAL PAPER SER, V60
CR  - Wasser SK, 2008, CONSERV BIOL, V22, P1065, DOI 10.1111/j.1523-1739.2008.01012.x
CR  - Wood JD, 2005, J APPL ECOL, V42, P587, DOI 10.1111/j.1365-2664.2005.01044.x
CR  - Yao, 2020, ARXIV PREPRINT ARXIV
CR  - Zhu WQ, 2020, ADV GEOPHYS, V61, P151, DOI 10.1016/bs.agph.2020.07.003
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - APR
PY  - 2022
VL  - 8
IS  - 2
SP  - 236
EP  - 250
DO  - 10.1002/rse2.242
AN  - WOS:000716254000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  55
ER  -

TY  - CPAPER
AU  - Brum, D
AU  - Muller, M
AU  - Veronez, MR
AU  - de Souza, EM
AU  - Gonzaga , L
AU  - Nhanga, CJA
AU  - Conrado, GT
AU  - Procksch, N
AU  - Dias, J
AU  - Viegas, F
AU  - Cauduro, G
AU  - Silva, VS
AU  - Lima, GC
AU  - Amaral, I
AU  - Carvalho, CM
AU  - Goncalves, LO
A1  - IEEE
TI  - PROPOSAL OF A METHOD FOR WILDLIFE-VEHICLE COLLISIONS RISK ASSESSMENT BASED ON GEOGRAPHIC INFORMATION SYSTEMS AND DEEP LEARNING
T2  - IGARSS 2020 - 2020 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM
LA  - English
CP  - IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
KW  - Animal Road-kill
KW  - Risk Assessment
KW  - Deep Learning
KW  - Geoprocessing
KW  - Barrier
AB  - This work proposes a deep learning and GIS based workflow to assess the influence of highway barriers on wildlife collisions. Our work consists of using Convolutional Neural Networks to classify images extracted automatically from Google Street View to determine the type of barrier, and using geoprocessing tools to estimate parameters as barrier length and location. The method was applied in a real dataset, classifying correctly the barriers in the road-kill points with accuracy of 84.44%. Statistical tests were used to evaluate the influence of each type of barrier on the road-kills.
AD  - Univ Vale Rio dos Sinos, Vizlab, X Real & GeoInformat Lab, Sao Leopoldo, RS, BrazilAD  - Univ Vale Rio dos Sinos, Grad Program Appl Comp, Sao Leopoldo, RS, BrazilAD  - Univ Vale Rio dos Sinos, Grad Program Biol, Sao Leopoldo, RS, BrazilAD  - Univ Fed Rio Grande do Sul, Grad Program Ecol, Porto Alegre, RS, BrazilAD  - Av UNISINOS 950, BR-93022000 Sao Leopoldo, RS, BrazilC3  - Universidade do Vale do Rio dos Sinos (Unisinos)C3  - Universidade do Vale do Rio dos Sinos (Unisinos)C3  - Universidade do Vale do Rio dos Sinos (Unisinos)C3  - Universidade Federal do Rio Grande do SulFU  - PETROBRAS; ANP [4600556376]; Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) [001]
FX  - This research was funded by PETROBRAS and ANP grant number 4600556376 and by the Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) - Finance Code 001.
CR  - [Anonymous], LISTA NACL ESPECIES
CR  - Ascensao F, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151500
CR  - Ceia-Hasse A., ECOLOGICAL MODELLING, V375, P45
CR  - FAHRIG L, 1995, BIOL CONSERV, V73, P177, DOI 10.1016/0006-3207(94)00102-V
CR  - Grilo C, 2018, ECOLOGY, V99, P2625, DOI 10.1002/ecy.2464
CR  - Haddad NM, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1500052
CR  - Jackson ND, 2011, BIOL CONSERV, V144, P3143, DOI 10.1016/j.biocon.2011.09.010
CR  - Pagany R., 2019, ISPRS INT J GEO INF, V8, P66, DOI 10.3390/ijgi8020066
CR  - Paula R.C., 2013, BIODIVER BRAS, P146
CR  - Pereira S. G., CIENCIA ANIMAL, V17, P1
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2020
SP  - 569
EP  - 572
DO  - 10.1109/IGARSS39084.2020.9323722
AN  - WOS:000664335300129
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  10
ER  -

TY  - JOUR
AU  - Moallem, G
AU  - Pathirage, DD
AU  - Reznick, J
AU  - Gallagher, J
AU  - Sari-Sarraf, H
TI  - An explainable deep vision system for animal classification and detection in trail-camera images with automatic post-deployment retraining
T2  - KNOWLEDGE-BASED SYSTEMS
LA  - English
KW  - Automatic wildlife monitoring
KW  - Animal classification and detection
KW  - Data drift and retraining
KW  - Model explainability
KW  - Convolutional neural networks (CNN)
KW  - Deep learning
AB  - This paper introduces an automated vision system for animal detection in trail-camera images taken from a field under the administration of the Texas Parks and Wildlife Department. As traditional wildlife counting techniques are intrusive and labor intensive to conduct, trail-camera imaging is a comparatively non-intrusive method for capturing wildlife activity. However, given the large volume of images produced from trail-cameras, manual analysis of the images remains time-consuming and inefficient. We implemented a two-stage deep convolutional neural network pipeline to find animal-containing images in the first stage and then process these images to detect birds in the second stage. The animal classification system classifies animal images with overall 93% sensitivity and 96% specificity. The bird detection system achieves better than 93% sensitivity, 92% specificity, and 68% average Intersection-over-Union rate. The entire pipeline processes an image in less than 0.5 s as opposed to an average 30 s for a human labeler. We also addressed post-deployment issues related to data drift for the animal classification system as image features vary with seasonal changes. This system utilizes an automatic retraining algorithm to detect data drift and update the system. We introduce a novel technique for detecting drifted images and triggering the retraining procedure. Two statistical experiments are also presented to explain the prediction behavior of the animal classification system. These experiments investigate the cues that steers the system towards a particular decision. Statistical hypothesis testing demonstrates that the presence of an animal in the input image significantly contributes to the system's decisions. (C) 2021 Elsevier B.V. All rights reserved.
AD  - Texas Tech Univ, Elect & Comp Engn Dept, Lubbock, TX 79409 USAAD  - Texas Parks & Wildlife Dept, Mason, TX 76856 USAC3  - Texas Tech University SystemC3  - Texas Tech UniversityFU  - Texas Parks and Wildlife Department, USA [522285]
FX  - The authors would like to thank the members of the Applied Vision Lab at Texas Tech University for their assistance in image annotation, especially Peter Wharton, Rupa Vani Battula, Shawn Spicer, Farshad Bolouri, Colin Lynch, and Rishab Tewari. This research was funded by a grant from the Texas Parks and Wildlife Department, USA (522285).
CR  - Adefurin A, 2017, PHARMACOGENOMICS J
CR  - Beery S., 2019, ARXIV PREPRINT ARXIV
CR  - Bianco S, 2018, IEEE ACCESS, V6, P64270, DOI 10.1109/ACCESS.2018.2877890
CR  - Boudaoud L.B., 2019, P OCEANS 2019 MARS M, P1
CR  - Chen T., 2020, ARXIV PREPRINT ARXIV
CR  - Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
CR  - Choo J, 2018, IEEE COMPUT GRAPH, V38, P84, DOI 10.1109/MCG.2018.042731661
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Hong SJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071651
CR  - Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329
CR  - Hudelot C, 2018, 2018 IEEE INT C FUZZ, P1
CR  - Koh P.W., 2017, ARXIV PREPRINT ARXIV
CR  - Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
CR  - LIU G., 2018, P ECCV, P85
CR  - Liu GJ, 2017, KNOWL-BASED SYST, V123, P102, DOI 10.1016/j.knosys.2017.02.016
CR  - Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011
CR  - Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
CR  - Pankajakshan A, 2020, ADV INTELL SYST COMP, V1024, P197, DOI 10.1007/978-981-32-9291-8_17
CR  - Pintor M, 2019, ARXIV PREPRINT ARXIV
CR  - Qiaosong Wang, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P146, DOI 10.1007/978-3-319-50835-1_14
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Ross S., 2006, 1 COURSE PROBABILITY
CR  - Samek W., 2017, ARXIV PREPRINT ARXIV
CR  - Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820
CR  - Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
CR  - Shang R., 2020, KNOWL-BASED SYST
CR  - Simons ES, 2019, ECOL EVOL, V9, P11878, DOI 10.1002/ece3.5695
CR  - Simonyan K., 2013, INT C LEARN REPR ICL
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tang TY, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020336
CR  - Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
CR  - Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
CR  - Yu SQ, 2017, NEUROCOMPUTING, V219, P88, DOI 10.1016/j.neucom.2016.09.010
CR  - Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAR 15
PY  - 2021
VL  - 216
DO  - 10.1016/j.knosys.2021.106815
AN  - WOS:000620980200001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Kellenberger, B
AU  - Marcos, D
AU  - Tuia, D
TI  - Detecting mammals in UAV images: Best practices to address a substantially imbalanced dataset with deep learning
T2  - REMOTE SENSING OF ENVIRONMENT
LA  - English
KW  - Animal census
KW  - Wildlife monitoring
KW  - Unmanned Aerial Vehicles
KW  - Object detection
KW  - Deep learning
KW  - Convolutional Neural Networks
KW  - ANIMALS
AB  - Knowledge over the number of animals in large wildlife reserves is a vital necessity for park rangers in their efforts to protect endangered species. Manual animal censuses are dangerous and expensive, hence Unmanned Aerial Vehicles (UAVs) with consumer level digital cameras are becoming a popular alternative tool to estimate livestock. Several works have been proposed that semi-automatically process UAV images to detect animals, of which some employ Convolutional Neural Networks (CNNs), a recent family of deep learning algorithms that proved very effective in object detection in large datasets from computer vision. However, the majority of works related to wildlife focuses only on small datasets (typically subsets of UAV campaigns), which might be detrimental when presented with the sheer scale of real study areas for large mammal census. Methods may yield thousands of false alarms in such cases. In this paper, we study how to scale CNNs to large wildlife census tasks and present a number of recommendations to train a CNN on a large UAV dataset. We further introduce novel evaluation protocols that are tailored to censuses and model suitability for subsequent human verification of detections. Using our recommendations, we are able to train a CNN reducing the number of false positives by an order of magnitude compared to previous state-of-the-art. Setting the requirements at 90% recall, our CNN allows to reduce the amount of data required for manual verification by three times, thus making it possible for rangers to screen all the data acquired efficiently and to detect almost all animals in the reserve automatically.
AD  - Wageningen Univ Sc Res, Lab Geoinformat Sci & Remote Sensing, Wageningen, NetherlandsFU  - Swiss National Science Foundation [PZ00P2-136827]
FX  - This work has been supported by the Swiss National Science Foundation (grant PZ00P2-136827 (DT, http://p3.snf.ch/project-136827). The authors would like to acknowledge the SAVMAP consortium (in particular Dr. Friedrich Reinhard of Kuzikus Wildlife Reserve, Namibia) and the QCRI and Micromappers (in particular Dr. Ferda Ofli and Ji Kim Lucas) for the support in the collection of ground truth data.
CR  - Andrew W, 2017, IEEE INT CONF COMP V, P2850, DOI 10.1109/ICCVW.2017.336
CR  - BAYLISS P, 1989, AUST WILDLIFE RES, V16, P651
CR  - Bengio Y., 2009, PROC ICML, P41
CR  - Berger-Tal O, 2014, CURR ZOOL, V60, P515, DOI 10.1093/czoolo/60.4.515
CR  - Bouche P, 2012, BIOTECHNOL AGRON SOC, V16, P77
CR  - Buda M., 2017, ARXIV171005381
CR  - Castelluccio M., 2015, LAND USE CLASSIFICAT, V1508, P1
CR  - Chamoso P., 2014, UAVS APPL COUNTING M, P71
CR  - Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
CR  - Diaz-Delgado Ricardo, 2017, P277
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Gadiye D., 2016, INT J BIOL RES, V4, P232
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
CR  - Hodgson J. C., 2018, METHODS ECOL EVOL
CR  - Hollings T., 2018, METHODS ECOL EVOL
CR  - JACHMANN H, 1991, AFR J ECOL, V29, P188, DOI 10.1111/j.1365-2028.1991.tb01001.x
CR  - Kellenberger B., 2017, IEEE INT GEOSC REM S
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
CR  - Nazir S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0169758
CR  - Norton-Griffiths M., 1978, 1 SERENGETI ECOLOGIC
CR  - Ofli F, 2016, BIG DATA-US, V4, P47, DOI 10.1089/big.2014.0064
CR  - Pelletier C, 2016, REMOTE SENS ENVIRON, V187, P156, DOI 10.1016/j.rse.2016.10.010
CR  - Piel AK, 2015, GLOB ECOL CONSERV, V3, P188, DOI 10.1016/j.gecco.2014.11.014
CR  - Radford A., 2015, ARXIV PREPRINT ARXIV
CR  - Redmon J., 2016, IEEE C COMP VIS PATT
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Schlossberg S, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0164904
CR  - Shrivastava A., 2016, IEEE C COMP VIS PATT
CR  - Shrivastava Ashish, 2017, P IEEE C COMP VIS PA, V3, P6
CR  - Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
CR  - Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
CR  - Yang Z, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115989
CR  - Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
PU  - ELSEVIER SCIENCE INC
PI  - NEW YORK
PA  - STE 800, 230 PARK AVE, NEW YORK, NY 10169 USA
DA  - OCT
PY  - 2018
VL  - 216
SP  - 139
EP  - 153
DO  - 10.1016/j.rse.2018.06.028
AN  - WOS:000445990100010
N1  - Times Cited in Web of Science Core Collection:  109
Total Times Cited:  114
Cited Reference Count:  41
ER  -

TY  - CPAPER
AU  - Nepovinnykh, E
AU  - Eerola, T
AU  - Kalviainen, H
AU  - Radchenko, G
ED  - BlancTalon, J
ED  - Helbert, D
ED  - Philips, W
ED  - Popescu, D
ED  - Scheunders, P
TI  - Identification of Saimaa Ringed Seal Individuals Using Transfer Learning
T2  - ADVANCED CONCEPTS FOR INTELLIGENT VISION SYSTEMS, ACIVS 2018
LA  - English
CP  - 19th International Conference on Advanced Concepts for Intelligent Vision Systems (ACIVS)
KW  - Animal biometrics
KW  - Saimaa ringed seals
KW  - Convolutional neural networks
KW  - Transfer learning
KW  - Identification
KW  - Image segmentation
AB  - The conservation efforts of the endangered Saimaa ringed seal depend on the ability to reliably estimate the population size and to track individuals. Wildlife photoidentification has been successfully utilized in monitoring for various species. Traditionally, the collected images have been analyzed by biologists. However, due to the rapid increase in the amount of image data, there is a demand for automated methods. Ringed seals have pelage patterns that are unique to each seal enabling the individual identification. In this work, two methods of Saimaa ringed seal identification based on transfer learning are proposed. The first method involves retraining of an existing convolutional neural network (CNN). The second method uses the CNN trained for image classification to extract features which are then used to train a Support Vector Machine (SVM) classifier. Both approaches show over 90% identification accuracy on challenging image data, the SVM based method being slightly better.
AD  - Lappeenranta Univ Technol, Sch Engn Sci, Dept Computat & Proc Engn, Machine Vis & Pattern Recognit Lab, Lappeenranta, FinlandAD  - South Ural State Univ, Sch Elect Engn & Comp Sci, Chelyabinsk, RussiaC3  - Lappeenranta University of TechnologyC3  - South Ural State UniversityCR  - Albu A. B., 2008, P ICPR WORKSH AN INS
CR  - Allwein EL, 2001, J MACH LEARN RES, V1, P113, DOI 10.1162/15324430152733133
CR  - Anderson C. L., 2007, THESIS
CR  - Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
CR  - Auttila M, 2014, ANN ZOOL FENN, V51, P526, DOI 10.5735/086.051.0601
CR  - Bendik NF, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0059424
CR  - Chehrsimin T, 2018, IET COMPUT VIS, V12, P146, DOI 10.1049/iet-cvi.2017.0082
CR  - Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
CR  - Guschanski K, 2009, BIOL CONSERV, V142, P290, DOI 10.1016/j.biocon.2008.10.024
CR  - Halloran KM, 2015, AFR J ECOL, V53, P147, DOI 10.1111/aje.12145
CR  - Hoque S., 2011, INT J BIOSCIENCE BIO, V3, P45
CR  - Koivuniemi M, 2016, ENDANGER SPECIES RES, V30, P29, DOI 10.3354/esr00723
CR  - Kovacs KM, 2012, MAR MAMMAL SCI, V28, P414, DOI 10.1111/j.1748-7692.2011.00479.x
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
CR  - Nepovinnykh E, 2017, THESIS
CR  - Ojansivu V, 2008, LECT NOTES COMPUT SC, V5099, P236, DOI 10.1007/978-3-540-69905-7_27
CR  - Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zhelezniakov A, 2015, LECT NOTES COMPUT SC, V9475, P227, DOI 10.1007/978-3-319-27863-6_21
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
PY  - 2018
VL  - 11182
SP  - 211
EP  - 222
DO  - 10.1007/978-3-030-01449-0_18
AN  - WOS:000476892400018
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  20
ER  -

TY  - CPAPER
AU  - Kellenberger, B
AU  - Marcos, D
AU  - Courty, N
AU  - Tuia, D
A1  - IEEE
TI  - DETECTING ANIMALS IN REPEATED UAV IMAGE ACQUISITIONS BY MATCHING CNN ACTIVATIONS WITH OPTIMAL TRANSPORT
T2  - IGARSS 2018 - 2018 IEEE INTERNATIONAL GEOSCIENCE AND REMOTE SENSING SYMPOSIUM
LA  - English
CP  - 38th IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
KW  - Livestock estimation
KW  - Unmanned Aerial Vehicle (UAV)
KW  - Domain Adaptation
KW  - Optimal Transport
KW  - Convolutional Neural Network (CNN)
AB  - Repeated animal censuses are crucial for wildlife parks to ensure ecological equilibriums. They are increasingly conducted using images generated by Unmanned Aerial Vehicles (UAVs), often coupled to semi-automatic object detection methods. Such methods have shown great progress also thanks to the employment of Convolutional Neural Networks (CNNs), but even the best models trained on the data acquired in one year struggle predicting animal abundances in subsequent campaigns due to the inherent shift between the datasets. In this paper we adapt a CNN-based animal detector to a follow-up UAV dataset by employing an unsupervised domain adaptation method based on Optimal Transport. We show how to infer updated labels from the source dataset by means of an ensemble of bootstraps. Our method increases the precision compared to the unmodified CNN, while not requiring additional labels from the target set.
AD  - Wageningen Univ, Lab GeoInformat Sci & Remote Sensing, Wageningen, NetherlandsAD  - Univ Bretagne Sud, IRISA, Lorient, FranceC3  - Wageningen University & ResearchC3  - Universite de Bretagne OccidentaleFU  - Swiss National Science Foundation [PP00P2-150593]; SAVMAP consortium
FX  - The authors acknowledge the Swiss National Science Foundation (no. PP00P2-150593) and the SAVMAP consortium (http://lasig.epfl.ch/savmap) for support.
CR  - BAYLISS P, 1989, AUST WILDLIFE RES, V16, P651
CR  - Chamoso P., 2014, UAVS APPL COUNTING M, P71
CR  - Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
CR  - Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
CR  - Flamary R., 2017, POT PYTHON OPTIMAL T
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
CR  - Silver SC, 2004, ORYX, V38, P148, DOI 10.1017/S0030605304000286
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - Tuia D, 2016, IEEE GEOSC REM SEN M, V4, P41, DOI 10.1109/MGRS.2016.2548504
CR  - Volpi M, 2017, IEEE T GEOSCI REMOTE, V55, P881, DOI 10.1109/TGRS.2016.2616585
CR  - Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2018
SP  - 3643
EP  - 3646
AN  - WOS:000451039803156
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  15
ER  -

TY  - JOUR
AU  - Dlamini, N
AU  - van Zyl, TL
TI  - Comparing Class-Aware and Pairwise Loss Functions for Deep Metric Learning in Wildlife Re-Identification
T2  - SENSORS
LA  - English
KW  - Proxy-NCA
KW  - similarity learning
KW  - triplet-loss
KW  - semi-hard negative mining
KW  - CATTLE IDENTIFICATION
KW  - ANIMAL BIOMETRICS
AB  - Similarity learning using deep convolutional neural networks has been applied extensively in solving computer vision problems. This attraction is supported by its success in one-shot and zero-shot classification applications. The advances in similarity learning are essential for smaller datasets or datasets in which few class labels exist per class such as wildlife re-identification. Improving the performance of similarity learning models comes with developing new sampling techniques and designing loss functions better suited to training similarity in neural networks. However, the impact of these advances is tested on larger datasets, with limited attention given to smaller imbalanced datasets such as those found in unique wildlife re-identification. To this end, we test the advances in loss functions for similarity learning on several animal re-identification tasks. We add two new public datasets, Nyala and Lions, to the challenge of animal re-identification. Our results are state of the art on all public datasets tested except Pandas. The achieved Top-1 Recall is 94.8% on the Zebra dataset, 72.3% on the Nyala dataset, 79.7% on the Chimps dataset and, on the Tiger dataset, it is 88.9%. For the Lion dataset, we set a new benchmark at 94.8%. We find that the best performing loss function across all datasets is generally the triplet loss; however, there is only a marginal improvement compared to the performance achieved by Proxy-NCA models. We demonstrate that no single neural network architecture combined with a loss function is best suited for all datasets, although VGG-11 may be the most robust first choice. Our results highlight the need for broader experimentation and exploration of loss functions and neural network architecture for the more challenging task, over classical benchmarks, of wildlife re-identification.
AD  - Univ Witwatersrand, Fac Sci, Sch Comp Sci & Appl Math, ZA-2000 Johannesburg, South AfricaAD  - Univ Johannesburg, Inst Intelligent Syst, Auckland Pk Campus, ZA-2006 Johannesburg, South AfricaAD  - 1 Jan Smuts Ave, ZA-2000 Johannesburg, South AfricaAD  - Kingsway Ave & Univ Rd, ZA-2006 Johannesburg, South AfricaC3  - University of WitwatersrandC3  - University of JohannesburgCR  - Ariff MH, 2013, 2013 IEEE CONFERENCE ON SYSTEMS, PROCESS & CONTROL (ICSPC), P154, DOI 10.1109/SPC.2013.6735123
CR  - Awad AI, 2016, COMPUT ELECTRON AGR, V123, P423, DOI 10.1016/j.compag.2016.03.014
CR  - Borchers DL, 1998, BIOMETRICS, V54, P1207, DOI 10.2307/2533651
CR  - Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
CR  - Burghardt T., 2004, TRACKING ANIMALS WIL
CR  - Burns J., 2021, P SACAIR 2020 MULD, P288
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Chen L, 2019, ASIA-PAC J ATMOS SCI, V55, P303, DOI 10.1007/s13143-018-0064-5
CR  - Chen P, 2020, ECOL EVOL, V10, P3561, DOI 10.1002/ece3.6152
CR  - Chen T., 2020, ARXIV201102803
CR  - Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
CR  - Clarke R., 1994, INFORM TECHNOL PEOPL, V7, P6
CR  - Cui ZH, 2020, NEURAL COMPUT, V32, P659, DOI 10.1162/neco_a_01262
CR  - Dlamini N., 2019, P 2019 INT MULT INF, P1
CR  - El-Naqa I, 2004, IEEE T MED IMAGING, V23, P1233, DOI 10.1109/TMI.2004.834601
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
CR  - He K., 2015, CVPR
CR  - Henschel P, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083500
CR  - Hinton G. E., 2004, ADV NEURAL INF PROCE, P513
CR  - Hou J, 2020, BIOL CONSERV, V242, DOI 10.1016/j.biocon.2020.108414
CR  - Huang GL, 2017, IEEE ICC
CR  - Huo JH, 2020, INT CONF SOFT COMP, P68, DOI 10.1109/ISCMI51676.2020.9311580
CR  - Jain A. K., 2007, HDB BIOMETRICS
CR  - Jiankang Deng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P741, DOI 10.1007/978-3-030-58621-8_43
CR  - Kaya M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091066
CR  - Kim JH, 2019, IEEE ACCESS, V7, P41273, DOI 10.1109/ACCESS.2019.2907327
CR  - Korschens M, 2019, IEEE INT CONF COMP V, P263, DOI 10.1109/ICCVW.2019.00035
CR  - Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
CR  - Lahiri M., 2011, P 1 ACM INT C MULTIM, P1
CR  - Li J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091366
CR  - Li Shuyuan, 2019, ARXIV190605586
CR  - Manack H, 2020, PROCEEDINGS OF 2020 23RD INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2020), P662
CR  - Matkowski WM, 2019, IEEE IMAGE PROC, P1680, DOI 10.1109/ICIP.2019.8803125
CR  - Meyer BJ, 2019, IEEE INT CONF ROBOT, P2924, DOI 10.1109/ICRA.2019.8794188
CR  - Movshovitz-Attias Y, 2017, IEEE I CONF COMP VIS, P360, DOI 10.1109/ICCV.2017.47
CR  - Musgrave Kevin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P681, DOI 10.1007/978-3-030-58595-2_41
CR  - Musgrave K., 2020, ARXIV200809164
CR  - Nepovinnykh E, 2020, IEEE WINT CONF APPL, P25, DOI 10.1109/WACVW50321.2020.9096935
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Qiao YL, 2019, IFAC PAPERSONLINE, V52, P318, DOI 10.1016/j.ifacol.2019.12.558
CR  - Rippel O., 2015, ARXIV151105939
CR  - Roth K., 2020, INT C MACHINE LEARNI, P8242
CR  - Rowcliffe JM, 2008, J APPL ECOL, V45, P1228, DOI 10.1111/j.1365-2664.2008.01473.x
CR  - Schacter CR, 2017, WILSON J ORNITHOL, V129, P459, DOI 10.1676/16-084.1
CR  - Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
CR  - Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
CR  - Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Sun Y., 2014, ARXIV14064773
CR  - Teh E.W., 2020, EUR C COMP VIS ECCV
CR  - van Zyl TL, 2020, PROCEEDINGS OF 2020 23RD INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION 2020), P1066
CR  - Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
CR  - Wah C., 2011, CALTECH UCSD BIRDS 2
CR  - Wang T., 2020, INT C MACH LEARN, P9929
CR  - Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
CR  - Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
CR  - Wright DW, 2019, CURR ZOOL, V65, P665, DOI 10.1093/cz/zoy093
CR  - Xuan H, 2020, IEEE WINT CONF APPL, P2463, DOI 10.1109/WACV45572.2020.9093432
CR  - Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
CR  - Yu BS, 2018, LECT NOTES COMPUT SC, V11210, P71, DOI 10.1007/978-3-030-01231-1_5
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - SEP
PY  - 2021
VL  - 21
IS  - 18
DO  - 10.3390/s21186109
AN  - WOS:000701185400001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  64
ER  -

TY  - JOUR
AU  - Norouzzadeh, MS
AU  - Nguyen, A
AU  - Kosmala, M
AU  - Swanson, A
AU  - Palmer, MS
AU  - Packer, C
AU  - Clune, J
TI  - Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning
T2  - PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA
LA  - English
KW  - deep learning
KW  - deep neural networks
KW  - artificial intelligence
KW  - camera-trap images
KW  - wildlife ecology
KW  - MANAGEMENT
KW  - LANDSCAPE
KW  - SOFTWARE
KW  - MODEL
KW  - FEAR
AB  - Having accurate, detailed, and up-to-date information about the location and behavior of animals in the wild would improve our ability to study and conserve ecosystems. We investigate the ability to automatically, accurately, and inexpensively collect such data, which could help catalyze the transformation of many fields of ecology, wildlife biology, zoology, conservation biology, and animal behavior into "big data" sciences. Motion-sensor "camera traps" enable collecting wildlife pictures inexpensively, unobtrusively, and frequently. However, extracting information from these pictures remains an expensive, time-consuming, manual task. We demonstrate that such information can be automatically extracted by deep learning, a cutting-edge type of artificial intelligence. We train deep convolutional neural networks to identify, count, and describe the behaviors of 48 species in the 3.2 million-image Snapshot Serengeti dataset. Our deep neural networks automatically identify animals with >93.8% accuracy, and we expect that number to improve rapidly in years to come. More importantly, if our system classifies only images it is confident about, our system can automate animal identification for 99.3% of the data while still performing at the same 96.6% accuracy as that of crowdsourced teams of human volunteers, saving >8.4 y (i.e., >17,000 h at 40 h/wk) of human labeling effort on this 3.2 million-image dataset. Those efficiency gains highlight the importance of using deep neural networks to automate data extraction from camera-trap images, reducing a roadblock for this widely used technology. Our results suggest that deep learning could enable the inexpensive, unobtrusive, high-volume, and even real-time collection of a wealth of information about vast numbers of animals in the wild.
AD  - Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USAAD  - Auburn Univ, Dept Comp Sci & Software Engn, Auburn, AL 36849 USAAD  - Harvard Univ, Dept Organism & Evolutionary Biol, Cambridge, MA 02138 USAAD  - Univ Oxford, Dept Phys, Oxford OX1 3RH, EnglandAD  - Univ Minnesota, Dept Ecol Evolut & Behav, St Paul, MN 55108 USAAD  - Uber Al Labs, San Francisco, CA 94103 USAC3  - University of WyomingC3  - Auburn University SystemC3  - Auburn UniversityC3  - Harvard UniversityC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - University of Minnesota SystemC3  - University of Minnesota Twin CitiesFU  - National Science Foundation CAREER Award [1453549]
FX  - We thank Sarah Benson-Amram, the SS volunteers, and the members of the Evolving AI Laboratory at the University of Wyoming for valuable feedback, especially Joost Huizinga, Tyler Jaszkowiak, Roby Velez, Henok Mengistu, and Nick Cheney. J.C. was supported by National Science Foundation CAREER Award 1453549.
CR  - Anderson TM, 2016, PHILOS T R SOC B, V371, DOI 10.1098/rstb.2015.0314
CR  - Bahdanau D., 2014, ARXIV14090473
CR  - Bahdanau D., 2016, ICASSP
CR  - Bengio Y, 2014, ARXIV14062572
CR  - Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
CR  - Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
CR  - Bowkett AE, 2008, AFR J ECOL, V46, P479, DOI 10.1111/j.1365-2028.2007.00881.x
CR  - Bridle J. S., 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28
CR  - Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
CR  - Chattopadhyay P., 2016, ABS160403505 CORR, V1
CR  - Chherawala Y, 2013, 2013 INT C DOC AN RE
CR  - Cho K., 2014, CORR
CR  - Collobert R., 2008, P 25 INT C MACH LEAR, P160, DOI 10.1145/1390156.1390177
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Deng L, 2013, IEEE INT NEW CIRC
CR  - Donahue J, 2014, PR MACH LEARN RES, V32
CR  - Fegraus EH, 2011, ECOL INFORM, V6, P345, DOI 10.1016/j.ecoinf.2011.06.003
CR  - Fei-Fei L, 2005, PROC CVPR IEEE, P524
CR  - Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
CR  - Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
CR  - Harris G., 2010, B ECOL SOC AM, V91, P352, DOI DOI 10.1890/0012-9623-91.3.352
CR  - He K., 2015, CVPR
CR  - Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
CR  - Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619
CR  - Kashif MN, 2016, I S BIOMED IMAGING, P1029, DOI 10.1109/ISBI.2016.7493441
CR  - Krishnappa YS, 2014, ECOL INFORM, V24, P11, DOI 10.1016/j.ecoinf.2014.06.004
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lin M., 2013, ARXIV PREPRINT ARXIV
CR  - Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
CR  - Mohri M., 2012, FDN MACHINE LEARNING
CR  - O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
CR  - Onoro- Rubio D., 2016, ECCV
CR  - Palmer MS, 2017, ECOL LETT, V20, P1364, DOI 10.1111/ele.12832
CR  - Palmer MS, 2018, AFR J ECOL, V56, P882, DOI 10.1111/aje.12505
CR  - Park SR, 2018, OPT EXPRESS, V26, P4004, DOI 10.1364/OE.26.004004
CR  - Rampasek L, 2018, CELL, V172, P893, DOI 10.1016/j.cell.2018.02.013
CR  - Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - SAMUEL AL, 1959, IBM J RES DEV, V3, P211, DOI 10.1147/rd.33.0210
CR  - Sener O., 2018, ARXIV170800489
CR  - Settles Burr, 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI DOI 10.2200/S00429ED1V01Y201207AIM018
CR  - Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Sorower M. S, 2010, LIT SURVEY ALGORITHM, V18
CR  - Sutskever I, 2014, ADV NEUR IN, V27
CR  - Swanson A, 2016, ECOL EVOL, V6, P8534, DOI 10.1002/ece3.2569
CR  - Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Tsoumakas G., 2007, INT J DATA WAREH MIN, V3, P1
CR  - Wang HB, 2014, J MED IMAGING, V1, DOI 10.1117/1.JMI.1.3.034003
CR  - Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
CR  - Zhang PJ, 2014, IEEE C ELEC DEVICES
PU  - NATL ACAD SCIENCES
PI  - WASHINGTON
PA  - 2101 CONSTITUTION AVE NW, WASHINGTON, DC 20418 USA
DA  - JUN 19
PY  - 2018
VL  - 115
IS  - 25
SP  - E5716
EP  - E5725
DO  - 10.1073/pnas.1719367115
AN  - WOS:000435585200013
N1  - Times Cited in Web of Science Core Collection:  300
Total Times Cited:  316
Cited Reference Count:  63
ER  -

TY  - JOUR
AU  - Stavelin, H
AU  - Rasheed, A
AU  - San, O
AU  - Hestnes, AJ
TI  - Applying object detection to marine data and exploring explainability of a fully convolutional neural network using principal component analysis
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Neural networks
KW  - PCA
KW  - Object detection
KW  - XAI
KW  - Machine learning
KW  - YOLO
KW  - SYSTEMS
AB  - With the rise of focus on man made changes to our planet and wildlife therein, more and more emphasis is put on sustainable and responsible gathering of resources. In an effort to preserve maritime wildlife the Norwegian government decided to create an overview of the presence and abundance of various species of marine lives in the Norwegian fjords and oceans. The current work evaluates the possibility of utilizing machine learning methods in particular the You Only Look Once version 3 algorithm to detect fish in challenging conditions characterized by low light, undesirable algae growth and high noise. It was found that the algorithm trained on images collected during the day time under natural light could detect fish successfully in images collected during night under artificial lighting. The overall average precision score of 88% was achieved. Later principal component analysis was used to analyze the features learned in different layers of the network. It is concluded that for the purpose of object detection in specific application areas, the network can be considerably simplified since many of the feature detector turns our to be redundant.
AD  - Norwegian Univ Sci & Technol, Elektro D-B2,235 Gloshaugen,OS Bragstads Plass 2, Trondheim, NorwayAD  - SINTEF Digital, Math & Cybernet, Klaebuveien 153, Trondheim, NorwayAD  - Oklahoma State Univ, 201 Gen Acad Bldg, Stillwater, OK 74078 USAAD  - Kongsberg Maritime, Strandpromenaden 50, Horten, NorwayC3  - Norwegian University of Science & Technology (NTNU)C3  - SINTEFC3  - Oklahoma State University SystemC3  - Oklahoma State University - StillwaterCR  - Abbe E., 2018, ARXIV181206369
CR  - Cai KW, 2020, AQUACULT ENG, V91, DOI 10.1016/j.aquaeng.2020.102117
CR  - Chen, 2017, ARXIV170404861, DOI DOI 10.1016/J.JAL.2014.11.010
CR  - Choi J, 2019, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2019.00059
CR  - Choi Sungbin, 2015, CLEF WORKING NOTES
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Flach P, 2019, THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P9808
CR  - Frisk Oslofjord, 2020, FRISK OSLOFJORD
CR  - Gopalakrishna Aravind Kota, 2013, Machine Learning and Data Mining in Pattern Recognition. 9th International Conference, MLDM 2013. Proceedings: LNCS 7988, P195, DOI 10.1007/978-3-642-39712-7_15
CR  - HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936
CR  - Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
CR  - Hsieh W. W., 2009, MACHINE LEARNING MET
CR  - Hsieh WW, 2001, TELLUS A, V53, P599, DOI 10.1034/j.1600-0870.2001.00251.x
CR  - Ivesdal I., 2020, IMENCO GOBLIN SHARK
CR  - Jalal A, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101088
CR  - Jolliffe I., 2002, PRINCIPAL COMPONENT
CR  - Joly A, 2015, LECT NOTES COMPUT SC, V9283, P462, DOI 10.1007/978-3-319-24027-5_46
CR  - Kathuria A., 2020, IMPLEMENT YOLO OBJEC
CR  - KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
CR  - KRAMER MA, 1991, AICHE J, V37, P233, DOI 10.1002/aic.690370209
CR  - Labao AB, 2019, ECOL INFORM, V52, P103, DOI 10.1016/j.ecoinf.2019.05.004
CR  - Laroca R., 2018 INT JOINT C NEU, P1
CR  - Mishra S P, 2017, INT J LIVESTOCK RES, V7, P60, DOI 10.5455/ijlr.20170415115235
CR  - Moniruzzaman M, 2017, LECT NOTES COMPUT SC, V10617, P150, DOI 10.1007/978-3-319-70353-4_13
CR  - Monroy J, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18124174
CR  - Olsvik E., 2019, ARXIV190402768
CR  - Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Qu HQ, 2018, 2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018)
CR  - Rasheed A, 2020, IEEE ACCESS, V8, P21980, DOI 10.1109/ACCESS.2020.2970143
CR  - Redmon J., 2018, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.690
CR  - Redmon J., P IEEE C COMP VIS PA, P7263
CR  - Redmon J., 2015, ARXIV150602640, P779, DOI DOI 10.1109/CVPR.2016.91
CR  - Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
CR  - Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
CR  - Sun X, 2018, PROC SPIE, V10615, DOI 10.1117/12.2302695
CR  - Sung M, 2017, OCEANS-IEEE
CR  - Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
CR  - Tian YN, 2019, J SENSORS, V2019, DOI 10.1155/2019/7630926
CR  - Tu JV, 1996, J CLIN EPIDEMIOL, V49, P1225, DOI 10.1016/S0895-4356(96)00002-9
CR  - Tzutalin, 2019, LABELIMG
CR  - Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
CR  - Villon S, 2016, LECT NOTES COMPUT SC, V10016, P160, DOI 10.1007/978-3-319-48680-2_15
CR  - Wenwei Xu, 2018, 2018 5th International Conference on Computational Science and Computational Intelligence (CSCI), P313, DOI 10.1109/CSCI46756.2018.00067
CR  - Xu Z, 2018, INT CONF SYST INFORM, P407, DOI 10.1109/ICSAI.2018.8599403
CR  - Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2021
VL  - 62
DO  - 10.1016/j.ecoinf.2021.101269
AN  - WOS:000640472400004
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  46
ER  -

TY  - CPAPER
AU  - Uwanuakwa, ID
AU  - Isienyi, UG
AU  - Idoko, JB
AU  - Albrka, SI
ED  - Zhang, G
TI  - Traffic Warning System for Wildlife Road Crossing Accidents Using Artificial Intelligence
T2  - INTERNATIONAL CONFERENCE ON TRANSPORTATION AND DEVELOPMENT 2020: TRANSPORTATION SAFETY
LA  - English
CP  - ASCE International Conference on Transportation and Development (ASCE ICTD)
KW  - Traffic
KW  - Wildlife-vehicle collisions
KW  - Deep Learning
KW  - AlexNet
KW  - GoogleNet
KW  - Accident
KW  - VEHICLE COLLISIONS
KW  - ANIMALS
KW  - TRENDS
AB  - Wildlife-vehicle collision (WVC) is a major problem associated with regions with high-density wildlife. Urban designers have in the past introduce overpasses, underpasses fence, reflectors, and sensors to aid safe wildlife road crossing, but these have not been able to reduce the wildlife-vehicle collision. This research focused on the automated warning system to vehicle users to minimise wildlife-vehicle collision which could integrate computer vision in the detection of features on the road together with the location-time information feed. The proposed system was trained using AlexNet, GoogelNet, ResNet-50, and VGG-16 algorithm on a deep convolutional neural network (CNN) using 20,964 images of 25 variables consisting of 21 animals and four different vehicle body type. The dataset was divided into training and validation set. The results show that CNN algorithms could identify objects on real-life traffic data with noise background at a reliable accuracy. The GoogelNet, ResNet-50, and VGG-16 model outputs were found to have a better prediction accuracy than the AlexNet model in detecting the object features on the traffic images.
AD  - Near East Univ, Dept Civil Engn, Nicosia, TurkeyAD  - Near East Univ, Appl Artificial Intelligence Res Ctr, Nicosia, TurkeyC3  - Near East UniversityC3  - Near East UniversityCR  - Amato G, 2017, EXPERT SYST APPL, V72, P327, DOI 10.1016/j.eswa.2016.10.055
CR  - Benten A, 2018, ACCIDENT ANAL PREV, V120, P64, DOI 10.1016/j.aap.2018.08.003
CR  - Conn J. M., 2004, Morbidity and Mortality Weekly Report, V53, P675
CR  - Cserkesz T, 2015, NORTH-WEST J ZOOL, V11, P41
CR  - Glorot X., 2010, J MACHINE LEARNING R
CR  - Ha H, 2018, ECOL INFORM, V43, P212, DOI 10.1016/j.ecoinf.2017.10.005
CR  - Knapp K.K., 2004, DEER VEHICLE CRASH C
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Mammeri A, 2016, IEEE T SYST MAN CY-S, V46, P1287, DOI 10.1109/TSMC.2015.2497235
CR  - Murphy KP, 2012, MACHINE LEARNING PRO
CR  - Pingel J., 2017, DEEP LEARNING COMPUT
CR  - Rowden P, 2008, ACCIDENT ANAL PREV, V40, P1865, DOI 10.1016/j.aap.2008.08.002
CR  - Saleh K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18061913
CR  - Seiler A, 2004, WILDLIFE BIOL, V10, P301, DOI 10.2981/wlb.2004.036
CR  - Sharma SU, 2017, IEEE ACCESS, V5, P347, DOI 10.1109/ACCESS.2016.2642981
CR  - Sullivan JM, 2011, J SAFETY RES, V42, P9, DOI 10.1016/j.jsr.2010.11.002
CR  - Wei FW, 2011, RED PANDA: BIOLOGY AND CONSERVATION OF THE FIRST PANDA, P375, DOI 10.1016/B978-1-4377-7813-7.00021-5
CR  - Wilkins DC, 2019, ACCIDENT ANAL PREV, V131, P157, DOI 10.1016/j.aap.2019.05.030
CR  - Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
CR  - Zhou D., 2012, P INT C IM PROC COMP, P1
PU  - AMER SOC CIVIL ENGINEERS
PI  - NEW YORK
PA  - UNITED ENGINEERING CENTER, 345 E 47TH ST, NEW YORK, NY 10017-2398 USA
PY  - 2020
SP  - 194
EP  - 203
AN  - WOS:000629135000017
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  20
ER  -

TY  - JOUR
AU  - Monkman, GG
AU  - Hyder, K
AU  - Kaiser, MJ
AU  - Vidal, FP
TI  - Using machine vision to estimate fish length from images using regional convolutional neural networks
T2  - METHODS IN ECOLOGY AND EVOLUTION
LA  - English
KW  - convolutional neural networks
KW  - European sea bass
KW  - fiducial marker
KW  - fish length
KW  - machine vision
KW  - photogrammetry
KW  - regional convolutional neural network
KW  - videogrammetry
KW  - SYSTEM
KW  - GROUPER
KW  - VIDEO
AB  - An image can encode date, time, location and camera information as metadata and implicitly encodes species information and data on human activity, for example the size distribution of fish removals. Accurate length estimates can be made from images using a fiducial marker; however, their manual extraction is time-consuming and estimates are inaccurate without control over the imaging system. This article presents a methodology which uses machine vision to estimate the total length (TL) of a fusiform fish (European sea bass). Three regional convolutional neural networks (R-CNN) were trained from public images. Images of European sea bass were captured with a fiducial marker with three non-specialist cameras. Images were undistorted using the intrinsic lens properties calculated for the camera in OpenCV; then TL was estimated using machine vision (MV) to detect both marker and subject. MV performance was evaluated for the three R-CNNs under downsampling and rotation of the captured images. Each R-CNN accurately predicted the location of fish in test images (mean intersection over union, 93%) and estimates of TL were accurate, with percent mean bias error (%MBE [95% CIs]) = 2.2% [2.0, 2.4]). Detections were robust to horizontal flipping and downsampling. TL estimates at absolute image rotations >20 degrees became increasingly inaccurate but %MBE [95% CIs] was reduced to -0.1% [-0.2, 0.1] using machine learning to remove outliers and model bias. Machine vision can classify and derive measurements of species from images without specialist equipment. It is anticipated that ecological researchers and managers will make increasing use of MV where image data are collected (e.g. in remote electronic monitoring, virtual observations, wildlife surveys and morphometrics) and MV will be of particular utility where large volumes of image data are gathered.
AD  - Bangor Univ, Sch Ocean Sci, Anglesey, WalesAD  - Ctr Environm Fisheries & Aquaculture Sci, Lowestoft, Suffolk, EnglandAD  - Univ East Anglia, Sch Environm Sci, Norwich, Norfolk, EnglandAD  - Heriot Watt Univ, Sch Energy Geosci Infrastruct & Soc, ILES, Lyell Ctr, Edinburgh, Midlothian, ScotlandAD  - Bangor Univ, Sch Comp Sci & Elect Engn, Bangor, Gwynedd, WalesC3  - Bangor UniversityC3  - Centre for Environment Fisheries & Aquaculture ScienceC3  - University of East AngliaC3  - Heriot Watt UniversityC3  - Bangor UniversityFU  - Fisheries Society of the British Isles; CEFAS Seedcorn [DP227AE]
FX  - Graham Monkman was supported by the Fisheries Society of the British Isles under a PhD Studentship. KH was supported by CEFAS Seedcorn (DP227AE).
CR  - Abadi M., 2015, LARGE SCALE MACHINE
CR  - Amazon, 2017, AM MECH TURK ART INT
CR  - Bartholomew DC, 2018, BIOL CONSERV, V219, P35, DOI 10.1016/j.biocon.2018.01.003
CR  - Bicknell AWJ, 2016, FRONT ECOL ENVIRON, V14, P424, DOI 10.1002/fee.1322
CR  - Chang SK, 2010, FISH RES, V105, P148, DOI 10.1016/j.fishres.2010.03.021
CR  - Chen, 2017, ARXIV170404861, DOI DOI 10.1016/J.JAL.2014.11.010
CR  - Costello C, 2012, SCIENCE, V338, P517, DOI 10.1126/science.1223389
CR  - Deakos MH, 2010, AQUAT BIOL, V10, P1, DOI 10.3354/ab00258
CR  - Dunbrack RL, 2006, FISH RES, V82, P327, DOI 10.1016/j.fishres.2006.08.017
CR  - Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2
CR  - Galesic M., 2006, J OFF STAT, V22, P313
CR  - Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
CR  - Google, 2018, TENS DET MOD ZOO
CR  - He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
CR  - Hoerger M, 2010, CYBERPSYCH BEH SOC N, V13, P697, DOI 10.1089/cyber.2009.0445
CR  - Hold N, 2015, ICES J MAR SCI, V72, P1811, DOI 10.1093/icesjms/fsv030
CR  - Hsiao YH, 2014, ECOL INFORM, V23, P13, DOI 10.1016/j.ecoinf.2013.10.002
CR  - Hyder K, 2018, FISH FISH, V19, P225, DOI 10.1111/faf.12251
CR  - ICES, 2017, 2016SSGIEOM10 ICES C
CR  - ICES, 2012, 012ACOMSCICOM01 ICES
CR  - IMAGENET, 2018, IMAGENET LARG SCAL V
CR  - International Game Fish Association, 2018, IGFA CATCH LOG
CR  - Jeong SJ, 2013, J ELECTR ENG TECHNOL, V8, P1194, DOI 10.5370/JEET.2013.8.5.1194
CR  - Joly A, 2014, ECOL INFORM, V23, P22, DOI 10.1016/j.ecoinf.2013.07.006
CR  - Jung A., 2018, IMGAUG IMAGE AUGMENT
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Konovalov D. A., 2017, ICAIP 2019, P90, DOI [10.1145/3133264.3133271, DOI 10.1145/3133264.3133271]
CR  - Lewin WC, 2006, REV FISH SCI, V14, P305, DOI 10.1080/10641260600886455
CR  - Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
CR  - Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17
CR  - Miranda JM, 2017, AQUACULT ENG, V76, P41, DOI 10.1016/j.aquaeng.2017.01.003
CR  - McClenachan Loren, 2009, Endangered Species Research, V7, P175, DOI 10.3354/esr00167
CR  - Monkman G. G., 2019, SEABASS DETECTION IM, DOI 10.5281/zenodo.3351471
CR  - National Oceanic and Atmospheric Administration, 2015, COST COMP AT SEA OBS
CR  - National Research Council, 2006, COMM REV RECR FISH S, DOI 10.17226/11616
CR  - Needle CL, 2015, ICES J MAR SCI, V72, P1214, DOI 10.1093/icesjms/fsu225
CR  - Neuswanger JR, 2016, CAN J FISH AQUAT SCI, V73, P1861, DOI 10.1139/cjfas-2016-0010
CR  - OpenCV Team, 2018, OPENCV CAM CAL 3D RE
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Perez L., 2017, 8 ARXIV
CR  - Radford Z, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0201666
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Ricard D, 2012, FISH FISH, V13, P380, DOI 10.1111/j.1467-2979.2011.00435.x
CR  - Rizgalla J, 2017, J FISH DIS, V40, P609, DOI 10.1111/jfd.12540
CR  - Rogers TD, 2017, MAR BIOL, V164, DOI 10.1007/s00227-017-3241-7
CR  - Rosen S, 2013, CAN J FISH AQUAT SCI, V70, P1456, DOI 10.1139/cjfas-2013-0124
CR  - Schmid K, 2017, HYDROBIOLOGIA, V784, P93, DOI 10.1007/s10750-016-2860-1
CR  - Schneider CA, 2012, NAT METHODS, V9, P671, DOI 10.1038/nmeth.2089
CR  - Silvertown J, 2015, ZOOKEYS, P125, DOI 10.3897/zookeys.480.8803
CR  - Spampinato C., 2010, P 1 ACM INT WORKSH A, P45, DOI [10.1145/1877868.1877881, DOI 10.1145/1877868.1877881]
CR  - Strachan N. J. C., 1993, Computers and Electronics in Agriculture, V8, P93, DOI 10.1016/0168-1699(93)90009-P
CR  - Struthers DP, 2015, FISHERIES, V40, P502, DOI 10.1080/03632415.2015.1082472
CR  - Sun X, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P471, DOI 10.1109/CISP-BMEI.2016.7852757
CR  - van Helmond ATM, 2017, ICES J MAR SCI, V74, P1421, DOI 10.1093/icesjms/fsw241
CR  - Weinstein BG, 2015, METHODS ECOL EVOL, V6, P357, DOI 10.1111/2041-210X.12320
CR  - White DJ, 2006, FISH RES, V80, P203, DOI 10.1016/j.fishres.2006.04.009
CR  - Zion B, 2007, COMPUT ELECTRON AGR, V56, P34, DOI 10.1016/j.compag.2006.12.007
CR  - Zooniverse, 2017, ZOON LIST ACT PROJ
CR  - Zoph B., 2017, INT C LEARN REPR TOU INT C LEARN REPR TOU
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - DEC
PY  - 2019
VL  - 10
IS  - 12
SP  - 2045
EP  - 2056
DO  - 10.1111/2041-210X.13282
AN  - WOS:000494553200001
N1  - Times Cited in Web of Science Core Collection:  19
Total Times Cited:  22
Cited Reference Count:  59
ER  -

TY  - CPAPER
AU  - Wang, HP
AU  - Song, YL
AU  - Li, S
AU  - Dai, W
AU  - Liu, JT
A1  - IEEE
TI  - TRANSFER LEARNING BASED WILDLIFE RECOGNITION FOR TELE-OBSERVATION IN FIELD OCCLUSION ENVIRONMENT
T2  - 2019 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
LA  - English
CP  - 26th IEEE International Conference on Image Processing (ICIP)
KW  - customized loss function
KW  - anti-occlusion
KW  - wildlife monitoring
KW  - transfer learning
KW  - CNN
AB  - Timely and credible species recognition of wildlife at their habitats is helpful for ecological monitoring. Thanks to the powerful feature extraction ability of convolutional neural networks(CNN), the performance of species identification has been significantly improved. However the CNN based approach still does not achieve their full potential. The cluttered backgrounds and rich feature changes of wild environment bring great challenges to wildlife recognition. In this paper, we propose a novel approach to improve the anti-occlusion ability of CNN model, which is achieved by training a improved anti-occlusion loss function. The anti-occlusion constraint in our proposed loss function works to reduce the distance of feature expression before and after a sample has been occluded. We validated the effect of occlusion based on public dataset cifar-10, which confirms that its necessary to improve the anti-occlusion ability of CNN model. Based on single-labeled training dataset and generalization dataset, comprehensive comparative evaluation proves that our proposed loss can effectively improve the generalization ability of CNN model during the identification of wildlife.
AD  - Nankai Univ, Coll Artificial Intelligence, Tianjin 300353, Peoples R ChinaAD  - Tianjin Key Lab Intelligent Robot, Tianjin 300353, Peoples R ChinaAD  - Peking Univ, Sch Life Sci, Beijing 100871, Peoples R ChinaC3  - Nankai UniversityC3  - Peking UniversityFU  - National Natural Science Foundation of China [61375087]; Key Program of Natural Science Foundation of Tianjin [15JCZDJC31200]
FX  - Thanks to National Natural Science Foundation of China(Grant No. 61375087) and Key Program of Natural Science Foundation of Tianjin (Grant No. 15JCZDJC31200) for funding.
CR  - Ahn S, 2018, IEEE IMAGE PROC, P619, DOI 10.1109/ICIP.2018.8451450
CR  - Bailer C, 2017, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2017.290
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
CR  - Cheng G, 2016, PROC CVPR IEEE, P2884, DOI 10.1109/CVPR.2016.315
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Krizhevsky Alex, TECHNICAL REPORT, V1, P7
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Parham J, 2018, IEEE WINT CONF APPL, P1075, DOI 10.1109/WACV.2018.00123
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - van Riel S, 2018, IEEE IMAGE PROC, P1383, DOI 10.1109/ICIP.2018.8451771
CR  - Yulinsong, 2018, 2018 IEEE C CYBER TE
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2019
SP  - 3392
EP  - 3396
AN  - WOS:000521828603106
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  14
ER  -

TY  - CPAPER
AU  - Solomes, AM
AU  - Stowell, D
A1  - IEEE
TI  - EFFICIENT BIRD SOUND DETECTION ON THE BELA EMBEDDED SYSTEM
T2  - 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING
LA  - English
CP  - IEEE International Conference on Acoustics, Speech, and Signal Processing
KW  - embedded
KW  - bioacoustics
KW  - acoustic
KW  - detection
KW  - deep learning
AB  - Monitoring wildlife is an important aspect of conservation initiatives. Deep learning detectors can help with this, although it is not yet clear whether they can run efficiently on an embedded system in the wild. This paper proposes an automatic detection algorithm for the Bela embedded Linux device for wildlife monitoring. The algorithm achieves good quality recognition, efficiently running on continuously streamed data on a commercially available platform. The program is capable of computing on-board detection using convolutional neural networks (CNNs) with an AUC score of 82.5% on the testing set of an international data challenge. This paper details how the model is exported to work on the Bela Mini in C++, with the spectrogram generation and the implementation of the feed-forward network, and evaluates its performance on the Bird Audio Detection challenge 2018 DCASE data.
AD  - Queen Mary Univ London, Ctr Digital Mus, Machine Listening Lab, London, EnglandC3  - University of LondonC3  - Queen Mary University LondonFU  - EPSRC fellowship [EP/L020505/1]; EPSRC [EP/L020505/1] Funding Source: UKRI
FX  - Supported by EPSRC fellowship EP/L020505/1.
CR  - Aodha O.M., 2018, PLOS COMPUTATIONAL B, V14, P1
CR  - Brown AF, 2015, BIRDS CONSERVATION C
CR  - Chhibber AV, 2019, OPEN FORUM INFECT DI, V6, DOI 10.1093/ofid/ofz453
CR  - Grill T, 2017, EUR SIGNAL PR CONF, P1764, DOI 10.23919/EUSIPCO.2017.8081512
CR  - Johnston A, 2013, NAT CLIM CHANGE, V3, P1055, DOI 10.1038/NCLIMATE2035
CR  - Lai L, 2018, CMSIS NN EFFICIENT N
CR  - Lai L., 2018, RETHINKING MACHINE L
CR  - McPherson Andrew, 2015, AUD ENG SOC 138 CONV
CR  - Sethi SS, 2018, METHODS ECOL EVOL, V9, P2383, DOI 10.1111/2041-210X.13089
CR  - Stowell D, 2019, METHODS ECOL EVOL, V10, P368, DOI 10.1111/2041-210X.13103
CR  - Zantalis F, 2019, FUTURE INTERNET, V11, DOI 10.3390/fi11040094
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2020
SP  - 746
EP  - 750
AN  - WOS:000615970400149
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  11
ER  -

TY  - JOUR
AU  - Bowler, E
AU  - Fretwell, PT
AU  - French, G
AU  - Mackiewicz, M
TI  - Using Deep Learning to Count Albatrosses from Space: Assessing Results in Light of Ground Truth Uncertainty
T2  - REMOTE SENSING
LA  - English
KW  - WorldView-3
KW  - convolutional neural network
KW  - VHR satellite imagery
KW  - wildlife monitoring
KW  - observer uncertainty
KW  - Wandering Albatross
KW  - RESOLUTION SATELLITE IMAGERY
KW  - UNMANNED AIRCRAFT SYSTEM
KW  - ABUNDANCE
KW  - DETECT
AB  - Many wildlife species inhabit inaccessible environments, limiting researchers ability to conduct essential population surveys. Recently, very high resolution (sub-metre) satellite imagery has enabled remote monitoring of certain species directly from space; however, manual analysis of the imagery is time-consuming, expensive and subjective. State-of-the-art deep learning approaches can automate this process; however, often image datasets are small, and uncertainty in ground truth labels can affect supervised training schemes and the interpretation of errors. In this paper, we investigate these challenges by conducting both manual and automated counts of nesting Wandering Albatrosses on four separate islands, captured by the 31 cm resolution WorldView-3 sensor. We collect counts from six observers, and train a convolutional neural network (U-Net) using leave-one-island-out cross-validation and different combinations of ground truth labels. We show that (1) interobserver variation in manual counts is significant and differs between the four islands, (2) the small dataset can limit the networks ability to generalise to unseen imagery and (3) the choice of ground truth labels can have a significant impact on our assessment of network performance. Our final results show the network detects albatrosses as accurately as human observers for two of the islands, while in the other two misclassifications are largely caused by the presence of noise, cloud cover and habitat, which was not present in the training dataset. While the results show promise, we stress the importance of considering these factors for any study where data is limited and observer confidence is variable.
AD  - Univ East Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, EnglandAD  - British Antarctic Survey, Mapping & Geog Informat Ctr, Cambridge CB3 0ET, EnglandC3  - University of East AngliaC3  - UK Research & Innovation (UKRI)C3  - Natural Environment Research Council (NERC)C3  - NERC British Antarctic SurveyFU  - Natural Environmental Research Council through the NEXUSS CDT [NE/N012070/1]; NERC [bas010011] Funding Source: UKRI
FX  - This research was funded by the Natural Environmental Research Council through the NEXUSS CDT, grant number NE/N012070/1, titled "Automated UAV and Satellite Image Analysis for Wildlife Monitoring".
CR  - Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150
CR  - BirdLife International, 2020, SPEC FACTSH DIOM EX
CR  - Borowicz A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212532
CR  - Bowler E, 2019, INT GEOSCI REMOTE SE, P10099, DOI 10.1109/IGARSS.2019.8898079
CR  - Brack IV, 2018, METHODS ECOL EVOL, V9, P1864, DOI 10.1111/2041-210X.13026
CR  - Chabot D, 2012, WATERBIRDS, V35, P170, DOI 10.1675/063.035.0119
CR  - Cubaynes HC, 2019, MAR MAMMAL SCI, V35, P466, DOI 10.1111/mms.12544
CR  - Forsyth D. A., 2002, COMPUTER VISION MODE
CR  - Fredembach C., 2008, P IS T SID 16 COL IM
CR  - Fretwell PT, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0222498
CR  - Fretwell PT, 2017, IBIS, V159, P481, DOI 10.1111/ibi.12482
CR  - Fretwell PT, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088655
CR  - Fretwell PT, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033751
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
CR  - Guo Fan, 2010, Journal of Computer Applications, V30, P2417, DOI 10.3724/SP.J.1087.2010.02417
CR  - Hansch R, 2019, INT GEOSCI REMOTE SE, P5594, DOI 10.1109/IGARSS.2019.8898003
CR  - Hollings T, 2018, METHODS ECOL EVOL, V9, P881, DOI 10.1111/2041-210X.12973
CR  - Hughes BJ, 2011, WILDLIFE BIOL, V17, P210, DOI 10.2981/10-106
CR  - Jiang H, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10060945
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - LaRue MA, 2014, POLAR BIOL, V37, P507, DOI 10.1007/s00300-014-1451-8
CR  - LaRue MA, 2018, POLAR BIOL, V41, P2621, DOI 10.1007/s00300-018-2384-4
CR  - LaRue MA, 2017, CONSERV BIOL, V31, P213, DOI 10.1111/cobi.12809
CR  - LaRue MA, 2015, WILDLIFE SOC B, V39, P772, DOI 10.1002/wsb.596
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lee KY, 2019, IEEE IMAGE PROC, P3581, DOI 10.1109/ICIP.2019.8803666
CR  - Li S, 2009, 2009 INTERNATIONAL CONFERENCE ON MEASURING TECHNOLOGY AND MECHATRONICS AUTOMATION, VOL II, P200, DOI 10.1109/ICMTMA.2009.347
CR  - Liang L, 2019, INT J REMOTE SENS, V40, P7252, DOI 10.1080/01431161.2019.1601286
CR  - Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
CR  - Naveen R, 2012, POLAR BIOL, V35, P1879, DOI 10.1007/s00300-012-1230-3
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Oishi Y, 2014, INT J REMOTE SENS, V35, P1374, DOI 10.1080/01431161.2013.876516
CR  - Patterson C, 2016, J UNMANNED VEH SYST, V4, P53, DOI 10.1139/juvs-2015-0014
CR  - Pettorelli N, 2014, J APPL ECOL, V51, P839, DOI 10.1111/1365-2664.12261
CR  - Phillips RA, 2016, BIOL CONSERV, V201, P169, DOI 10.1016/j.biocon.2016.06.017
CR  - Rodrigues F., 2018, P 32 AAAI C ART INT
CR  - Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
CR  - Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
CR  - Simonyan K, 2015, INT C LEARNING REPRE
CR  - Wang DL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111308
CR  - Weimerskirch H, 2018, POLAR BIOL, V41, P1963, DOI 10.1007/s00300-018-2335-0
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Witmer GW, 2005, WILDLIFE RES, V32, P259, DOI 10.1071/WR04003
CR  - Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - JUN
PY  - 2020
VL  - 12
IS  - 12
DO  - 10.3390/rs12122026
AN  - WOS:000554717400001
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  44
ER  -

TY  - JOUR
AU  - Zhang, W
AU  - Hu, BX
TI  - Forest roads extraction through a convolution neural network aided method
T2  - INTERNATIONAL JOURNAL OF REMOTE SENSING
LA  - English
KW  - IMAGE
KW  - CLASSIFICATION
KW  - SEGMENTATION
KW  - ENVIRONMENT
KW  - MULTISCALE
KW  - LIDAR
AB  - Forest roads are important features of the environment that have significant impacts on wildlife habitats. The U-net and U-shaped Fully Convoluted Network (FCN) deep-learning methods have demonstrated great potential in successfully extracting paved roads in urban environments; however, they require a large amount of pixel-based training samples, which is resource-consuming. During this study, a convolutional neural network (CNN) aided method for forest road identification and extraction was developed. The algorithm utilized the multivariate Gaussian and Laplacian of Gaussian (LoG) filters and VGG 16 on high spatial resolution multispectral imagery to extract both primary road and secondary roads in forested areas. It was tested on imagery over two areas in the Hearst forest located in central Ontario, Canada. Based on validation against manually digitized roads, over 74% of the roads from both test areas were successfully extracted.
AD  - York Univ, Dept Earth & Space Sci & Engn, Toronto, ON M3J 1P3, CanadaC3  - York University - CanadaFU  - Natural Sciences and Engineering Research Council (NSERC)
FX  - The authors gratefully acknowledge the Ontario Ministry of Natural Resources for providing the very high-resolution aerial imagery. This research was made possible due to financial support from the Natural Sciences and Engineering Research Council (NSERC).
CR  - Azizi Z, 2014, J FORESTRY RES, V25, P975, DOI 10.1007/s11676-014-0544-0
CR  - Boston K, 2016, CURR FOR REP, V2, P215, DOI 10.1007/s40725-016-0044-x
CR  - Bouziani M, 2010, IEEE T GEOSCI REMOTE, V48, P3198, DOI 10.1109/TGRS.2010.2044508
CR  - Butenuth M, 2012, MACH VISION APPL, V23, P91, DOI 10.1007/s00138-010-0294-8
CR  - Gao L, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11050552
CR  - Hong ZL, 2018, IEEE ACCESS, V6, P46988, DOI 10.1109/ACCESS.2018.2867210
CR  - Hruza P, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10040492
CR  - Hu J, 2007, IEEE T GEOSCI REMOTE, V45, P4144, DOI 10.1109/TGRS.2007.906107
CR  - Jackson S.D., 2000, WILDLIFE HIGHWAYS SE, P7
CR  - Jing LH, 2012, ISPRS J PHOTOGRAMM, V70, P88, DOI 10.1016/j.isprsjprs.2012.04.003
CR  - Laurance WF, 2009, TRENDS ECOL EVOL, V24, P659, DOI 10.1016/j.tree.2009.06.009
CR  - Lu XY, 2019, IEEE T GEOSCI REMOTE, V57, P9362, DOI 10.1109/TGRS.2019.2926397
CR  - Mattyus G, 2015, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2015.197
CR  - Medioni G., 2000, C FRANC REC FORM INT, P2000
CR  - Mena JB, 2005, PATTERN RECOGN LETT, V26, P1201, DOI 10.1016/j.patrec.2004.11.005
CR  - Movaghati S, 2010, IEEE T GEOSCI REMOTE, V48, P2807, DOI 10.1109/TGRS.2010.2041783
CR  - Ravanbakhsh M, 2008, PHOTOGRAMM REC, V23, P405, DOI 10.1111/j.1477-9730.2008.00496.x
CR  - Saito S, 2016, J IMAGING SCI TECHN, V60, DOI 10.2352/J.ImagingSci.Technol.2016.60.1.010402
CR  - Sherba J, 2014, REMOTE SENS-BASEL, V6, P4043, DOI 10.3390/rs6054043
CR  - Shi WZ, 2014, IEEE T GEOSCI REMOTE, V52, P3359, DOI 10.1109/TGRS.2013.2272593
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Soille P, 2002, IEEE T GEOSCI REMOTE, V40, P2042, DOI 10.1109/TGRS.2002.804618
CR  - Song MJ, 2004, PHOTOGRAMM ENG REM S, V70, P1365, DOI 10.14358/PERS.70.12.1365
CR  - Tang TQ, 2014, MEASUREMENT, V48, P63, DOI 10.1016/j.measurement.2013.10.035
CR  - Valero S, 2010, PATTERN RECOGN LETT, V31, P1120, DOI 10.1016/j.patrec.2009.12.018
CR  - Wei YN, 2017, IEEE GEOSCI REMOTE S, V14, P709, DOI 10.1109/LGRS.2017.2672734
CR  - White RA, 2010, REMOTE SENS-BASEL, V2, P1120, DOI 10.3390/rs2041120
CR  - Wiedemann C., 1998, WORKSH EMP EV METH C, P172
CR  - Zhang C., 1999, ISPRS WORKSH 3D GEOS, P12, DOI [10.3929/ethz-a-010025751, DOI 10.3929/ETHZ-A-010025751]
CR  - Zhang ZX, 2018, IEEE GEOSCI REMOTE S, V15, P749, DOI 10.1109/LGRS.2018.2802944
CR  - Zhong ZL, 2016, INT GEOSCI REMOTE SE, P1591, DOI 10.1109/IGARSS.2016.7729406
CR  - Zhu C, 2005, INT J REMOTE SENS, V26, P5493, DOI 10.1080/01431160500300354
PU  - TAYLOR & FRANCIS LTD
PI  - ABINGDON
PA  - 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
DA  - APR 3
PY  - 2021
VL  - 42
IS  - 7
SP  - 2706
EP  - 2721
DO  - 10.1080/01431161.2020.1862438
AN  - WOS:000605350800001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  32
ER  -

TY  - CPAPER
AU  - Buchanan, C
AU  - Bi, Y
AU  - Xue, B
AU  - Vennell, R
AU  - Childerhouse, S
AU  - Pine, MK
AU  - Briscoe, D
AU  - Zhang, MJ
ED  - Cree, MJ
TI  - Deep Convolutional Neural Networks for Detecting Dolphin Echolocation Clicks
T2  - PROCEEDINGS OF THE 2021 36TH INTERNATIONAL CONFERENCE ON IMAGE AND VISION COMPUTING NEW ZEALAND (IVCNZ)
LA  - English
CP  - 36th International Conference on Image and Vision Computing New Zealand (IVCNZ)
KW  - CLASSIFICATION
AB  - It is essential to monitor marine wildlife to build effective marine mammal management plans for the development of open ocean aquaculture (OOA) around New Zealand (NZ). However, this task is challenging due to the complexities of marine ecosystems, vocal plasticity and diversity of marine mammals, and the limitations of current models. In this paper, we design methods for automatic bottlenose dolphin click detection from easily available acoustic data, which is the initial step towards building an intelligent marine monitoring system in NZ. We collect a vast amount of acoustic data from NZ waters through the use of passive acoustic monitoring and design a preprocessing strategy that converts raw audio signals into spectrograms. A dataset of bottlenose dolphin click detection is created. Four traditional image classification methods and six convolutional neural networks (CNNs), i.e., LeNet, LeNet variants, and ResNet-18, are designed to solve this task. The results show that ResNet-18 achieves the best accuracy (97.44%) among all the methods on this task. This work represents the first study using CNNs for detecting dolphin echolocation clicks.
AD  - Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington, New ZealandAD  - Cawthron Inst, Nelson, New ZealandAD  - Univ Victoria, Victoria, BC, CanadaAD  - Ocean Acoust Ltd, Auckland, New ZealandC3  - Victoria University WellingtonC3  - Cawthron InstituteC3  - University of VictoriaFU  - Science for Technological Innovation Challenge (SfTI) [2019-S7-CRS]; MBIE Data Science SSIF Fund [RTVU1914]
FX  - This work was supported in part by the Science for Technological Innovation Challenge (SfTI) fund under contract 2019-S7-CRS, and MBIE Data Science SSIF Fund under the contract RTVU1914.
CR  - Al-Sahaf H, 2019, J ROY SOC NEW ZEAL, V49, P205, DOI 10.1080/03036758.2019.1609052
CR  - Cawthron, 2019, GOOD SCI KEY SUCC OP
CR  - Chai E, 2020, CONF REC ASILOMAR C, P1214, DOI 10.1109/IEEECONF51394.2020.9443275
CR  - Envirostrat Ltd, 2020, OP OC FINF AQ BUS CA
CR  - Griffiths ET, 2020, J ACOUST SOC AM, V147, P3511, DOI 10.1121/10.0001229
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Ibrahim AK, 2016, 2016 INTERNATIONAL SYMPOSIUM ON COMPUTER, CONSUMER AND CONTROL (IS3C), P260, DOI 10.1109/IS3C.2016.76
CR  - Jiang JJ, 2019, APPL ACOUST, V150, P169, DOI 10.1016/j.apacoust.2019.02.007
CR  - Jiang JJ, 2018, APPL ACOUST, V141, P26, DOI 10.1016/j.apacoust.2018.06.014
CR  - Lasseck M, P CLEF WORK NOT, V2125
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Liu YQ, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3100554
CR  - Ministry for Primary Industries, 2020, SIT OUTL PRIM IND DA
CR  - Nair V., 2010, P 27 INT C INT C MAC
CR  - Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
CR  - Pine MK, 2021, GLOBAL CHANGE BIOL, V27, P4839, DOI 10.1111/gcb.15798
CR  - Roch MA, 2008, CAN ACOUSTACOUST CAN, V36, P41
CR  - Sejdic E, 2009, DIGIT SIGNAL PROCESS, V19, P153, DOI 10.1016/j.dsp.2007.12.004
CR  - Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
CR  - Siddagangaiah S, 2020, ECOL INDIC, V117, DOI 10.1016/j.ecolind.2020.106559
CR  - Towsey M., 2018, J ECOACOUSTICS, V2, DOI [10.22261/JEA.IUSWUI, DOI 10.22261/JEA.IUSWUI]
CR  - Towsey MW, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON DATA MINING WORKSHOP (ICDMW), P788, DOI 10.1109/ICDMW.2015.118
CR  - Yan ZC, 2021, IEEE C EVOL COMPUTAT, P2015, DOI 10.1109/CEC45853.2021.9504737
CR  - Zhang M., 2021, GENETIC PROGRAMMING
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2021
DO  - 10.1109/IVCNZ54163.2021.9653250
AN  - WOS:000750617500013
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  24
ER  -

TY  - JOUR
AU  - Chilson, C
AU  - Avery, K
AU  - McGovern, A
AU  - Bridge, E
AU  - Sheldon, D
AU  - Kelly, J
TI  - Automated detection of bird roosts using NEXRAD radar data and Convolutional Neural Networks
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - Aeroecology
KW  - bird roosts
KW  - deep learning
KW  - machine learning
KW  - CITIZEN SCIENCE
KW  - WEATHER RADAR
KW  - PATTERNS
KW  - MIGRATION
KW  - SONGBIRD
KW  - DRIVERS
AB  - Although NEXRAD radars have proven to be an effective tool for detecting airborne animals, detecting biological phenomena in radar images often involves a manual, time- consuming data-extraction process. This paper focuses on applying machine learning to automatically find radar data that snapshots large aggregations of birds (specifically Purple Martins and Tree Swallows) as they depart en masse from roosting sites. These aggregations are evident in radar images as rings of elevated reflectivity that appear early in the morning as birds depart from roost sites. Our goal was to develop an algorithm that could determine whether an individual radar image contained at least one Purple Martin or Tree Swallow roost. We use a dataset of known roost locations to train three machine learning algorithms that employed (1) a traditional Artificial Neural Network (ANN), (2) a sophisticated preexisting Convolutional Neural Network (CNN) called Inception-v3, and (3) a shallow CNN built from scratch. The resulting programs were all effective at finding bird roosts, with both the shallow CNN and the Inception-v3 network making correct determinations about 90 per cent of the time with an AUC above .9. To the best of our knowledge, this study is the first to apply neural networks in the analysis of bird roosts in radar imagery, and these analytical tools offer new avenues of research into the ecology and behavior of flying animals, with practical applications to wind farm placement, air traffic administration and wildlife conservation. The NEXRAD radar network offers a tremendous archive of continental-scale data and has the potential to capture entire vertebrate populations. We apply existing machine learning models to a new dataset which constitutes a valuable approach to extracting information from this archive.
AD  - Univ Oklahoma, Sch Comp Sci, Norman, OK 73019 USAAD  - Univ Oklahoma, Sch Meteorol, Norman, OK 73019 USAAD  - Univ Oklahoma, Oklahoma Biol Survey, Norman, OK 73019 USAAD  - Univ Oklahoma, Dept Biol, Norman, OK 73019 USAAD  - Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USAAD  - Mt Holyoke Coll, Dept Comp Sci, S Hadley, MA 01075 USAAD  - Univ Oklahoma, Corix Plains Inst, Norman, OK 73019 USAC3  - University of Oklahoma SystemC3  - University of Oklahoma - NormanC3  - University of Oklahoma SystemC3  - University of Oklahoma - NormanC3  - University of Oklahoma SystemC3  - University of Oklahoma - NormanC3  - University of Oklahoma SystemC3  - University of Oklahoma - NormanC3  - University of Massachusetts SystemC3  - University of Massachusetts AmherstC3  - Mount Holyoke CollegeC3  - University of Oklahoma SystemC3  - University of Oklahoma - NormanFU  -  [NSF-DGE-1545261]; Direct For Computer & Info Scie & Enginr [1522054] Funding Source: National Science Foundation; Direct For Education and Human Resources [1545261] Funding Source: National Science Foundation
FX  - The funding from the NSF-DGE-1545261 grant helped make this research possible. We thank Sandra Pletschet for her time spent collecting the roost data and Dr. Phillip Chilson for his advice on the project. Some of the computing for this project was performed at the OU Supercomputing Center for Education & Research (OSCER) at the University of Oklahoma (OU).
CR  - Bauer S, 2017, BIOSCIENCE, V67, P912, DOI 10.1093/biosci/bix074
CR  - Bridge ES, 2016, LANDSCAPE ECOL, V31, P43, DOI 10.1007/s10980-015-0279-0
CR  - Chilson P. B., 2012, RADAR AEROECOLOGY EX
CR  - Chilson PB, 2012, B AM METEOROL SOC, V93, P669, DOI 10.1175/BAMS-D-11-00099.1
CR  - Cohen P.R., 1995, EMPIRICAL METHODS AR, V139
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - DeVault TL, 2013, WILDL MANAGE CONSERV, P1
CR  - Diehl R. H., 2005, INTRO WSR 88D NEXRAD
CR  - Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632
CR  - Donahue B, 2015, 2015 IEEE AEROSPACE CONFERENCE
CR  - Driss S. B., 2017, SPIE C REAL TIM IM V, V10223
CR  - Efron B., 1986, BOOTSTRAP METHODS ST, V1, P54, DOI [10.1214/ss/1177013815, DOI 10.1214/SS/1177013815]
CR  - Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
CR  - Gauthreaux SA, 2003, AUK, V120, P266, DOI 10.1642/0004-8038(2003)120[0266:ROABC]2.0.CO;2
CR  - Gauthreaux SA, 1998, WEATHER FORECAST, V13, P453, DOI 10.1175/1520-0434(1998)013<0453:DOBMOT>2.0.CO;2
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Helmus J. J., 2016, J OPEN RES SOFTWARE, V4, P25, DOI [10.5334/jors.119, DOI 10.5334/JORS.119]
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Kelly JF, 2018, REMOTE SENS ECOL CON, V4, P166, DOI 10.1002/rse2.66
CR  - Kohavi R., 1995, INT JOINT C ART INT
CR  - Koistinen J, 2000, PHYS CHEM EARTH PT B, V25, P1185, DOI 10.1016/S1464-1909(00)00176-3
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lakshmanan V, 2014, J ATMOS OCEAN TECH, V31, P1234, DOI 10.1175/JTECH-D-13-00073.1
CR  - Laughlin AJ, 2016, ECOGRAPHY, V39, P1017, DOI 10.1111/ecog.01988
CR  - Laughlin AJ, 2014, BEHAV ECOL, V25, P734, DOI 10.1093/beheco/aru044
CR  - Laughlin AJ, 2013, AUK, V130, P230, DOI 10.1525/auk.2013.12229
CR  - Maggiori E, 2017, INT GEOSCI REMOTE SE, P5157, DOI 10.1109/IGARSS.2017.8128163
CR  - Mitchell TM., 1997, MACH LEARN, V45, P81
CR  - Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
CR  - Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
CR  - Shipley JR, 2018, REMOTE SENS ECOL CON, V4, P127, DOI 10.1002/rse2.62
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Stepanian PM, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1539
CR  - Stepanian PM, 2015, IEEE T GEOSCI REMOTE, V53, P6518, DOI 10.1109/TGRS.2015.2443131
CR  - Szegedy C., 2015, P IEEE C COMPUTER VI, P1, DOI DOI 10.1109/CVPR.2015.7298594
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Troxel S., 2001, DESIGNING TERMINAL A
CR  - Van den Broeke MS, 2013, J ATMOS OCEAN TECH, V30, P2754, DOI 10.1175/JTECH-D-13-00056.1
CR  - Wu L, 2016, ARXIV160601609
CR  - Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
CR  - Zaugg S, 2008, J R SOC INTERFACE, V5, P1041, DOI 10.1098/rsif.2007.1349
CR  - Zrnic DS, 1998, IEEE T GEOSCI REMOTE, V36, P661, DOI 10.1109/36.662746
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - MAR
PY  - 2019
VL  - 5
IS  - 1
SP  - 20
EP  - 32
DO  - 10.1002/rse2.92
AN  - WOS:000462652500002
N1  - Times Cited in Web of Science Core Collection:  15
Total Times Cited:  15
Cited Reference Count:  42
ER  -

TY  - JOUR
AU  - Wei, WD
AU  - Luo, G
AU  - Ran, JH
AU  - Li, J
TI  - Zilong: A tool to identify empty images in camera-trap data
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Image classification
KW  - Non-machine learning algorithm
KW  - Software
KW  - Wildlife management
KW  - POPULATIONS
KW  - TIGER
AB  - The use of camera traps to research and monitor wildlife results in a large number of images. Many of the images are the result of a false trigger, resulting in an empty photo. Manually removing empty images is time-intensive and costly. To increase image processing efficiency, we present a non-machine learning algorithm to identify empty images in camera-trap data, and developed freely available software, Zilong. We applied Zilong to 53,598 camera-trap images from 24 sites and compared the results to a CNN-based (Convolutional Neural Network) R package MLWIC (Machine Learning for Wildlife Image Classification). Zilong correctly identified 87% of animal images and correctly identified 85% of empty images, while MLWIC identified 65% and 69%, respectively. Our results suggest that Zilong performed better than MLWIC on identifying empty images. Zilong performed well for most of sites (22/24), with reduced performance identifying empty images when there was vegetation swinging significantly in front of camera (2/24). By using Zilong, wildlife researchers can reduce time and resources required to review camera-trap images.
AD  - Sichuan Univ, Coll Life Sci, Key Lab Bioresources & Ecoenvironm, Minist Educ, 24 South Sect 1,Yihuan Rd, Chengdu 610065, Peoples R ChinaC3  - Sichuan UniversityCR  - Burton AC, 2015, J APPL ECOL, V52, P675, DOI 10.1111/1365-2664.12432
CR  - Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
CR  - Coster M, 2001, CEMENT CONCRETE COMP, V23, P133, DOI 10.1016/S0958-9465(00)00058-5
CR  - Figueroa K, 2014, LECT NOTES COMPUT SC, V8827, P940, DOI 10.1007/978-3-319-12568-8_114
CR  - Garrote G, 2011, EUR J WILDLIFE RES, V57, P355, DOI 10.1007/s10344-010-0440-7
CR  - Janecka JE, 2011, J MAMMAL, V92, P771, DOI 10.1644/10-MAMM-A-036.1
CR  - Jumeau J, 2017, ECOL EVOL, V7, P7399, DOI 10.1002/ece3.3149
CR  - Kaehler A., 2016, LEARNING OPENCV 3 CO
CR  - Li S, 2010, IBIS, V152, P299, DOI 10.1111/j.1474-919X.2009.00989.x
CR  - Luo G, 2019, AVIAN RES, V10, DOI 10.1186/s40657-019-0144-y
CR  - Niedballa J, 2016, METHODS ECOL EVOL, V7, P1457, DOI 10.1111/2041-210X.12600
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - O'Brien TG, 2003, ANIM CONSERV, V6, P131, DOI 10.1017/S1367943003003172
CR  - O'Brien TG, 2008, BIRD CONSERV INT, V18, pS144, DOI 10.1017/S0959270908000348
CR  - Singh P, 2017, J MAMMAL, V98, P1453, DOI 10.1093/jmammal/gyx104
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Tabak M.A., 2018, METHODS ECOL EVOL, V2018, P1
CR  - Tack JLP, 2016, ECOL INFORM, V36, P145, DOI 10.1016/j.ecoinf.2016.11.003
CR  - Tan CKW, 2017, BIOL CONSERV, V206, P65, DOI 10.1016/j.biocon.2016.12.012
CR  - Team RC, 2018, R LANG ENV STAT COMP
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - JAN
PY  - 2020
VL  - 55
DO  - 10.1016/j.ecoinf.2019.101021
AN  - WOS:000510985900007
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  8
Cited Reference Count:  22
ER  -

TY  - CPAPER
AU  - Islam, SB
AU  - Valles, D
ED  - Charkrabarti, S
ED  - Paul, R
TI  - Identification of Wild Species in Texas from Camera-trap Images using Deep Neural Network for Conservation Monitoring
T2  - 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC)
LA  - English
CP  - 10th Annual Computing and Communication Workshop and Conference (CCWC)
KW  - DCNN
KW  - image classification
KW  - species recognition
KW  - camera traps
KW  - wildlife monitoring
AB  - Protection of endangered species requires continuous monitoring and updated information about the existence, location, and behavioral alterations in their habitat. Remotely activated camera or "camera traps" is a reliable and effective method of photo documentation of local population size, locomotion, and predator-prey relationships of wild species. However, manual data processing from a large volume of images and captured videos is extremely laborious, time-consuming, and expensive. The recent advancement of deep learning methods has shown great outcomes for object and species identification in images. This paper proposes an automated wildlife monitoring system by image classification using computer vision algorithms and machine learning techniques. The goal is to train and validate a Convolutional Neural Network (CNN) that will be able to detect Snakes, Lizards and Toads/Frogs from camera trap images. The initial experiment implies building a flexible CNN architecture with labeled images accumulated from standard benchmark datasets of different citizen science projects. After accessing satisfactory accuracy, new camera-trap imagery data (collected from Bastrop County, Texas) will be implemented to the model to detect species. The performance will be evaluated based on the accuracy of prediction within their classification. The suggested hardware and software framework will offer an efficient monitoring system, speed up wildlife investigation analysis, and formulate resource management decisions.
AD  - Texas Sate Univ, Ingram Sch Engn, San Marcos, TX 78666 USACR  - Al Bashit A, 2018, 2018 IEEE 9TH ANNUAL INFORMATION TECHNOLOGY, ELECTRONICS AND MOBILE COMMUNICATION CONFERENCE (IEMCON), P438, DOI 10.1109/IEMCON.2018.8615076
CR  - [Anonymous], ZOONIVERSE DATABASE
CR  - [Anonymous], IMAGENET LARGE SCALE
CR  - Bashit A. A, 2019, INT S MEAS CONTR ROB
CR  - Brownlee J, INTRO IMAGENET CHALL
CR  - Brownlee J, PREPARE DATA MACHINE
CR  - Che Yong Yeo, 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing & its Applications (CSPA 2011), P198, DOI 10.1109/CSPA.2011.5759872
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Gomez A., 2016, ARXIV160306169
CR  - He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
CR  - Kays R., 2010, ARXIV10095718V1CSNI
CR  - Mech L.D., 2002, CRITIQUE WILDLIFE RA
CR  - Moreaux M, TOAD IMAGE DATA
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Norouzzadeh M.S., 2017, ARXIV170305830V5
CR  - Rosebrock A., 2017, DEEP LEARNING COMPUT
CR  - Sahu R, 2019, VISUAL OBJECT TRACKI, DOI [10.5772/intechopen.88437, DOI 10.5772/INTECHOPEN.88437]
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yim J, 2017, P INT C DIG IM COMP, DOI 10.1109/DICTA.2017.8227427
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2020
SP  - 537
EP  - 542
AN  - WOS:000668567200085
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  22
ER  -

TY  - JOUR
AU  - Hahn-Klimroth, M
AU  - Kapetanopoulos, T
AU  - Gubert, J
AU  - Dierkes, PW
TI  - Deep learning-based pose estimation for African ungulates in zoos
T2  - ECOLOGY AND EVOLUTION
LA  - English
KW  - animal behavior states
KW  - automated monitoring
KW  - convolutional neural networks
KW  - deep learning tools
KW  - ecology of savannah animals
KW  - image classification
KW  - BEHAVIOR
AB  - The description and analysis of animal behavior over long periods of time is one of the most important challenges in ecology. However, most of these studies are limited due to the time and cost required by human observers. The collection of data via video recordings allows observation periods to be extended. However, their evaluation by human observers is very time-consuming. Progress in automated evaluation, using suitable deep learning methods, seems to be a forward-looking approach to analyze even large amounts of video data in an adequate time frame.
   In this study, we present a multistep convolutional neural network system for detecting three typical stances of African ungulates in zoo enclosures which works with high accuracy. An important aspect of our approach is the introduction of model averaging and postprocessing rules to make the system robust to outliers.
   Our trained system achieves an in-domain classification accuracy of >0.92, which is improved to >0.96 by a postprocessing step. In addition, the whole system performs even well in an out-of-domain classification task with two unknown types, achieving an average accuracy of 0.93. We provide our system at so that interested users can train their own models to classify images and conduct behavioral studies of wildlife.
   The use of a multistep convolutional neural network for fast and accurate classification of wildlife behavior facilitates the evaluation of large amounts of image data in ecological studies and reduces the effort of manual analysis of images to a high degree. Our system also shows that postprocessing rules are a suitable way to make species-specific adjustments and substantially increase the accuracy of the description of single behavioral phases (number, duration). The results in the out-of-domain classification strongly suggest that our system is robust and achieves a high degree of accuracy even for new species, so that other settings (e.g., field studies) can be considered.
AD  - Goethe Univ, Dept Comp Sci & Math, 10 Robert Mayer St, D-60325 Frankfurt, GermanyAD  - Goethe Univ, Fac Biol Sci Biosci Educ & Zoo Biol, Frankfurt, GermanyC3  - Goethe University FrankfurtC3  - Goethe University FrankfurtFU  - von Opel Hessische Zoostiftung
FX  - von Opel Hessische Zoostiftung
CR  - Abdulla W., 2017, MASK R CNN OBJECT DE
CR  - Andrews W., P BRIT MACH VIS C SW, V60, P1, DOI [10.5244/c.29.60, DOI 10.5244/C.29.60]
CR  - Beery S., 2018, P EUR C COMP VIS ECC
CR  - Bochkovskiy A., 2020, YOLOV4 OPTIMAL SPEED
CR  - Bradski G, 2000, DR DOBBS J, V25, P120
CR  - Burger AL, 2020, APPL ANIM BEHAV SCI, V229, DOI 10.1016/j.applanim.2020.105012
CR  - Chakravarty P, 2020, METHODS ECOL EVOL, V11, P1639, DOI 10.1111/2041-210X.13491
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
CR  - Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
CR  - Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
CR  - Ferreira AC, 2020, METHODS ECOL EVOL, V11, P1072, DOI 10.1111/2041-210X.13436
CR  - Graving JM, 2019, ELIFE, V8, DOI 10.7554/eLife.47994
CR  - He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
CR  - Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
CR  - Kabra M, 2013, NAT METHODS, V10, P64, DOI [10.1038/NMETH.2281, 10.1038/nmeth.2281]
CR  - Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
CR  - Kingma DP, 2015, COMPUTER SCI, P1
CR  - Kogler J, 2020, J ZOO AQUAR RES, V8, P124
CR  - Li C, 2019, PROC CVPR IEEE, P7864, DOI 10.1109/CVPR.2019.00806
CR  - Lima SL, 2005, ANIM BEHAV, V70, P723, DOI 10.1016/j.anbehav.2005.01.008
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Mathis MW, 2020, CURR OPIN NEUROBIOL, V60, P1, DOI 10.1016/j.conb.2019.10.008
CR  - Melzheimer J, 2020, P NATL ACAD SCI USA, V117, P33325, DOI 10.1073/pnas.2002487117
CR  - Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
CR  - Norouzzadeh MS, 2021, METHODS ECOL EVOL, V12, P150, DOI 10.1111/2041-210X.13504
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Pedersen GR, 2004, J EQUINE VET SCI, V24, P153, DOI 10.1016/j.jevs.2004.03.013
CR  - Porto SMC, 2013, BIOSYST ENG, V115, P184, DOI 10.1016/j.biosystemseng.2013.03.002
CR  - Quionero-Candela J., 2009, DATASET SHIFT MACHIN, DOI [10.5555/1462129, DOI 10.5555/1462129]
CR  - Rast W, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0227317
CR  - Recht B, 2019, PR MACH LEARN RES, V97
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - RYDER OA, 1995, BIODIVERS CONSERV, V4, P671, DOI 10.1007/BF00222522
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Simonyan K., 2014, ADV NEURAL INFORM PR, P568
CR  - Stanton LA, 2015, APPL ANIM BEHAV SCI, V173, P3, DOI 10.1016/j.applanim.2015.04.001
CR  - Stern U, 2015, SCI REP-UK, V5, DOI 10.1038/srep14351
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tan M., 2020, EFFICIENTDET SCALABL, P10778, DOI [10.1109/CVPR42600.2020.01079, DOI 10.1109/CVPR42600.2020.01079]
CR  - Tan MX, 2019, PR MACH LEARN RES, V97
CR  - Teitelbaum CS, 2015, ECOL LETT, V18, P545, DOI 10.1111/ele.12435
CR  - Ternman E, 2014, APPL ANIM BEHAV SCI, V160, P12, DOI 10.1016/j.applanim.2014.08.014
CR  - Touvron H., 2020, FIXING TRAIN TEST RE
CR  - Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
CR  - Weinstein BG, 2018, METHODS ECOL EVOL, V9, P1435, DOI 10.1111/2041-210X.13011
CR  - Xie Q., 2019, SELF TRAINING NOISY
CR  - Yakubovskiy P., 2019, EFFICIENTNET
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Zepelin H., 2005, PRINCIPLES PRACTICE, P91, DOI DOI 10.1016/B0-72-160797-7/50015-X
CR  - Zhao YX, 2020, EURASIP J IMAGE VIDE, V2020, DOI 10.1186/s13640-020-00501-x
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - JUN
PY  - 2021
VL  - 11
IS  - 11
SP  - 6015
EP  - 6032
DO  - 10.1002/ece3.7367
AN  - WOS:000646574600001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  54
ER  -

TY  - CPAPER
AU  - Arshad, B
AU  - Barthelemy, J
AU  - Pilton, E
AU  - Perez, P
A1  - IEEE
TI  - Where is my Deer?-Wildlife Tracking And Counting via Edge Computing And Deep Learning
T2  - 2020 IEEE SENSORS
LA  - English
CP  - IEEE Sensors Conference
KW  - computer vision
KW  - ecology
KW  - invasive species management
KW  - deer monitoring
KW  - edge computing
KW  - remote sensing
KW  - visual object tracking (VOT)
AB  - Reliable, informative and up-to-date information regarding the location, mobility and behavioral patterns of animals enhances our ability to preserve biodiversity, manage invasive species and conduct research. The basis of which is an accurate count of the animals present in a specified region. In literature, previous studies have presented automated animal counting methods, usually relying on using single images. Thus, accuracy is challengeable due to several factors, including wildlife movement, light fluctuations, overlapping, occlusions and re-counting of the same animal reappearing in other images. In this paper, we present a novel approach of identification, introduction to tracking pipeline, and counting wildlife accurately. Having applied the techniques of deep convolutional neural network (CNN), edge computing, and online tracking in a field trial to determine the population density of deer in a given area. Our approach produced accurate and actionable results, thus there is viable commercial potential.
AD  - Univ Wollongong, SMART Infrastruct Facil, Wollongong, NSW 2522, AustraliaAD  - Wize Dynam Pty Ltd, North Wollongong, NSW 2500, AustraliaC3  - University of WollongongFU  - New South Wales Government's Minimum Viable Product Grant [MVP193355]
FX  - This work was supported by the New South Wales Government's Minimum Viable Product Grant, number MVP193355.
CR  - Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
CR  - Andriluka M, 2008, PROC CVPR IEEE, P1873, DOI 10.1109/CVPR.2008.4587583
CR  - Arshad B, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19225012
CR  - Chen G., 2020, ARXIV PREPRINT ARXIV
CR  - Franklin D., 2017, NVIDIA ACCELERATED C
CR  - Gonzalez LF, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010097
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Klein D. J., 2015, BLOOMB DAT GOOD EXCH
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Liu X, 2019, IEEE ROBOT AUTOM LET, V4, P2296, DOI 10.1109/LRA.2019.2901987
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Redmon J., 2018, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.690
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Rivas A, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072048
CR  - Rovero F, 2013, HYSTRIX, V24, P148, DOI 10.4404/hystrix-24.2-6316
CR  - Toh Y.H., 2009, INT C COMP INT SOFTW
CR  - Torney CJ, 2019, METHODS ECOL EVOL, V10, P779, DOI 10.1111/2041-210X.13165
CR  - Trolliet F, 2014, BIOTECHNOL AGRON SOC, V18, P446
CR  - Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
CR  - Wei J, 2018, INFORMATION, V9, DOI 10.3390/info9030061
CR  - Zhang L, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19051188
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2020
AN  - WOS:000646236300218
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  21
ER  -

TY  - JOUR
AU  - Chabot, D
AU  - Stapleton, S
AU  - Francis, CM
TI  - Using Web images to train a deep neural network to detect sparsely distributed wildlife in large volumes of remotely sensed imagery: A case study of polar bears on sea ice
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Artificial intelligence
KW  - Big data
KW  - Computer vision
KW  - Conservation
KW  - Machine learning
KW  - Marine mammals
KW  - AERIAL SURVEYS
KW  - SENSING DATA
KW  - AIRCRAFT
KW  - ANIMALS
KW  - SYSTEMS
KW  - COUNTS
AB  - Remote sensing can be a valuable alternative or complement to traditional techniques for monitoring wildlife populations, but often entails operational bottlenecks at the image analysis stage. For example, photographic aerial surveys have several advantages over surveys employing airborne observers or other more intrusive monitoring techniques, but produce onerous amounts of imagery for manual analysis when conducted across vast areas, such as the Arctic. Deep learning algorithms, chiefly convolutional neural networks (CNNs), have shown promise for automatically detecting wildlife in large and/or complex image sets. But for sparsely distributed species, such as polar bears (Ursus maritimus), there may not be sufficient known instances of the animals in an image set to train a CNN. We investigated the feasibility of instead providing 'synthesized' training data to a CNN to detect polar bears throughout large volumes of aerial imagery from a survey of the Baffin Bay subpopulation. We harvested 534 miscellaneous images of polar bears from the Web that we edited to more closely resemble 21 known images of bears from the aerial survey that were solely used for validation. We combined the Web images of polar bears with 6292 random background images from the aerial survey to train a CNN (ResNet-50), which subsequently correctly classified 20/21 (95%) bear images from the survey and 1172/1179 (99.4%) random background validation images. Given that even a small background misclassification rate could produce multitudinous false positives over many thousands of photos, we describe a potential workflow to efficiently screen out erroneous detections. We also discuss potential avenues to improve CNN accuracy, and the broader applicability of our approach to other image-based wildlife monitoring scenarios. Our results demonstrate the feasibility of using miscellaneously sourced images of animals to train deep neural networks for specific wildlife detection tasks.
AD  - droneMetrics, 7 Tauvette St, Ottawa, ON K1B 3A1, CanadaAD  - Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, 2003 Buford Circle, St Paul, MN 55108 USAAD  - Canadian Wildlife Serv, Environm & Climate Change Canada, 1125 Colonel By Dr, Ottawa, ON K1A 0H3, CanadaC3  - University of Minnesota SystemC3  - University of Minnesota Twin CitiesC3  - Environment & Climate Change CanadaC3  - Canadian Wildlife ServiceFU  - Government of Nunavut, the Greenland Institute of Natural Resources; Nunavut Wildlife Management Board, the Polar Continental Shelf Program, and the University of Minnesota
FX  - The image analysis work was commissioned and funded by Envi-ronment and Climate Change Canada. We thank the members of the Nattivak Hunting and Trapping Organization and Conservation Officers Seeglook Ageeagook, B.J. Hainu and Steve Levesque for their assistance with the Baffin Bay logistical preparations and field work associated with the aerial survey. Funding and logistical support for the aerial survey was provided by the Government of Nunavut, the Greenland Institute of Natural Resources, the Nunavut Wildlife Management Board, the Polar Continental Shelf Program, and the University of Minnesota. We thank our pilots and Unaalik Aviation
CR  - Aars J, 2009, MAR MAMMAL SCI, V25, P35, DOI 10.1111/j.1748-7692.2008.00228.x
CR  - Andrew ME, 2017, REMOTE SENS ECOL CON, V3, P66, DOI 10.1002/rse2.38
CR  - BAJZAK D, 1990, WILDLIFE SOC B, V18, P125
CR  - Ball JE, 2017, J APPL REMOTE SENS, V11, DOI 10.1117/1.JRS.11.042609
CR  - Bengio Y., 2009, PROC ICML, P41
CR  - Benz UC, 2004, ISPRS J PHOTOGRAMM, V58, P239, DOI 10.1016/j.isprsjprs.2003.10.002
CR  - Borowicz A., 2018, Sci
CR  - Boudaoud L.B., 2019, P OCEANS 2019 MARS F
CR  - Buckland ST, 2012, J APPL ECOL, V49, P960, DOI 10.1111/j.1365-2664.2012.02150.x
CR  - Chabot D, 2019, BIOL CONSERV, V237, P125, DOI 10.1016/j.biocon.2019.06.022
CR  - Chabot D, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01205-130115
CR  - Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
CR  - Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
CR  - Chretien LP, 2016, WILDLIFE SOC B, V40, P181, DOI 10.1002/wsb.629
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Conn PB, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0251130
CR  - Corcoran E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-39917-5
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Gonalves B.C., 2020, Remote Sens. Environ., V239
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P1490, DOI 10.1111/2041-210X.13246
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
CR  - Groom G, 2013, ECOL INFORM, V14, P2, DOI 10.1016/j.ecoinf.2012.12.001
CR  - Hatfield M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12193112
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hollings T, 2018, METHODS ECOL EVOL, V9, P881, DOI 10.1111/2041-210X.12973
CR  - Hong SJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071651
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Laliberte AS, 2003, WILDLIFE SOC B, V31, P362
CR  - LaRue MA, 2017, CONSERV BIOL, V31, P213, DOI 10.1111/cobi.12809
CR  - Leblanc G, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8040273
CR  - LeCun Y., 2015, Nature, V521, P436
CR  - Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
CR  - Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015
CR  - Ma L, 2017, ISPRS J PHOTOGRAMM, V130, P277, DOI 10.1016/j.isprsjprs.2017.06.001
CR  - Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
CR  - Nakhatovich M.A., 2020, COMMUNICATIONS COMPU, V1235, P3
CR  - Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Sasse DB, 2003, WILDLIFE SOC B, V31, P1015
CR  - Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
CR  - Stapleton S., 2013, THESIS U MINNESOTA
CR  - Stapleton S, 2014, BIOL CONSERV, V170, P38, DOI 10.1016/j.biocon.2013.12.040
CR  - Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
CR  - Tajbakhsh N, 2016, IEEE T MED IMAGING, V35, P1299, DOI 10.1109/TMI.2016.2535302
CR  - Torney CJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156342
CR  - Tuia D, 2016, IEEE GEOSC REM SEN M, V4, P41, DOI 10.1109/MGRS.2016.2548504
CR  - Wang DL, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111308
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Xu BB, 2020, COMPUT ELECTRON AGR, V171, DOI 10.1016/j.compag.2020.105300
CR  - Yuan QQ, 2020, REMOTE SENS ENVIRON, V241, DOI 10.1016/j.rse.2020.111716
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2022
VL  - 68
DO  - 10.1016/j.ecoinf.2021.101547
AN  - WOS:000793038300002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  51
ER  -

TY  - JOUR
AU  - Wang, L
AU  - Diakogiannis, F
AU  - Mills, S
AU  - Bajema, N
AU  - Atkinson, I
AU  - Bishop-Hurley, GJ
AU  - Charmley, E
TI  - A noise robust automatic radiolocation animal tracking system
T2  - ANIMAL BIOTELEMETRY
LA  - English
KW  - Radiolocation
KW  - Machine learning
KW  - Encoder
KW  - decoder
KW  - Convolutional neural network
KW  - SATELLITE
AB  - Agriculture is becoming increasingly reliant upon accurate data from sensor arrays, with localization an emerging application in the livestock industry. Ground-based time difference of arrival (TDoA) radio location methods have the advantage of being lightweight and exhibit higher energy efficiency than methods reliant upon Global Navigation Satellite Systems (GNSS). Such methods can employ small primary battery cells, rather than rechargeable cells, and still deliver a multi-year deployment. In this paper, we present a novel deep learning algorithm adapted from a one-dimensional U-Net implementing a convolutional neural network (CNN) model, originally developed for the task of semantic segmentation. The presented model (ResUnet-1d) both converts TDoA sequences directly to positions and reduces positional errors introduced by sources such as multipathing. We have evaluated the model using simulated animal movements in the form of TDoA position sequences in combination with real-world distributions of TDoA error. These animal tracks were simulated at various step intervals to mimic potential TDoA transmission intervals. We compare ResUnet-1d to a Kalman filter to evaluate the performance of our algorithm to a more traditional noise reduction approach. On average, for simulated tracks having added noise with a standard deviation of 50 m, the described approach was able to reduce localization error by between 66.3% and 73.6%. The Kalman filter only achieved a reduction of between 8.0% and 22.5%. For a scenario with larger added noise having a standard deviation of 100 m, the described approach was able to reduce average localization error by between 76.2% and 81.9%. The Kalman filter only achieved a reduction of between 31.0% and 39.1%. Results indicate that this novel 1D CNN U-Net like encoder/decoder for TDoA location error correction outperforms the Kalman filter. It is able to reduce average localization errors to between 16 and 34 m across all simulated experimental treatments while the uncorrected average TDoA error ranged from 55 to 188 m.
AD  - CSIRO, Agr & Food, Townsville, Qld 4814, AustraliaAD  - CSIRO, Agr & Food, Brisbane, Qld 4067, AustraliaAD  - Univ Western Australia, ICRAR, Perth, WA 6009, AustraliaAD  - CSIRO, Data61, Perth, WA 6014, AustraliaAD  - James Cook Univ, eRes Ctr, Townsville, Qld 4814, AustraliaC3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)C3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)C3  - University of Western AustraliaC3  - Commonwealth Scientific & Industrial Research Organisation (CSIRO)C3  - James Cook UniversityFU  - Advance Queensland Innovation Partnerships (AQIP)-Smart Ear Tag for Livestock
FX  - This document is the result of a research project funded by Advance Queensland Innovation Partnerships (AQIP)-Smart Ear Tag for Livestock, 2016.
CR  - Alonso-Gonzalez I, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041040
CR  - Bengio Y, 2009, NEURAL COMPUT, V21, P1601, DOI 10.1162/neco.2008.11-07-647
CR  - Chan YT, 2005, IEEE T SIGNAL PROCES, V53, P2625, DOI 10.1109/TSP.2005.850336
CR  - Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
CR  - Clark DD., 1989, P OCEANS, V3, P934, DOI [10.1109/OCEANS.1989.586711, DOI 10.1109/OCEANS.1989.586711]
CR  - CraigheadJr FC, 1972, SATELLITE GROUND RAD
CR  - Desouky M. A. A., 2019, 29 AAS AIAA ASTR SPE, P1
CR  - Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
CR  - Heinrich Mattias P., 2018, Current Directions in Biomedical Engineering, V4, P297, DOI 10.1515/cdbme-2018-0072
CR  - JOUVENTIN P, 1990, NATURE, V343, P746, DOI 10.1038/343746a0
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Komatsu R, 2019, COMPUT SCI INF TECHN, P1, DOI 10.5121/csit.2019.90201
CR  - Liu D, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P842
CR  - Marselli C, 1998, P C INT INSTR C2I 18, P443
CR  - MARSHALL WILLIAM H., 1962, JOUR WIDLIFE MANAGEMENT, V26, P75, DOI 10.2307/3798169
CR  - Menzies D, 2016, COMPUT ELECTRON AGR, V124, P175, DOI 10.1016/j.compag.2016.04.001
CR  - Mo J, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122783
CR  - Morales JM, 2004, ECOLOGY, V85, P2436, DOI 10.1890/03-0269
CR  - O'Driscoll KKM, 2009, J DAIRY SCI, V92, P4249, DOI 10.3168/jds.2008-1906
CR  - PRIEDE IG, 1984, FISH RES, V2, P201, DOI 10.1016/0165-7836(84)90003-1
CR  - Quaglietta L, 2019, MOV ECOL, V7, DOI 10.1186/s40462-019-0154-8
CR  - Reynolds AM, 2014, SCI REP-UK, V4, DOI 10.1038/srep04409
CR  - Ronneberger O., 2015, PROC INT C MED IMAGE, V9351, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, 10.1007/978-3-319-24574-4_28]
CR  - Swain DL, 2008, ECOL MODEL, V212, P273, DOI 10.1016/j.ecolmodel.2007.10.027
CR  - Turchin Peter, 1998
CR  - Vincent P, 2008, P 25 INT C MACH LEAR, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
CR  - Wu Liming, 2013, Information Technology Journal, V12, P8563, DOI 10.3923/itj.2013.8563.8569
CR  - Zhang YF, 2019, IEEE INTERNET THINGS, V6, P6618, DOI 10.1109/JIOT.2019.2909038
CR  - Zhang ZK, 2018, INT J DISTRIB SENS N, V14, DOI 10.1177/1550147718815798
PU  - SPRINGERNATURE
PI  - LONDON
PA  - CAMPUS, 4 CRINAN ST, LONDON, N1 9XW, ENGLAND
DA  - AUG 1
PY  - 2021
VL  - 9
IS  - 1
DO  - 10.1186/s40317-021-00248-w
AN  - WOS:000682939000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Knight, EC
AU  - Hannah, KC
AU  - Foley, GJ
AU  - Scott, CD
AU  - Brigham, RM
AU  - Bayne, E
TI  - Recommendations for acoustic recognizer performance assessment with application to five common automated signal recognition programs
T2  - AVIAN CONSERVATION AND ECOLOGY
LA  - English
KW  - automated signal recognition
KW  - autonomous recording unit
KW  - bioacoustics
KW  - Common Nighthawk
KW  - recognizer
KW  - signal processing
KW  - AUTONOMOUS RECORDING UNITS
KW  - BAT ECHOLOCATION CALLS
KW  - GEOGRAPHIC-VARIATION
KW  - MICROPHONE ARRAYS
KW  - NEURAL-NETWORKS
KW  - CLASSIFICATION
KW  - IDENTIFICATION
KW  - BIRDS
KW  - SONG
KW  - TIME
AB  - Automated signal recognition software is increasingly used to extract species detection data from acoustic recordings collected using autonomous recording units (ARUs), but there is little practical guidance available for ecologists on the application of this technology. Performance evaluation is an important part of employing automated acoustic recognition technology because the resulting data quality can vary with a variety of factors. We reviewed the bioacoustic literature to summarize performance evaluation and found little consistency in evaluation, metrics employed, or terminology used. We also found that few studies examined how score threshold, i. e., cut-off for the level of confidence in target species classification, affected performance, but those that did showed a strong impact of score threshold on performance. We used the lessons learned from our literature review and best practices from the field of machine learning to evaluate the performance of five readily-available automated signal recognition programs. We used the Common Nighthawk (Chordeiles minor) as our model species because it has simple, consistent, and frequent vocalizations. We found that automated signal recognition was effective for determining Common Nighthawk presence-absence and call rate, particularly at low score thresholds, but that occupancy estimates from the data processed with recognizers were consistently lower than from data generated by human listening and became unstable at high score thresholds. Of the five programs evaluated, our convolutional neural network (CNN) recognizer performed best, with recognizers built in Song Scope and MonitoR also performing well. The RavenPro and Kaleidoscope recognizers were moderately effective, but produced more false positives than the other recognizers. Finally, we synthesized six general recommendations for ecologists who employ automated signal recognition software, including what to use as a test benchmark, how to incorporate score threshold, what metrics to use, and how to evaluate efficiency. Future studies should consider our recommendations to build a body of literature on the effectiveness of this technology for avian research and monitoring.
AD  - Univ Alberta, Dept Biol Sci, Bioacoust Unit, Edmonton, AB, CanadaAD  - WildResearch, WildRes Nightjar Survey, New Westminster, BC, CanadaAD  - Canadian Wildlife Serv, Environm & Climate Change Canada, Sackville, NB, CanadaAD  - Univ Regina, Dept Biol, Regina, SK, CanadaC3  - University of AlbertaC3  - Environment & Climate Change CanadaC3  - Canadian Wildlife ServiceC3  - University of ReginaFU  - Natural Sciences and Engineering Research Council of Canada; Alberta Biodiversity Monitoring Institute; Ecological Monitoring Committee for the Lower Athabasca; Joint Oil Sands Monitoring Program; Environment and Climate Change Canada; Mitacs; Baillie Fund; Science Horizons; Public Conservation Assistance Fund; TD Friends of the Environment Foundation
FX  - Our sincere thanks to the Subject Editor and three reviewers for their insightful comments, which greatly improved the clarity and accuracy of the manuscript. We gratefully acknowledge funding provided for the bioacoustic component of this work from the Natural Sciences and Engineering Research Council of Canada, the Alberta Biodiversity Monitoring Institute, the Ecological Monitoring Committee for the Lower Athabasca, and the Joint Oil Sands Monitoring Program. Funding for Common Nighthawk research was provided by the Natural Sciences and Engineering Research Council of Canada, Environment and Climate Change Canada, Mitacs, the Baillie Fund, Science Horizons, the Public Conservation Assistance Fund, and the TD Friends of the Environment Foundation.
CR  - Abadi M., 2015, TensorFlow: large-scale machine learning on heterogeneous systems
CR  - Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005
CR  - Agranat I., 2009, AUTOMATICALLY IDENTI
CR  - Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
CR  - Anderson SE, 1996, J ACOUST SOC AM, V100, P1209, DOI 10.1121/1.415968
CR  - Armitage DW, 2010, ECOL INFORM, V5, P465, DOI 10.1016/j.ecoinf.2010.08.001
CR  - ARMSTRONG JT, 1965, ECOLOGY, V46, P619, DOI 10.2307/1935001
CR  - Bang A. V., 2014, INT J IMAGE PROCESSI, V1, P6
CR  - Bardeli R, 2010, PATTERN RECOGN LETT, V31, P1524, DOI 10.1016/j.patrec.2009.09.014
CR  - BART J, 1984, AUK, V101, P307, DOI 10.1093/auk/101.2.307
CR  - Bedoya C., 2014, ECOLOGICAL INFORM, V24, P1
CR  - Belyaeva N. A., WHATFROG COMP CLASSI
CR  - Blumstein DT, 2011, J APPL ECOL, V48, P758, DOI 10.1111/j.1365-2664.2011.01993.x
CR  - Brauer CL, 2016, WILDLIFE SOC B, V40, P140, DOI 10.1002/wsb.619
CR  - Briggs F, 2012, J ACOUST SOC AM, V131, P4640, DOI 10.1121/1.4707424
CR  - Burnham KP., 2002, MODEL SELECTION MULT, V2
CR  - Cakir E, 2017, EUR SIGNAL PR CONF, P1744, DOI 10.23919/EUSIPCO.2017.8081508
CR  - Campbell M, 2012, J FIELD ORNITHOL, V83, P391, DOI 10.1111/j.1557-9263.2012.00389.x
CR  - Campbell P, 2010, EVOLUTION, V64, P1955, DOI 10.1111/j.1558-5646.2010.00962.x
CR  - Campos-Cerqueira M, 2016, METHODS ECOL EVOL, V7, P1340, DOI 10.1111/2041-210X.12599
CR  - Catchpole CK, 2008, BIRD SONG: BIOLOGICAL THEMES AND VARIATIONS, 2ND EDITION, P1, DOI 10.1017/CBO9780511754791
CR  - Charif R.A., 2010, RAVEN PRO 1 4 USERS
CR  - Chesmore E. D., 2007, B ENTOMOL RES, V94, P1
CR  - Chu W, 2011, INT CONF ACOUST SPEE, P345
CR  - Colbert DS, 2015, WILDLIFE SOC B, V39, P757, DOI 10.1002/wsb.577
CR  - Colonna JG, 2015, EXPERT SYST APPL, V42, P7367, DOI 10.1016/j.eswa.2015.05.030
CR  - Crump PS, 2017, ECOL EVOL, V7, P3087, DOI 10.1002/ece3.2730
CR  - Davis J., 2006, KNOWLEDGE INTENSIVE, P233
CR  - Davis J., 2006, P 23 INT C MACH LEAR, P432
CR  - Demsar J, 2006, J MACH LEARN RES, V7, P1
CR  - Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197
CR  - Digby A, 2013, METHODS ECOL EVOL, V4, P675, DOI 10.1111/2041-210X.12060
CR  - Dolezel P, 2015, INT J ENG RES AFR, V18, P184, DOI 10.4028/www.scientific.net/JERA.18.184
CR  - Dong XY, 2015, ECOL INFORM, V29, P66, DOI 10.1016/j.ecoinf.2015.07.007
CR  - Drake KL, 2016, WILDLIFE SOC B, V40, P346, DOI 10.1002/wsb.658
CR  - Duan S, 2013, P 25 INN APPL ART IN, P1519
CR  - Dufour O., 2014, SOUNDSCAPE SEMIOTICS, P83
CR  - Ehnes M, 2015, BIOACOUSTICS, V24, P111, DOI 10.1080/09524622.2014.994228
CR  - Environment Canada, 2016, SPECIES RISK ACT REC
CR  - Erbs F, 2017, J ACOUST SOC AM, V141, P2489, DOI 10.1121/1.4978000
CR  - Feng J., 2016, PLOS ONE, V8
CR  - Ferroudj M., 2015, THESIS
CR  - Forcey GM, 2006, J WILDLIFE MANAGE, V70, P1674, DOI 10.2193/0022-541X(2006)70[1674:COTDPA]2.0.CO;2
CR  - Furnas BJ, 2015, J WILDLIFE MANAGE, V79, P325, DOI 10.1002/jwmg.821
CR  - Ganchev T, 2007, BIOACOUSTICS, V16, P281, DOI 10.1080/09524622.2007.9753582
CR  - Ganchev TD, 2015, EXPERT SYST APPL, V42, P6098, DOI 10.1016/j.eswa.2015.03.036
CR  - Glotin H., 2016, 2016 IEEE INT WORKSH
CR  - Goeau H., 2017, LIFECLEF 2017 WORKIN, V1866, P8
CR  - Gonzalez R., 2010, 2010 4 INT C SIGN PR, P1
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Goyette JL, 2011, J FIELD ORNITHOL, V82, P279, DOI 10.1111/j.1557-9263.2011.00331.x
CR  - Gradisek A, 2017, BIOACOUSTICS, V26, P63, DOI 10.1080/09524622.2016.1190946
CR  - Grau J, 2015, BIOINFORMATICS, V31, P2595, DOI 10.1093/bioinformatics/btv153
CR  - Hafner S. D., 2017, SHORT INTRO ACOUSTIC
CR  - Harma A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P545
CR  - Haselmayer J, 2000, CONDOR, V102, P887, DOI 10.1650/0010-5422(2000)102[0887:ACOPCA]2.0.CO;2
CR  - Heinicke S, 2015, METHODS ECOL EVOL, V6, P753, DOI 10.1111/2041-210X.12384
CR  - Holmes Stephen B., 2015, Canadian Field-Naturalist, V129, P115
CR  - Holmes SB, 2014, WILDLIFE SOC B, V38, P591, DOI 10.1002/wsb.421
CR  - Huang CJ, 2009, EXPERT SYST APPL, V36, P3737, DOI 10.1016/j.eswa.2008.02.059
CR  - Jaafar H., 2014, P 2014 INT C COMM SI, P172
CR  - Jahn O, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0169041
CR  - Jaiswara R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075930
CR  - Jeliazkov A, 2016, GLOB ECOL CONSERV, V6, P208, DOI 10.1016/j.gecco.2016.02.008
CR  - Jennings N, 2008, CAN J ZOOL, V86, P371, DOI 10.1139/Z08-009
CR  - Joly A, 2016, LECT NOTES COMPUT SC, V9822, P286, DOI 10.1007/978-3-319-44564-9_26
CR  - Kaewtip K, 2013, INT CONF ACOUST SPEE, P768, DOI 10.1109/ICASSP.2013.6637752
CR  - Kalan AK, 2015, ECOL INDIC, V54, P217, DOI 10.1016/j.ecolind.2015.02.023
CR  - Kasten EP, 2010, ECOL INFORM, V5, P153, DOI 10.1016/j.ecoinf.2010.02.003
CR  - Katz J, 2016, BIOACOUSTICS, V25, P177, DOI 10.1080/09524622.2015.1133320
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Koops HV, 2014, CEUR WORKSHOP PROC, V1180, P634
CR  - Lee CH, 2006, PATTERN RECOGN LETT, V27, P93, DOI 10.1016/j.patrec.2005.07.004
CR  - Lee H., 2017, LIBROSA 0 5 0
CR  - Luther DA, 2012, ANIM BEHAV, V83, P1059, DOI 10.1016/j.anbehav.2012.01.034
CR  - MacKenzie DI, 2002, ECOLOGY, V83, P2248, DOI 10.1890/0012-9658(2002)083[2248:ESORWD]2.0.CO;2
CR  - McClintock BT, 2010, ECOLOGY, V91, P2446, DOI 10.1890/09-1287.1
CR  - Mencia E. L., 2013, P INT S NEUR INF SCA, P1
CR  - Mesaros A, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6060162
CR  - Nicholson D, 2016, P 15 PYTH SCI C JUL, P57, DOI DOI 10.25080/MAJORA-629E541A-008
CR  - Arencibia JJN, 2015, INT CONF CONTEMP, P59, DOI 10.1109/IC3.2015.7346653
CR  - Noda JJ, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6120443
CR  - Noda JJ, 2016, EXPERT SYST APPL, V50, P100, DOI 10.1016/j.eswa.2015.12.020
CR  - Oliveira B. C., 2014, CIENCIA NATURA, V36, P1
CR  - Potamitis I, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0096936
CR  - Potamitis I, 2014, APPL ACOUST, V80, P1, DOI 10.1016/j.apacoust.2014.01.001
CR  - Prohl H, 2006, EVOLUTION, V60, P1669
CR  - Provost F., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P445
CR  - Ptacek L, 2016, BIOACOUSTICS, V25, P55, DOI 10.1080/09524622.2015.1089524
CR  - Qian K, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P1317, DOI 10.1109/GlobalSIP.2015.7418412
CR  - R Core Team, 2019, R LANG ENV STAT COMP
CR  - Raffel C., 2014, P 15 INT SOC MUS INF
CR  - RAGHAVAN VV, 1989, ACM T INFORM SYST, V7, P205, DOI 10.1145/65943.65945
CR  - Ranjard L, 2008, J ACOUST SOC AM, V123, P4358, DOI 10.1121/1.2903861
CR  - Ranjard L, 2015, J ACOUST SOC AM, V137, P2542, DOI 10.1121/1.4919329
CR  - Ribeiro E., 2015, 20152 AMAL THEA REU
CR  - Rowe K. M. C., 2017, EMU AUSTRAL ORNITHOL, V117, P1
CR  - Russo D, 2016, ECOL INDIC, V66, P598, DOI 10.1016/j.ecolind.2016.02.036
CR  - Rydell J, 2017, ECOL INDIC, V78, P416, DOI 10.1016/j.ecolind.2017.03.023
CR  - Salamon J., 2016, PLOS ONE, V11
CR  - Salamon J, 2017, INT CONF ACOUST SPEE, P141, DOI 10.1109/ICASSP.2017.7952134
CR  - Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
CR  - Salzberg SL, 1997, DATA MIN KNOWL DISC, V1, P317, DOI 10.1023/A:1009752403260
CR  - Shonfield J, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00974-120114
CR  - Sidie-Slettedahl AM, 2015, WILDLIFE SOC B, V39, P626, DOI 10.1002/wsb.569
CR  - Sing T, 2005, BIOINFORMATICS, V21, P3940, DOI 10.1093/bioinformatics/bti623
CR  - Skowronski MD, 2006, J ACOUST SOC AM, V119, P1817, DOI 10.1121/1.2166948
CR  - Slabbekoorn H, 2002, PHILOS T R SOC B, V357, P493, DOI 10.1098/rstb.2001.1056
CR  - Sokolova M, 2006, LECT NOTES COMPUT SC, V4304, P1015
CR  - Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
CR  - Stowell D, 2014, PEERJ, V2, DOI 10.7717/peerj.488
CR  - Swiston KA, 2009, J FIELD ORNITHOL, V80, P42, DOI 10.1111/j.1557-9263.2009.00204.x
CR  - Tachibana RO, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092584
CR  - Tan L. N., 2012, INTERSPEECH
CR  - Tan LN, 2015, J ACOUST SOC AM, V137, P1069, DOI 10.1121/1.4906168
CR  - Thakur A, 2017, BIRD AUDIO DETECTION
CR  - Thessen A., 2016, ONE ECOSYST, V1, P8621, DOI [10.3897/oneeco.1.e8621, DOI 10.3897/ONEECO.1.E8621]
CR  - Tsai WH, 2014, J INF SCI ENG, V30, P1927
CR  - Turesson HK, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163041
CR  - Ulloa JS, 2016, ECOL INFORM, V31, P91, DOI 10.1016/j.ecoinf.2015.11.012
CR  - Vega G, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.70
CR  - Ventura TM, 2015, EXPERT SYST APPL, V42, P8463, DOI 10.1016/j.eswa.2015.07.002
CR  - Vignolo L., 2016, 17 S ARG INT ART ASA, P53
CR  - Waddle JH, 2009, HERPETOL CONSERV BIO, V4, P384
CR  - Walters CL, 2012, J APPL ECOL, V49, P1064, DOI 10.1111/j.1365-2664.2012.02182.x
CR  - Wildlife Acoustics, 2011, SONG SCOP BIOAC SOFT
CR  - Wildlife Acoustics, 2016, CONV SONG SCOP REC K
CR  - Xie J., 2016, 2016 IEEE INT C PROG, P1, DOI DOI 10.1109/ICPHM.2016.7542845
CR  - Xie J, 2015, IEEE IMAGE PROC, P4190, DOI 10.1109/ICIP.2015.7351595
CR  - Yip DA, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00997-120111
CR  - YOUDEN WJ, 1950, CANCER-AM CANCER SOC, V3, P32, DOI 10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3
PU  - RESILIENCE ALLIANCE
PI  - WOLFVILLE
PA  - ACADIA UNIV, BIOLOGY DEPT, WOLFVILLE, NS B0P 1X0, CANADA
DA  - DEC
PY  - 2017
VL  - 12
IS  - 2
DO  - 10.5751/ACE-01114-120214
AN  - WOS:000419347000019
N1  - Times Cited in Web of Science Core Collection:  76
Total Times Cited:  79
Cited Reference Count:  131
ER  -

TY  - JOUR
AU  - Ruff, ZJ
AU  - Lesmeister, DB
AU  - Ducha, LS
AU  - Padmaraju, BK
AU  - Sullivan, CM
TI  - Automated identification of avian vocalizations with deep convolutional neural networks
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - Acoustic monitoring
KW  - avian vocalization
KW  - Bioacoustics
KW  - machine learning
KW  - neural networks
KW  - spotted owls
KW  - BARRED OWLS
KW  - CONSERVATION
KW  - FOREST
AB  - Passive acoustic monitoring is an emerging approach to wildlife monitoring that leverages recent improvements in automated recording units and other technologies. A central challenge of this approach is the task of locating and identifying target species vocalizations in large volumes of audio data. To address this issue, we developed an efficient data processing pipeline using a deep convolutional neural network (CNN) to automate the detection of owl vocalizations in spectrograms generated from unprocessed field recordings. While the project was initially focused on spotted and barred owls, we also trained the network to recognize northern saw-whet owl, great horned owl, northern pygmy-owl, and western screech-owl. Although classification performance varies across species, initial results are promising. Recall, or the proportion of calls in the dataset that are detected and correctly identified, ranged from 63.1% for barred owl to 91.5% for spotted owl based on raw network output. Precision, the rate of true positives among apparent detections, ranged from 0.4% for spotted owl to 77.1% for northern saw-whet owl based on raw output. In limited tests, the CNN performed as well as or better than human technicians at detecting owl calls. Our model output is suitable for developing species encounter histories for occupancy models and other analyses. We believe our approach is sufficiently general to support long-term, large-scale monitoring of a broad range of species beyond our target species list, including birds, mammals, and others.
AD  - US Forest Serv, Pacific Northwest Res Stn, USDA, Corvallis, OR 97331 USAAD  - Oregon State Univ, Dept Fisheries & Wildlife, Corvallis, OR 97331 USAAD  - Oregon Cooperat Fish & Wildlife Res Unit, Corvallis, OR USAAD  - Oregon State Univ, Ctr Genome Res & Biocomp, Corvallis, OR 97331 USAC3  - United States Department of Agriculture (USDA)C3  - United States Forest ServiceC3  - Oregon State UniversityC3  - Oregon State UniversityFU  - USDA Forest Service; USDI Bureau of Land Management
FX  - The authors thank C. Cardillo, M. Corr, D. Culp, T. Garrido, E. Guzman, A. Ingrassia, D. Jacobsma, E. Johnston, R. Justice, K. McLaughlin, P. Papajcik, and W. Swank for field assistance in collecting data and C. Cardillo, D. Culp, Z. Farrand, R. Justice, A. Munes, and S. Pruett for validating CNN output and locating additional training data. Three anonymous reviewers provided valuable insights and feedback which have greatly improved the manuscript. Funding and logistical support were provided by USDA Forest Service and USDI Bureau of Land Management. The Center for Genome Research and Biocomputing, Oregon State University provided C. Sullivan salary and biocomputing infrastructure support. This work was partially supported through a Research Participation Program administered by Oak Ridge Institute for Science and Education (ORISE) and hosted by US Forest Service, Pacific Northwest Research Station. The findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA or U.S. Government determination or policy. The use of trade or firm names in this publication is for reader information and does not imply endorsement by the U.S. Department of Agriculture of any product or service.
CR  - Abadi Martin, 2015, TENSORFLOW LARGE SCA
CR  - Alonso JB, 2017, EXPERT SYST APPL, V72, P83, DOI 10.1016/j.eswa.2016.12.019
CR  - Artuso C., 2013, BIRDS N AM
CR  - Brown JC, 2007, J ACOUST SOC AM, V122, P1201, DOI 10.1121/1.2747198
CR  - Campos-Cerqueira M, 2016, METHODS ECOL EVOL, V7, P1340, DOI 10.1111/2041-210X.12599
CR  - Cannings R. J., 2017, BIRDS N AM, DOI 10.2173/bna.wesowl1.03
CR  - Chambert T, 2018, METHODS ECOL EVOL, V9, P560, DOI 10.1111/2041-210X.12910
CR  - Dugger KM, 2016, CONDOR, V118, P57, DOI 10.1650/CONDOR-15-24.1
CR  - Figueira L, 2015, BIOL CONSERV, V184, P27, DOI 10.1016/j.biocon.2014.12.020
CR  - FORSMAN ED, 1984, J WILDLIFE MANAGE, P5
CR  - Ganchev T, 2007, BIOACOUSTICS, V16, P281, DOI 10.1080/09524622.2007.9753582
CR  - Gutierrez RJ, 2007, BIOL INVASIONS, V9, P181, DOI 10.1007/s10530-006-9025-5
CR  - Heinicke S, 2015, METHODS ECOL EVOL, V6, P753, DOI 10.1111/2041-210X.12384
CR  - Holt D. W., 2000, BIRDS N AM, DOI [10.2173/bna.494, DOI 10.2173/BNA.494]
CR  - Kahl S., 2017, LARGE SCALE BIRD SOU
CR  - Kingma D., 2015, ADAM METHOD STOCHAST
CR  - Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lesmeister Damon B., 2018, U S Forest Service Pacific Northwest Research Station General Technical Report PNW-GTR, V1, P245
CR  - Luo W., 2019, J ACOUST SOC AM, V145, P7
CR  - MacKenzie D.I., 2018, OCCUPANCY ESTIMATION, V2nd
CR  - Mazur K. M, 2000, BIRDS N AM, DOI [10.2173/bna.508, DOI 10.2173/BNA.508]
CR  - Nvidia, 2019, NVIDIA DRIVE AUT VEH
CR  - Odom KJ, 2010, CONDOR, V112, P549, DOI 10.1525/cond.2010.090163
CR  - Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
CR  - Rasmussen J.L., 2008, BIRDS N AM, DOI [10.2173/bna.42., DOI 10.2173/BNA.42]
CR  - Russo D, 2003, ECOGRAPHY, V26, P197, DOI 10.1034/j.1600-0587.2003.03422.x
CR  - Shonfield J, 2018, J RAPTOR RES, V52, P42, DOI 10.3356/JRR-17-52.1
CR  - Somervuo P, 2019, BIOACOUSTICS, V28, P257, DOI 10.1080/09524622.2018.1431958
CR  - Taigman Y., 2014, DEEPFACE CLOSING GAP
CR  - Trifa VM, 2008, J ACOUST SOC AM, V123, P2424, DOI 10.1121/1.2839017
CR  - U. S. Department of Agriculture [USDA] Forest Service and U. S. Department of Interior [USDI] Bureau of Land Management, 1994, FIN SUPPL ENV IMP ST
CR  - U. S. Fish and Wildlife Service, 1990, FED REGISTER, V55, P26114
CR  - Wiens JD, 2014, WILDLIFE MONOGR, V185, P1, DOI 10.1002/wmon.1009
CR  - Wood CM, 2019, ECOL INDIC, V98, P492, DOI 10.1016/j.ecolind.2018.11.018
CR  - Wrege PH, 2017, METHODS ECOL EVOL, V8, P1292, DOI 10.1111/2041-210X.12730
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - MAR
PY  - 2020
VL  - 6
IS  - 1
SP  - 79
EP  - 92
DO  - 10.1002/rse2.125
AN  - WOS:000519756100006
N1  - Times Cited in Web of Science Core Collection:  16
Total Times Cited:  17
Cited Reference Count:  37
ER  -

TY  - JOUR
AU  - Lee, H
AU  - Seo, B
AU  - Koellner, T
AU  - Lautenbach, S
TI  - Mapping cultural ecosystem services 2.0-Potential and shortcomings from unlabeled crowd sourced images
T2  - ECOLOGICAL INDICATORS
LA  - English
KW  - Social media photo services
KW  - Cloud computing
KW  - Tagging
KW  - Keywords network analysis
KW  - Mapping ecosystem services
KW  - Big data analysis
KW  - SOCIAL MEDIA
KW  - COMMUNITY STRUCTURE
KW  - NETWORKS
KW  - LANDSCAPES
KW  - INDICATORS
KW  - CENTRALITY
KW  - DEMAND
KW  - VALUES
KW  - FOREST
AB  - The volume of accessible geotagged crowdsourced photos has increased. Such data include spatial, temporal, and thematic information on recreation and outdoor activities, thus can be used to quantify the demand for cultural ecosystem services (CES). So far photo content has been analyzed based on user-labeled tags or the manual labeling of photos. Both approaches are challenged with respect to consistency and cost-efficiency, especially for large-scale studies with an enormous volume of photos. In this study, we aim at developing a new method to analyze the content of large volumes of photos and to derive indicators of socio-cultural usage of landscapes. The method uses machine-learning and network analysis to identify clusters of photo content that can be used as an indicator of cultural services provided by landscapes. The approach was applied in the Mulde river basin in Saxony, Germany. All public Flickr photos (n = 12,635) belonging to the basin were tagged by deep convolutional neural networks through a cloud computing platform, Clarifai. The machine-predicted tags were analyzed by a network analysis that leads to nine hierarchical clusters. Those clusters were used to distinguish between photos related to CES (65%) and not related to CES (35%). Among the nine clusters, two clusters were related to CES: 'landscape aesthetics' and 'existence'. This step allowed mapping of different aspects of CES and separation of non-relevant photos from further analysis. We further analyzed the impact of protected areas on the spatial pattern of CES and not-related CES photos. The presence of protected areas had a significant positive impact on the areas with both 'landscape aesthetics' and 'existence' photos: the total number of days in each mapping unit where at least one photo was taken by a user ('photo-user-day') increased with the share of protected areas around the location. The presented approach has shown its potential for reliable mapping of socio-cultural uses of landscapes. It is expected to scale well with large numbers of photos and to be easily transferable to different regions.
AD  - Univ Bonn, Fac Agr, Land Use Modeling & Ecosyst Serv, Nussallee 1, D-53115 Bonn, GermanyAD  - Karlsruhe Inst Technol, Inst Meteorol & Climate Res, Atmospher Environm Res IMK IFU, Kreuzeckbahnstr 19, D-82467 Garmisch Partenkirchen, GermanyAD  - Kangwon Natl Univ, Inst Environm Res, Gangwondaehak Ro 1, Chunchon 24341, South KoreaAD  - Univ Bayreuth, Professorship Ecol Serv, Bayreuth Ctr Ecol & Environm Res BayCEER, Univ Str 30, D-95440 Bayreuth, GermanyAD  - Heidelberg Univ, GISci Res Grp, Inst Geog, D-69120 Heidelberg, GermanyC3  - University of BonnC3  - Helmholtz AssociationC3  - Karlsruhe Institute of TechnologyC3  - Kangwon National UniversityC3  - University of BayreuthC3  - League of European Research Universities - LERUC3  - Ruprecht Karls University HeidelbergFU  - EU FP-7 project OPERAs [584 308393]; German Academic Exchange Service (DAAD); Agenda Program of the Rural Development Administration in South Korea [PJ00997802]; Asian Institute for Energy, Environment & Sustainability, Seoul National University, South Korea; CONNECT from BiodiveERsA 2011 [BMBF: FK 01LC1103A]
FX  - This project was funded by the EU FP-7 project OPERAs (Grant No. 584 308393) and CONNECT from BiodiveERsA 2011 (BMBF: FK 01LC1103A). We acknowledge the support of the German Academic Exchange Service (DAAD) in the form of an International Travel Grant which enabled H.L. to attend the Natural Capital Symposium 2017. The work of B.S. was supported by Agenda Program of the Rural Development Administration (PJ00997802) in South Korea and Asian Institute for Energy, Environment & Sustainability, Seoul National University, South Korea. B.S. thanks to Dowon Lee who introduced network analysis for the first time.
CR  - Anderson A., 2008, TECHNICAL REPORT
CR  - [Anonymous], 2004, PHYS REV E, DOI DOI 10.1103/PhysRevE.69.026113
CR  - Bastian M., 2009, INT AAAI C WEBL SOC, DOI DOI 10.13140/2.1.1341.1520
CR  - Bivand R., 2018, RGEOS INTERFACE GEOM, P3
CR  - Bivand Rs., 2017, SPATIAL DEPENDENCE W
CR  - BONACICH P, 1987, AM J SOCIOL, V92, P1170, DOI 10.1086/228631
CR  - Brown G, 2015, ECOSYST SERV, V13, P119, DOI 10.1016/j.ecoser.2014.10.007
CR  - Byers BA, 2001, HUM ECOL, V29, P187, DOI 10.1023/A:1011012014240
CR  - Casalegno S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068437
CR  - Cattuto C, 2007, P NATL ACAD SCI USA, V104, P1461, DOI 10.1073/pnas.0610487104
CR  - Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111
CR  - Cord AF, 2015, LANDSCAPE URBAN PLAN, V144, P151, DOI 10.1016/j.landurbplan.2015.08.015
CR  - Crossman ND, 2013, ECOSYST SERV, V4, P4, DOI 10.1016/j.ecoser.2013.02.001
CR  - Csardi G, 2006, INTERJOURNAL COMPLEX, V5, P19
CR  - Daniel TC, 2012, P NATL ACAD SCI USA, V109, P8812, DOI 10.1073/pnas.1114773109
CR  - Danon L, 2005, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2005/09/P09008
CR  - Delgado-Aguilar MJ, 2017, ECOL INDIC, V73, P460, DOI 10.1016/j.ecolind.2016.10.020
CR  - Di Minin E, 2015, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00063
CR  - Eisenstein J, 2015, ARXIV PREPRINT ARXIV
CR  - Ford JD, 2016, P NATL ACAD SCI USA, V113, P10729, DOI 10.1073/pnas.1614023113
CR  - Fortunato S, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.056104
CR  - Gee K, 2010, ECOL COMPLEX, V7, P349, DOI 10.1016/j.ecocom.2010.02.008
CR  - Gliozzo G, 2016, ECOL SOC, V21, DOI 10.5751/ES-08436-210306
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
CR  - Hijmans R. J., 2016, R PACKAGE VERSION, V2, P5
CR  - Hodeck A., 2016, Journal of Sport & Tourism, V20, P335
CR  - Hu J, 2012, J COMPUT SCI TECH-CH, V27, P527, DOI 10.1007/s11390-012-1241-0
CR  - Isenberg P, 2017, IEEE T VIS COMPUT GR, V23, P771, DOI 10.1109/TVCG.2016.2598827
CR  - Jacomy M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098679
CR  - Keeler BL, 2015, FRONT ECOL ENVIRON, V13, P76, DOI 10.1890/140124
CR  - La Rosa D, 2016, ECOL INDIC, V61, P74, DOI 10.1016/j.ecolind.2015.04.028
CR  - Landau D., 2005, GUIDE MONTE CARLO SI
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lee KH, 2011, ACTA HORTIC, V900, P251, DOI 10.1109/ICDMW.2011.171
CR  - Luke D., 2017, USERNETR DATA SETS U
CR  - Luke Douglas A., 2015, USERS GUIDE NETWORK, DOI [10.1007/978-3-319-23883-8, DOI 10.1007/978-3-319-23883-8]
CR  - Pastur GM, 2016, LANDSCAPE ECOL, V31, P383, DOI 10.1007/s10980-015-0254-9
CR  - Milcu AI, 2013, ECOL SOC, V18, DOI 10.5751/ES-05790-180344
CR  - Mousselly-Sergieh H, 2013, INFORSID, P319
CR  - Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
CR  - Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066133
CR  - Norton LR, 2012, LAND USE POLICY, V29, P449, DOI 10.1016/j.landusepol.2011.09.002
CR  - Oteros-Rozas E, 2018, ECOL INDIC, V94, P74, DOI 10.1016/j.ecolind.2017.02.009
CR  - Pebesma E.J., 2005, R News, V5, P9, DOI DOI 10.1007/978-1-4614-7618-4_2
CR  - Plieninger T, 2013, LAND USE POLICY, V33, P118, DOI 10.1016/j.landusepol.2012.12.013
CR  - Pons P, 2005, LECT NOTES COMPUT SC, V3733, P284
CR  - R Core Team, 2019, R LANG ENV STAT COMP
CR  - Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/neco_a_00990, 10.1162/NECO_a_00990]
CR  - Regelman G., 2006, 15 WORLD WID WEB C W, P15
CR  - Richards DR, 2018, ECOSYST SERV, V31, P318, DOI 10.1016/j.ecoser.2017.09.004
CR  - Richards DR, 2015, ECOL INDIC, V53, P187, DOI 10.1016/j.ecolind.2015.01.034
CR  - Ruths D, 2014, SCIENCE, V346, P1063, DOI 10.1126/science.346.6213.1063
CR  - Santonen T., 2016, P 27 ISPIM INN C
CR  - Schmitz P., 2006, P COLL WEB TAGG WORK, P206
CR  - Schnegg M., 1996, J COMPUT APPL MATH, V8, P7, DOI DOI 10.1177/1525822X960080020601
CR  - Seo B, 2016, IEEE J-STARS, V9, P3941, DOI 10.1109/JSTARS.2016.2544802
CR  - Sharp R, 2016, INVEST VERSION USERS
CR  - Sigurbjornsson B, 2008, P 17 INT C WORLD WID
CR  - Sonter LJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162372
CR  - Sood G., 2016, CLARIFAI R CLIENT CL
CR  - Tenerelli P, 2016, ECOL INDIC, V64, P237, DOI 10.1016/j.ecolind.2015.12.042
CR  - Thiagarajah J, 2015, AMBIO, V44, P666, DOI 10.1007/s13280-015-0647-7
CR  - Tieskens KF, 2017, LAND USE POLICY, V62, P29, DOI 10.1016/j.landusepol.2016.12.001
CR  - Tisselli E, 2010, COMMUN ACM, V53, P141, DOI 10.1145/1787234.1787270
CR  - Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
CR  - TV Erzegebirge, 2014, TECHNICAL REPORT
CR  - van Berkel DB, 2014, ECOL INDIC, V37, P163, DOI 10.1016/j.ecolind.2012.06.025
CR  - van Zanten BT, 2016, P NATL ACAD SCI USA, V113, P12974, DOI 10.1073/pnas.1614158113
CR  - von Heland J, 2014, GLOBAL ENVIRON CHANG, V24, P251, DOI 10.1016/j.gloenvcha.2013.11.003
CR  - WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
CR  - Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8_3
CR  - Wood SA, 2013, SCI REP-UK, V3, DOI 10.1038/srep02976
CR  - Yang Z, 2016, SCI REP-UK, V6, DOI 10.1038/srep30750
CR  - Yi S, 2012, SCIENTOMETRICS, V90, P1015, DOI 10.1007/s11192-011-0560-1
CR  - Yon G. V., 2015, R PACKAGE VERSION, P3
CR  - Yoshimura N, 2017, ECOSYST SERV, V24, P68, DOI 10.1016/j.ecoser.2017.02.009
CR  - Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
CR  - Zoderer BM, 2016, LAND USE POLICY, V56, P251, DOI 10.1016/j.landusepol.2016.05.004
CR  - Zuur AF., 2009, MIXED EFFECTS MODELS, DOI [10.1007/978-0-387-87458-6, DOI 10.1007/978-0-387-87458-6]
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - JAN
PY  - 2019
VL  - 96
SP  - 505
EP  - 515
DO  - 10.1016/j.ecolind.2018.08.035
AN  - WOS:000464889600050
N1  - Times Cited in Web of Science Core Collection:  43
Total Times Cited:  45
Cited Reference Count:  80
ER  -

TY  - JOUR
AU  - Munian, Y
AU  - Martinez-Molina, MEA
AU  - Alamaniotis, M
TI  - Active advanced arousal system to alert and avoid the crepuscular animal based vehicle collision
T2  - INTELLIGENT DECISION TECHNOLOGIES-NETHERLANDS
LA  - English
KW  - Animal detection
KW  - Thermography
KW  - HOG
KW  - CNN
KW  - AI
KW  - alert/response system
KW  - nocturnal
AB  - Animal Vehicle Collision (AVC) is relatively an evolving source of fatality resulting in the deficit of wildlife conservancy along with carnage. It's a globally distressing and disturbing experience that causes monetary damage, injury, and human-animal mortality. Roadkill has always been atop the research domain and serendipitously provided heterogeneous solutions for collision mitigation and prevention. Despite the abundant solution availability, this research throws a new spotlight on wildlife-vehicle collision mitigation using highly efficient artificial intelligence during nighttime hours. This study focuses mainly on arousal mechanisms of the "Histogram of Oriented Gradients (HOG)" intelligent system with extracted thermography image features, which are then processed by a trained, convolutional neural network (1D-CNN). The above computer vision - deep learning-based alert system has an accuracy between 94%, and 96% on the arousal mechanisms with the empowered real-time data set utilization.
AD  - Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX USAAD  - Univ Texas San Antonio, Dept Architecture, San Antonio, TX USAC3  - University of Texas SystemC3  - University of Texas at San Antonio (UTSA)C3  - University of Texas SystemC3  - University of Texas at San Antonio (UTSA)CR  - [Anonymous], 2012, INSURANCE J CAR DEER
CR  - Benten A, 2018, ACCIDENT ANAL PREV, V120, P64, DOI 10.1016/j.aap.2018.08.003
CR  - Christiansen P, 2014, SENSORS-BASEL, V14, P13778, DOI 10.3390/s140813778
CR  - Druta C, 2020, TRANSPORT RES REC, V2674, P680, DOI 10.1177/0361198120936651
CR  - Grace MK, 2017, ACCIDENT ANAL PREV, V109, P55, DOI 10.1016/j.aap.2017.10.003
CR  - Lao YT, 2012, J TRANSP ENG, V138, P520, DOI 10.1061/(ASCE)TE.1943-5436.0000351
CR  - McGowen PT., 2003, OVERVIEW ANIMAL DETE
CR  - Peeters J, 2018, OPTIM ENG, V19, P163, DOI 10.1007/s11081-017-9368-z
CR  - Saleh K, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18061913
CR  - Santhi V, 2017, ADV CIV IND ENG BOOK, P1, DOI 10.4018/978-1-5225-2423-6
CR  - Sawyer H, 2016, WILDLIFE SOC B, V40, P211, DOI 10.1002/wsb.650
CR  - Sharma SU, 2017, IEEE ACCESS, V5, P347, DOI 10.1109/ACCESS.2016.2642981
CR  - Sibanda V, 2019, PROC CIRP, V84, P755, DOI 10.1016/j.procir.2019.04.175
CR  - Sisiopiku VP, 2005, J TRANSP ENG, V131, P205, DOI 10.1061/(ASCE)0733-947X(2005)131:3(205)
CR  - Statefarm Insurance, FACTS STAT DEER VEH
CR  - U.S. Department of Transportation, FED HIGHW ADM
CR  - Uwanuakwa ID, 2020, INTERNATIONAL CONFERENCE ON TRANSPORTATION AND DEVELOPMENT 2020: TRANSPORTATION SAFETY, P194
CR  - Wang S, 2012, P INT C IM PROC COMP, P1
CR  - Yuvaraj M, 2020, 11 INT C INF INT SYS
CR  - Yuvaraj M, 2021, INT C TRANSP DEV 202, P438, DOI [10.1061/ 9780784483534.038, DOI 10.1061/9780784483534.038]
PU  - IOS PRESS
PI  - AMSTERDAM
PA  - NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS
PY  - 2021
VL  - 15
IS  - 4
SP  - 707
EP  - 720
DO  - 10.3233/IDT-210204
AN  - WOS:000749999000015
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  20
ER  -

TY  - JOUR
AU  - Duporge, I
AU  - Isupova, O
AU  - Reece, S
AU  - Macdonald, DW
AU  - Wang, TJ
TI  - Using very-high-resolution satellite imagery and deep learning to detect and count African elephants in heterogeneous landscapes
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - Machine Learning
KW  - Convolutional Neural Network
KW  - Aerial Survey
KW  - Wildlife Census
KW  - Endangered Species
KW  - Conservation
KW  - Anthropocene
KW  - Object Detection
KW  - SOUTH-AFRICA
KW  - LANDSAT TM
KW  - WILDLIFE
KW  - DEGRADATION
KW  - ALGORITHMS
KW  - ABUNDANCE
KW  - PENGUINS
KW  - CENSUS
KW  - ISLAND
KW  - LAKE
AB  - Satellites allow large-scale surveys to be conducted in short time periods with repeat surveys possible at intervals of <24 h. Very-high-resolution satellite imagery has been successfully used to detect and count a number of wildlife species in open, homogeneous landscapes and seascapes where target animals have a strong contrast with their environment. However, no research to date has detected animals in complex heterogeneous environments or detected elephants from space using very-high-resolution satellite imagery and deep learning. In this study, we apply a Convolution Neural Network (CNN) model to automatically detect and count African elephants in a woodland savanna ecosystem in South Africa. We use WorldView-3 and 4 satellite data -the highest resolution satellite imagery commercially available. We train and test the model on 11 images from 2014 to 2019. We compare the performance accuracy of the CNN against human accuracy. Additionally, we apply the model on a coarser resolution satellite image (GeoEye-1) captured in Kenya, without any additional training data, to test if the algorithm can generalize to an elephant population outside of the training area. Our results show that the CNN performs with high accuracy, comparable to human detection capabilities. The detection accuracy (i.e., F2 score) of the CNN models was 0.78 in heterogeneous areas and 0.73 in homogenous areas. This compares with the detection accuracy of the human labels with an averaged F2 score 0.77 in heterogeneous areas and 0.80 in homogenous areas. The CNN model can generalize to detect elephants in a different geographical location and from a lower resolution satellite. Our study demonstrates the feasibility of applying state-of-the-art satellite remote sensing and deep learning technologies for detecting and counting African elephants in heterogeneous landscapes. The study showcases the feasibility of using high resolution satellite imagery as a promising new wildlife surveying technique. Through creation of a customized training dataset and application of a Convolutional Neural Network, we have automated the detection of elephants in satellite imagery with accuracy as high as human detection capabilities. The success of the model to detect elephants outside of the training data site demonstrates the generalizability of the technique.
AD  - Univ Oxford, Recanati Kaplan Ctr, Dept Zool, Wildlife Conservat Res Unit, Tubney, EnglandAD  - Univ Bath, Dept Comp Sci, Bath, Avon, EnglandAD  - Univ Oxford, Dept Engn Sci, Oxford, EnglandAD  - Univ Twente, Fac Geoinformat Sci & Earth Observat ITC, Enschede, NetherlandsC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - University of BathC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - University of TwenteFU  - DigitalGlobe Foundation (Maxar Technologies); European Space Agency; Wildlife Conservation Research Unit, University of Oxford from the Ralph Mistler Trust; LLoyd's Register Foundation through the Alan Turing Institute's Data Centric Engineering programme
FX  - We greatly appreciate the generous support by the DigitalGlobe Foundation (Maxar Technologies) and European Space Agency for awarding the image grants to support this work without which this research would not have been possible. We are also grateful to Hexagon Geospatial for providing a complementary license to ERDAS Imagine which enabled us to process the satellite imagery. We are grateful to all the human volunteer labellers for taking the time to label the images and giving us a point of comparison to the CNN. ID is grateful for a bequest to the Wildlife Conservation Research Unit, University of Oxford from the Ralph Mistler Trust which supported her to carry out this research. SR is grateful for funding from the LLoyd's Register Foundation through the Alan Turing Institute's Data Centric Engineering programme. We are grateful to Sofia Minano Gonzalez and Hannah Cubaynes for their valuable comments on the manuscript.
CR  - Abileah R., 2002, US NAVY J UNDERWATER, V52
CR  - Barber-Meyer SM, 2007, POLAR BIOL, V30, P1565, DOI 10.1007/s00300-007-0317-8
CR  - Barnes RFW, 1997, J WILDLIFE MANAGE, V61, P1384, DOI 10.2307/3802142
CR  - Barnosky AD, 2014, ANTHROPOCENE REV, V1, P78, DOI 10.1177/2053019613516290
CR  - Borowicz A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212532
CR  - Bowler E., 2019, REMOTE SENSING, V12
CR  - Bowley C., 2018, LECT NOTES COMPUTER, V10860
CR  - Bruijning M, 2018, METHODS ECOL EVOL, V9, P965, DOI 10.1111/2041-210X.12975
CR  - Buma WG, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11212534
CR  - Cao CQ, 2019, IEEE ACCESS, V7, P106838, DOI 10.1109/ACCESS.2019.2932731
CR  - Cardinale B, 2012, SCIENCE, V336, P552, DOI 10.1126/science.1222102
CR  - CAUGHLEY G, 1976, J WILDLIFE MANAGE, V40, P290, DOI 10.2307/3800428
CR  - Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
CR  - Christie KS, 2016, FRONT ECOL ENVIRON, V14, P242, DOI 10.1002/fee.1281
CR  - Cubaynes HC, 2019, MAR MAMMAL SCI, V35, P466, DOI 10.1111/mms.12544
CR  - Dascalu A, 2019, EBIOMEDICINE, V43, P107, DOI 10.1016/j.ebiom.2019.04.055
CR  - de Carvalho OA, 2015, REMOTE SENS-BASEL, V7, P6950, DOI 10.3390/rs70606950
CR  - du Toit JCO, 2014, WATER SA, V40, P453, DOI 10.4314/wsa.v40i3.8
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Ferreira AC, 2020, METHODS ECOL EVOL, V11, P1072, DOI 10.1111/2041-210X.13436
CR  - Fretwell PT, 2017, IBIS, V159, P481, DOI 10.1111/ibi.12482
CR  - Fretwell PT, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088655
CR  - Fretwell PT, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033751
CR  - Fretwell PT, 2009, GLOBAL ECOL BIOGEOGR, V18, P543, DOI 10.1111/j.1466-8238.2009.00467.x
CR  - Fullman TJ, 2017, KOEDOE, V59, DOI 10.4102/koedoe.v59i1.1326
CR  - Gara T.E.A., 2016, AFRICAN J ECOLOGY, V55, P860
CR  - Ginosar S., 2014, COMP VIS ECCV 2014 W
CR  - Goncalves BC, 2020, REMOTE SENS ENVIRON, V239, DOI 10.1016/j.rse.2019.111617
CR  - Gray P.C., 2018, METHODS ECOLOGY EVOL
CR  - Guirado E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50795-9
CR  - Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
CR  - Hema EM, 2017, AFR J ECOL, V55, P609, DOI 10.1111/aje.12397
CR  - Hollings T, 2018, METHODS ECOL EVOL, V9, P881, DOI 10.1111/2041-210X.12973
CR  - Hordiiuk D, 2019, 2019 IEEE 39TH INTERNATIONAL CONFERENCE ON ELECTRONICS AND NANOTECHNOLOGY (ELNANO), P454, DOI 10.1109/ELNANO.2019.8783822
CR  - Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
CR  - Hughes BJ, 2011, WILDLIFE BIOL, V17, P210, DOI 10.2981/10-106
CR  - Jachmann H, 2002, J APPL ECOL, V39, P841, DOI 10.1046/j.1365-2664.2002.00752.x
CR  - Kakembo V, 2015, RANGELAND ECOL MANAG, V68, P461, DOI 10.1016/j.rama.2015.08.004
CR  - Kao AB, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2018.0130
CR  - Kellenberger B, 2019, IEEE T GEOSCI REMOTE, V57, P9524, DOI 10.1109/TGRS.2019.2927393
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Kiage LM, 2007, INT J REMOTE SENS, V28, P4285, DOI 10.1080/01431160701241753
CR  - Koneff MD, 2008, J WILDLIFE MANAGE, V72, P1641, DOI 10.2193/2008-036
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Kusimi JM, 2008, GEOJOURNAL, V71, P249, DOI 10.1007/s10708-008-9172-6
CR  - LaRue MA, 2014, POLAR BIOL, V37, P507, DOI 10.1007/s00300-014-1451-8
CR  - LaRue MA, 2018, POLAR BIOL, V41, P2621, DOI 10.1007/s00300-018-2384-4
CR  - LaRue MA, 2017, CONSERV BIOL, V31, P213, DOI 10.1111/cobi.12809
CR  - LaRue MA, 2015, WILDLIFE SOC B, V39, P772, DOI 10.1002/wsb.596
CR  - LaRue MA, 2011, POLAR BIOL, V34, P1727, DOI 10.1007/s00300-011-1023-0
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - LOFFLER E, 1980, REMOTE SENS ENVIRON, V9, P47, DOI 10.1016/0034-4257(80)90046-2
CR  - Lu D, 2004, INT J REMOTE SENS, V25, P2365, DOI 10.1080/0143116031000139863
CR  - Lynch HJ, 2014, AUK, V131, P457, DOI 10.1642/AUK-14-31.1
CR  - Lynch HJ, 2012, POLAR BIOL, V35, P963, DOI 10.1007/s00300-011-1138-3
CR  - Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
CR  - Mairea F., 2013, IEEE RSJ INT C INT R
CR  - Maki T, 2019, IFAC PAPERSONLINE, V52, P86, DOI 10.1016/j.ifacol.2019.12.288
CR  - McMahon CR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092613
CR  - Meng QM, 2011, INT J APPL EARTH OBS, V13, P120, DOI 10.1016/j.jag.2010.08.002
CR  - Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
CR  - Mierswa I, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P411, DOI 10.1145/2939672.2945365
CR  - Mulero-Pazmany M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178448
CR  - Ngo TV, 2019, ADVANCES IN ENGINEERING MATERIALS, STRUCTURES AND SYSTEMS: INNOVATIONS, MECHANICS AND APPLICATIONS, P197
CR  - Olczak J, 2017, ACTA ORTHOP, V88, P581, DOI 10.1080/17453674.2017.1344459
CR  - Oliveira ER, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111305
CR  - Pang JM, 2019, IEEE T GEOSCI REMOTE, V57, P5512, DOI 10.1109/TGRS.2019.2899955
CR  - Petersen S., 2019, USING MACHINE LEARNI
CR  - Poulsen JR, 2017, CURR BIOL, V27, pR134, DOI 10.1016/j.cub.2017.01.023
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Romero JR, 2013, COMPUT ELECTRON AGR, V96, P173, DOI 10.1016/j.compag.2013.05.006
CR  - Rulinda CM, 2010, INT J APPL EARTH OBS, V12, pS63, DOI 10.1016/j.jag.2009.10.008
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Sasamal SK, 2008, INT J REMOTE SENS, V29, P4865, DOI 10.1080/01431160701814336
CR  - Schlossberg S, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0164904
CR  - Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
CR  - Schneider S., DEEP LEARNING OBJECT
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Sharma N, 2018, LECT NOTES ARTIF INT, V11320, P224, DOI 10.1007/978-3-030-03991-2_23
CR  - Shiu Y., 2014, J ACOUST SOC AM, V135, P2334
CR  - Sibanda M, 2012, INT J APPL EARTH OBS, V19, P286, DOI 10.1016/j.jag.2012.05.014
CR  - Skogen K, 2018, J NAT CONSERV, V44, P12, DOI 10.1016/j.jnc.2018.06.001
CR  - Smit J, 2019, ORYX, V53, P368, DOI 10.1017/S0030605317000345
CR  - Soltis J, 2016, ENDANGER SPECIES RES, V31, P1, DOI 10.3354/esr00746
CR  - Stapleton S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0101513
CR  - Strigl D, 2010, EUROMICRO WORKSHOP P, P317, DOI 10.1109/PDP.2010.43
CR  - Szegedy C, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4278
CR  - Tambling CJ, 2012, ECOLOGY, V93, P1297
CR  - Thito K, 2016, AFR J AQUAT SCI, V41, P267, DOI 10.2989/16085914.2016.1173009
CR  - Torney CJ, 2019, METHODS ECOL EVOL, V10, P779, DOI 10.1111/2041-210X.13165
CR  - Tzutalin, 2015, TZUT LAB
CR  - van Wilgen NJ, 2016, INT J CLIMATOL, V36, P706, DOI 10.1002/joc.4377
CR  - Velasco M., 2009, QUICKBIRDS EYE VIEW
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Wickler W, 1997, ETHOLOGY, V103, P365
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Witharana C, 2016, ISPRS J PHOTOGRAMM, V113, P124, DOI 10.1016/j.isprsjprs.2015.12.009
CR  - Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
CR  - Yang Z, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115989
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - SEP
PY  - 2021
VL  - 7
IS  - 3
SP  - 369
EP  - 381
DO  - 10.1002/rse2.195
AN  - WOS:000600924000001
N1  - Times Cited in Web of Science Core Collection:  13
Total Times Cited:  14
Cited Reference Count:  100
ER  -

TY  - CPAPER
AU  - Evans, BC
AU  - Tucker, A
AU  - Wearn, OR
AU  - Carbone, C
ED  - Koprinska, I
ED  - Kamp, M
ED  - Appice, A
ED  - Loglisci, C
ED  - Antonie, L
ED  - Zimmermann, A
ED  - Guidotti, R
ED  - Ozgobek, O
TI  - Reasoning About Neural Network Activations: An Application in Spatial Animal Behaviour from Camera Trap Classifications
T2  - ECML PKDD 2020 WORKSHOPS
LA  - English
CP  - European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)
KW  - Animal behavior
KW  - Convolutional Neural Networks
KW  - Bayesian networks
KW  - Activation based reasoning
AB  - Camera traps are a vital tool for ecologists to enable them to monitor wildlife over large areas in order to determine population changes, habitat, and behaviour. As a result, camera-trap datasets are rapidly growing in size. Recent advancements in Artificial Neural Networks (ANN) have emerged in image recognition and detection tasks which are now being applied to automate camera-trap labelling. An ANN designed for species detection will output a set of activations, representing the observation of a particular species (an individual class) at a particular location and time and are often used as a way to calculate population sizes in different regions. Here we go one step further and explore how we can combine ANNs with probabilistic graphical models to reason about animal behaviour using the ANN outputs over different geographical locations. By using the output activations from ANNs as data along with the trap's associated spatial coordinates, we build spatial Bayesian networks to explore species behaviours (how they move and distribute themselves) and interactions (how they distribute in relation to other species). This combination of probabilistic reasoning and deep learning offers many advantages for large camera trap projects as well as potential for other remote sensing datasets that require automated labelling.
AD  - Brunel Univ London, Uxbridge UB8 3PH, Middx, EnglandAD  - Zool Soc London, Inst Zool, London NW1 4RY, EnglandC3  - Brunel UniversityC3  - Zoological Society of LondonFU  - NERC (The Natural Environment Research Council)
FX  - Benjamin C. Evans work is funded by NERC (The Natural Environment Research Council).
CR  - Beery S, 2019, ARXIV190706772
CR  - Devlin J, 2019, ARXIV181004805 CS, V1
CR  - Franco C, 2016, ENVIRON MODELL SOFTW, V80, P132, DOI 10.1016/j.envsoft.2016.02.029
CR  - Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
CR  - Maldonado AD, 2019, ENVIRON MODELL SOFTW, V118, P281, DOI 10.1016/j.envsoft.2019.04.011
CR  - Oord Avd, 2016, ARXIV160903499
CR  - Rowcliffe JM, 2008, ANIM CONSERV, V11, P185, DOI 10.1111/j.1469-1795.2008.00180.x
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - SPIRTES P, 1993, CAUSATION PREDICTION
CR  - Trifonova N, 2015, ECOL INFORM, V30, P142, DOI 10.1016/j.ecoinf.2015.10.003
CR  - Uusitalo L, 2007, ECOL MODEL, V203, P312, DOI 10.1016/j.ecolmodel.2006.11.033
CR  - Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
PY  - 2020
VL  - 1323
SP  - 26
EP  - 37
DO  - 10.1007/978-3-030-65965-3_2
AN  - WOS:000724139600002
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  12
ER  -

TY  - CPAPER
AU  - Shrestha, R
AU  - Glackin, C
AU  - Wall, J
AU  - Cannings, N
ED  - Farkas, I
ED  - Masulli, P
ED  - Otte, S
ED  - Wermter, S
TI  - Bird Audio Diarization with Faster R-CNN
T2  - ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2021, PT I
LA  - English
CP  - 30th International Conference on Artificial Neural Networks (ICANN)
KW  - Deep neural networks
KW  - Audio classification
KW  - Diarization
KW  - Automatic wildlife monitoring
AB  - Birds embody particular phonic and visual traits that distinguish them from 10,000 distinct bird species worldwide. Birds are also perceived to be indicators of biodiversity due to their propensity for responding to changes in their environment. An effective, automatic wildlife monitoring system based on bird bioacoustics, which can support manual classification, can be pivotal for the protection of the environment and endangered species. In modern machine learning, real-life bird audio classification is still considered as an esoteric challenge owing to the convoluted patterns present in bird song, and the complications that arise when numerous bird species are present in a common setting. Existing avian bioacoustic monitoring systems struggle when multiple bird species are present in an audio segment. To overcome these challenges, we propose a novel Faster Region-Based Convolutional Neural Network bird audio diarization system that incorporates object detection in the spectral domain and performs diarization of 50 bird species to effectively tackle the `which bird spoke when?' problem. Benchmark results are presented using the Bird Songs from Europe dataset achieving a Diarization Error Rate of 21.81, Jaccard Error Rate of 20.94 and F1, precision and recall values of 0.85, 0.83 and 0.87 respectively.
AD  - Intelligent Voice Ltd, London, EnglandAD  - Univ East London, London, EnglandC3  - University of East LondonCR  - Anderson SE, 1996, J ACOUST SOC AM, V100, P1209, DOI 10.1121/1.415968
CR  - Biewald L., 2020, EXPT TRACKING WEIGHT
CR  - Dong X., 2020, J PHYS C SERIES, V1544
CR  - Eibl, 2017, WORKING NOTES CLEF
CR  - Fagerlund S, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/38637
CR  - Howard J, 2020, INFORMATION, V11, DOI 10.3390/info11020108
CR  - Huang Z., 2020, IEEE INT C AC SPEECH
CR  - Incze A, 2018, I S INTELL SYST INFO, P295, DOI 10.1109/SISY.2018.8524677
CR  - Kahl S., 2020, C LABS EV FOR CLEF T
CR  - Koh C.Y., 2019, WORK NOT C LABS EV F
CR  - Lasseck M., 2013, P INT S NEUR INF SCA, P176
CR  - Lassek M, 2019, WORK NOT C LABS ENV
CR  - Lima F, 2020, AUDIO CLASSIFICATION
CR  - Lima F, 2020, BIRD SONGS EUROPE XE, DOI [10.34740/kaggle/dsv/1029985, DOI 10.34740/KAGGLE/DSV/1029985]
CR  - Maina CW, 2015, AFRICON
CR  - McFee Brian, 2020, LIBROSA LIBROSA 0 8
CR  - McIlraith AL, 1997, 1997 CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, CONFERENCE PROCEEDINGS, VOLS I AND II, P63, DOI 10.1109/CCECE.1997.614790
CR  - Mporas I, 2012, PROC INT C TOOLS ART, P778, DOI 10.1109/ICTAI.2012.110
CR  - Muhling M., 2020, WORK NOT C LABS EV F
CR  - Narasimhan R, 2017, INT CONF ACOUST SPEE, P146, DOI 10.1109/ICASSP.2017.7952135
CR  - Ng HW, 2013, IEEE INT WORKS MACH, DOI 10.1109/MLSP.2013.6661933
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Robert J., 2018, PYDUB
CR  - Schuller B, 2013, SIGNALS COMMUNICATIO, P99, DOI [10.1007/978-3-642-36806-6, DOI 10.1007/978-3-642-36806-6]
CR  - Silla CN, 2013, IEEE SYS MAN CYBERN, P1895, DOI 10.1109/SMC.2013.326
CR  - Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58
CR  - Sprengel E., 2016, WORKING NOTES CLEF 2, P547
CR  - Vazquez L., 2020, ICEVISION AGNOSTIC O
CR  - Vellinga W., XENO CANTO BIRD SOUN, DOI [10.15468/qv0ksn, DOI 10.15468/QV0KSN]
CR  - Weerasena H, 2018, TENCON IEEE REGION, P0235, DOI 10.1109/TENCON.2018.8650196
CR  - Zhao Z, 2017, ECOL INFORM, V39, P99, DOI 10.1016/j.ecoinf.2017.04.003
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
PY  - 2021
VL  - 12891
SP  - 415
EP  - 426
DO  - 10.1007/978-3-030-86362-3_34
AN  - WOS:000711965200034
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Tseng, YC
AU  - Eskelson, BNI
AU  - Martin, K
AU  - LeMay, V
TI  - Automatic bird sound detection: logistic regression based acoustic occupancy model
T2  - BIOACOUSTICS-THE INTERNATIONAL JOURNAL OF ANIMAL SOUND AND ITS RECORDING
LA  - English
KW  - Autonomous recording units (ARUs)
KW  - bird song recognition
KW  - biodiversity monitoring
KW  - convolutional neural networks
KW  - IEEE bird sound detection challenge
KW  - CLASSIFICATION
KW  - RECORDINGS
KW  - INDEXES
AB  - Avian bioacoustics research was greatly assisted by the introduction of autonomous recording units, which not only allow remote monitoring but also make large-scale studies possible. However, manual inspection of acoustic recordings becomes more challenging with increasingly larger datasets. In this study, we developed a logistic model to predict the probability of bird presence in audio recordings using sound frequency percentiles. The acoustic recordings covered bird songs and calls in a wide range of environments (e.g. grassland, forest, urban areas) along with the presence of noise due to weather, traffic, insects, and human speech. Based on leave-one-out cross-validation, our final logistic model resulted in a 75% overall accuracy and a 16% false negative rate using the optimal cut-off of 0.35 (i.e. probability >= 0.35 indicates the presence of birds). Compared with a convolutional neural network model using the same dataset, the logistic model was about seven times faster in terms of the processing time, but achieved slightly lower overall accuracy. This bird sound detection model using sound frequency percentiles in a logistic model opens up promising approaches to aid in automatic, accurate, and efficient analyses of large audio datasets for monitoring wildlife communities.
AD  - Univ British Columbia, Dept Forest Resources Management, Vancouver, BC, CanadaAD  - Univ British Columbia, Dept Forest & Conservat Sci, Vancouver, BC, CanadaAD  - Environm & Climate Change Canada, Pacific Wildlife Res Ctr, Vancouver, BC, CanadaC3  - University of British ColumbiaC3  - University of British ColumbiaC3  - Environment & Climate Change CanadaC3  - Canadian Wildlife ServiceC3  - Pacific Wildlife Research CentreFU  - Soundscape Association of Taiwan
FX  - This work was supported by the Soundscape Association of Taiwan.
CR  - Adavanne S, 2017, EUR SIGNAL PR CONF, P1729, DOI 10.23919/EUSIPCO.2017.8081505
CR  - BENNETCLARK HC, 1994, J EXP BIOL, V191, P291
CR  - BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209
CR  - Bonney R., 2007, EXEMPLARY SCI INFORM, P213
CR  - Brown A, 2019, APPL SOFT COMPUT, V81, DOI 10.1016/j.asoc.2019.105501
CR  - Byers BE., 2016, HDB BIRD BIOL, P355
CR  - Cakir E, 2017, EUR SIGNAL PR CONF, P1744, DOI 10.23919/EUSIPCO.2017.8081508
CR  - Chollet F., 2015, KERAS
CR  - de Oliveira AG, 2015, APPL ACOUST, V98, P34, DOI 10.1016/j.apacoust.2015.04.014
CR  - DELONG ER, 1988, BIOMETRICS, V44, P837, DOI 10.2307/2531595
CR  - Drake KL, 2016, WILDLIFE SOC B, V40, P346, DOI 10.1002/wsb.658
CR  - Dreiseitl S, 2002, J BIOMED INFORM, V35, P352, DOI 10.1016/S1532-0464(03)00034-0
CR  - Fagerlund S, 2014, THESIS
CR  - Garnett S., 2011, ACTION PLAN AUSTR BI
CR  - Hafner S, 2017, MONITOR ACOUSTIC TEM
CR  - Hedley RW, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00963-120106
CR  - Hosmer DW., 2013, LEMESHOW S STURDIVAN, DOI DOI 10.1002/0471722146
CR  - Joly A, 2019, OVERVIEW LIFECLEF 20
CR  - King DB, 2015, ACS SYM SER, V1214, P1
CR  - Klingbeil BT, 2015, PEERJ, V3, DOI 10.7717/peerj.973
CR  - Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
CR  - Kong QQ, 2017, EUR SIGNAL PR CONF, P1749, DOI 10.23919/EUSIPCO.2017.8081509
CR  - Lambert KTA, 2014, AUSTRAL ECOL, V39, P779, DOI 10.1111/aec.12143
CR  - Lima F, 2018, CONVOLUTIONAL NEURAL
CR  - McCullagh P, 1989, GEN LINEAR MODELS, V37
CR  - Mladenoff DJ, 1999, ECOL APPL, V9, P37, DOI 10.2307/2641166
CR  - Neter J., 1996, APPL LINEAR STAT MOD, V4, DOI 10.1128/cdli.3.4.369-370.1996
CR  - Pearce J, 2000, ECOL MODEL, V133, P225, DOI 10.1016/S0304-3800(00)00322-7
CR  - Pellegrini T, 2017, EUR SIGNAL PR CONF, P1734, DOI 10.23919/EUSIPCO.2017.8081506
CR  - Pieretti N, 2011, ECOL INDIC, V11, P868, DOI 10.1016/j.ecolind.2010.11.005
CR  - R Core Team, 2019, R LANG ENV STAT COMP
CR  - Rodriguez A, 2014, ECOL INFORM, V21, P133, DOI 10.1016/j.ecoinf.2013.12.006
CR  - Rognan Cameron B., 2012, Northwestern Naturalist, V93, P138
CR  - Rosenstock SS, 2002, AUK, V119, P46, DOI 10.1642/0004-8038(2002)119[0046:LCTCPA]2.0.CO;2
CR  - Rumsey F, 2012, SOUND AND RECORDING
CR  - Schrama T, 2007, P INT EXP M IT BAS D
CR  - Shonfield J, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00974-120114
CR  - Simons TR, 2007, AUK, V124, P986, DOI 10.1642/0004-8038(2007)124[986:EAOTAD]2.0.CO;2
CR  - SNEE RD, 1977, TECHNOMETRICS, V19, P415, DOI 10.2307/1267881
CR  - Sovern SG, 2014, J WILDLIFE MANAGE, V78, P1436, DOI 10.1002/jwmg.793
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - STOWELL D, 2016, IEEE INT WORKS MACH
CR  - Stowell D, 2014, P 53 INT C SEM AUD L
CR  - Sueur J, 2015, BIOSEMIOTICS-NETH, V8, P493, DOI 10.1007/s12304-015-9248-x
CR  - Sueur J, 2014, ACTA ACUST UNITED AC, V100, P772, DOI 10.3813/AAA.918757
CR  - Sueur J, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0004065
CR  - Sueur J, 2008, BIOACOUSTICS, V18, P213, DOI 10.1080/09524622.2008.9753600
CR  - Tanttu JT, 2006, BIOACOUSTICS, V15, P251, DOI 10.1080/09524622.2006.9753553
CR  - Tchernichovski O, 2000, ANIM BEHAV, V59, P1167, DOI 10.1006/anbe.1999.1416
CR  - Towsey, 2013, NOISE REMOVAL WAVE F
CR  - Towsey M, 2014, ECOL INFORM, V21, P110, DOI 10.1016/j.ecoinf.2013.11.007
CR  - Warblr, 2015, WARBLR ID UK BIRD SO
CR  - Wyse L., 2017, P 1 INT C DEEP LEARN, P37
PU  - TAYLOR & FRANCIS LTD
PI  - ABINGDON
PA  - 2-4 PARK SQUARE, MILTON PARK, ABINGDON OR14 4RN, OXON, ENGLAND
DA  - MAY 4
PY  - 2021
VL  - 30
IS  - 3
SP  - 324
EP  - 340
DO  - 10.1080/09524622.2020.1730241
AN  - WOS:000515699800001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  53
ER  -

TY  - JOUR
AU  - Su, JH
AU  - Piao, YC
AU  - Luo, Z
AU  - Yan, BP
TI  - Modeling Habitat Suitability of Migratory Birds from Remote Sensing Images Using Convolutional Neural Networks
T2  - ANIMALS
LA  - English
KW  - 1-D convolution
KW  - bar-head goose
KW  - convolutional neural network
KW  - DBIC
KW  - habitat preference
KW  - SPECIES DISTRIBUTION
KW  - CLASSIFICATION
KW  - AREAS
AB  - Simple Summary The understanding of the spatio-temporal distribution of the species habitats would facilitate wildlife resource management and conservation efforts. Existing methods have poor performance due to the limited availability of training samples. More recently, location-aware sensors have been widely used to track animal movements. The aim of the study was to generate suitability maps of bar-head geese using movement data coupled with environmental parameters, such as remote sensing images and temperature data. Therefore, we modified a deep convolutional neural network for the multi-scale inputs. The results indicate that the proposed method can identify the areas with the dense goose species around Qinghai Lake. In addition, this approach might also be interesting for implementation in other species with different niche factors or in areas where biological survey data are scarce.
   Abstract With the application of various data acquisition devices, a large number of animal movement data can be used to label presence data in remote sensing images and predict species distribution. In this paper, a two-stage classification approach for combining movement data and moderate-resolution remote sensing images was proposed. First, we introduced a new density-based clustering method to identify stopovers from migratory birds' movement data and generated classification samples based on the clustering result. We split the remote sensing images into 16 x 16 patches and labeled them as positive samples if they have overlap with stopovers. Second, a multi-convolution neural network model is proposed for extracting the features from temperature data and remote sensing images, respectively. Then a Support Vector Machines (SVM) model was used to combine the features together and predict classification results eventually. The experimental analysis was carried out on public Landsat 5 TM images and a GPS dataset was collected on 29 birds over three years. The results indicated that our proposed method outperforms the existing baseline methods and was able to achieve good performance in habitat suitability prediction.
AD  - Chinese Acad Sci, Comp Network Informat Ctr, Beijing 100190, Peoples R ChinaAD  - Univ Chinese Acad Sci, Beijing 100190, Peoples R ChinaC3  - Chinese Academy of SciencesC3  - Computer Network Information Center, CASC3  - Chinese Academy of SciencesC3  - University of Chinese Academy of Sciences, CASFU  - State Key Laboratory of Hydroscience and Engineering [sklhse-2017-B-03]; Natural Science Foundation of China [61361126011, 90912006]; Special Project of Informatization of Chinese Academy of Sciences in the "Twelfth Five-Year Plan" [XXH12504-1-06]
FX  - This research was partially supported by Open Research Fund Program of State Key Laboratory of Hydroscience and Engineering (sklhse-2017-B-03), the Natural Science Foundation of China under Grant No.61361126011 and No.90912006 and the Special Project of Informatization of Chinese Academy of Sciences in the "Twelfth Five-Year Plan" under Grant No. XXH12504-1-06. The authors thank USGS for providing the free Landsat data and thank USGS project on the Role of Wild Birds in highly pathogenic avian influenza H5N1. We also thank Meiyu Hao contributed analysis for Figure 9.
CR  - Bengio Y., 2010, P 13 INT C ARTIFICIA, P249, DOI DOI 10.1177/1753193409103364.
CR  - Bino G, 2008, INT J REMOTE SENS, V29, P3675, DOI 10.1080/01431160701772534
CR  - Birant D., 2007, ST DBSCAN ALGORITHM, P208
CR  - Bishop Mary A., 1997, Wildfowl, V48, P118
CR  - Bisrat SA, 2012, DIVERS DISTRIB, V18, P648, DOI 10.1111/j.1472-4642.2011.00867.x
CR  - Buermann W, 2008, J BIOGEOGR, V35, P1160, DOI 10.1111/j.1365-2699.2007.01858.x
CR  - BUSBY JR, 1986, AUST J ECOL, V11, P1, DOI 10.1111/j.1442-9993.1986.tb00912.x
CR  - Cord AF, 2014, J BIOGEOGR, V41, P736, DOI 10.1111/jbi.12225
CR  - Cui P, 2011, J ORNITHOL, V152, P83, DOI 10.1007/s10336-010-0552-6
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
CR  - Fu G, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9050498
CR  - HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
CR  - Hassan QK, 2009, REMOTE SENS-BASEL, V1, P393, DOI 10.3390/rs1030393
CR  - He KS, 2015, REMOTE SENS ECOL CON, V1, P4, DOI 10.1002/rse2.7
CR  - Hu JH, 2010, OECOLOGIA, V164, P555, DOI 10.1007/s00442-010-1732-z
CR  - Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619
CR  - Huang G., 2017, P 2017 IEEE C PATT R
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Kuffer M, 2016, IEEE J-STARS, V9, P1830, DOI 10.1109/JSTARS.2016.2538563
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lee S, 2014, INT J REMOTE SENS, V35, P3875, DOI 10.1080/01431161.2014.919680
CR  - Lee S, 2013, MAR POLLUT BULL, V67, P177, DOI 10.1016/j.marpolbul.2012.10.023
CR  - Liu N, 2018, IEEE ACCESS, V6, P11215, DOI 10.1109/ACCESS.2018.2798799
CR  - Maggiori E, 2017, IEEE T GEOSCI REMOTE, V55, P645, DOI 10.1109/TGRS.2016.2612821
CR  - Mboga N, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111106
CR  - Nielsen SE, 2005, ECOGRAPHY, V28, P197, DOI 10.1111/j.0906-7590.2005.04002.x
CR  - Parviainen M, 2013, BIODIVERS CONSERV, V22, P1731, DOI 10.1007/s10531-013-0509-1
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Salberg AB, 2015, INT GEOSCI REMOTE SE, P1893, DOI 10.1109/IGARSS.2015.7326163
CR  - Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10
CR  - Sharma A, 2017, NEURAL NETWORKS, V95, P19, DOI 10.1016/j.neunet.2017.07.017
CR  - Simonyan K., 2015, P INT C LEARN REPR I
CR  - SWETS JA, 1988, SCIENCE, V240, P1285, DOI 10.1126/science.3287615
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Takekawa John Y., 2009, Wildfowl, V59, P100
CR  - Wilson JW, 2013, BIOL CONSERV, V164, P170, DOI 10.1016/j.biocon.2013.04.021
CR  - Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
CR  - Zhang GG, 2014, FORKTAIL, P104
CR  - Zhang HK, 2017, REMOTE SENS LETT, V8, P438, DOI 10.1080/2150704X.2017.1280200
CR  - Zhang Yao-Nan, 2009, 2009 1st International Conference on Information Science and Engineering (ICISE 2009), P5104, DOI 10.1109/ICISE.2009.1065
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - MAY
PY  - 2018
VL  - 8
IS  - 5
DO  - 10.3390/ani8050066
AN  - WOS:000435189700007
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  42
ER  -

TY  - JOUR
AU  - Stancic, A
AU  - Vyroubal, V
AU  - Slijepcevic, V
TI  - Classification Efficiency of Pre-Trained Deep CNN Models on Camera Trap Images
T2  - JOURNAL OF IMAGING
LA  - English
KW  - classification
KW  - CNN
KW  - efficiency
KW  - pre-trained
KW  - camera trap
KW  - ARCHITECTURES
AB  - This paper presents the evaluation of 36 convolutional neural network (CNN) models, which were trained on the same dataset (ImageNet). The aim of this research was to evaluate the performance of pre-trained models on the binary classification of images in a "real-world" application. The classification of wildlife images was the use case, in particular, those of the Eurasian lynx (lat. "Lynx lynx"), which were collected by camera traps in various locations in Croatia. The collected images varied greatly in terms of image quality, while the dataset itself was highly imbalanced in terms of the percentage of images that depicted lynxes.
AD  - Karlovac Univ Appl Sci, Dept Engn, Ivana Mestrovica 10, Karlovac 47000, CroatiaAD  - Karlovac Univ Appl Sci, Dept Wildife Management & Nat Protect, Trg JJ Strossmayera 9, Karlovac 47000, CroatiaCR  - Aamir M., 2019, INT J IMAGE GRAPH SI, V10, P30, DOI [10.5815/ijigsp.2019.10.05, DOI 10.5815/IJIGSP.2019.10.05]
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Akosa Josephine, 2017, P SAS GLOB FOR, P2
CR  - Camus V, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P57, DOI 10.1109/AICAS.2019.8771610
CR  - Chen, 2017, ARXIV170404861, DOI DOI 10.1016/J.JAL.2014.11.010
CR  - Chicco D, 2020, BMC GENOMICS, V21, DOI 10.1186/s12864-019-6413-7
CR  - Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
CR  - Deep Residual Networks, RESNET 50 RESNET 101
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Grandini M., 2020, STATML200805756 ARXI
CR  - Gulli A., 2017, TENSORFLOW 1X DEEP L
CR  - He K., DEEP RESIDUAL LEARNI
CR  - He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
CR  - Hope T., 2017, LEARNING TENSORFLOW
CR  - Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
CR  - Huang G., DENSELY CONNECTED CO
CR  - ImageNet, LARGE SCALE VISUAL R
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Khan A, 2020, ARTIF INTELL REV, V53, P5455, DOI 10.1007/s10462-020-09825-6
CR  - Kinsley H., 2020, NEURAL NETWORKS SCRA
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Liu C, PROGR NEURAL ARCHITE
CR  - Powers D.M, EVALUATION PRECISION
CR  - Provost F, 1998, MACH LEARN, V30, P127, DOI 10.1023/A:1007442505281
CR  - Quinnell E, 2007, CONFERENCE RECORD OF THE FORTY-FIRST ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1-5, P331
CR  - Sandler M., INVERTED RESIDUALS L
CR  - Simonyan K., 2015, P INT C LEARN REPR I
CR  - Sokolova M, 2006, LECT NOTES COMPUT SC, V4304, P1015
CR  - Szegedy C., GOING DEEPER CONVOLU
CR  - Szegedy C, 2017, THIRTY-FIRST AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4278
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Tan M., MNASNET PLATFORM AWA
CR  - Tharwat A, 2021, APPL COMPUT INFORM, V17, P168, DOI [DOI 10.1016/J.ACI.2018.08.003, 10.1016/j.aci.2018.08.003]
CR  - Vasilev I., 2019, PYTHON DEEP LEARNING, P143
CR  - Yang T., NETADAPT PLATFORM AW
CR  - Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - FEB
PY  - 2022
VL  - 8
IS  - 2
DO  - 10.3390/jimaging8020020
AN  - WOS:000763451200001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Duggan, MT
AU  - Groleau, MF
AU  - Shealy, EP
AU  - Self, LS
AU  - Utter, TE
AU  - Waller, MM
AU  - Hall, BC
AU  - Stone, CG
AU  - Anderson, LL
AU  - Mousseau, TA
TI  - An approach to rapid processing of camera trap images with minimal human input
T2  - ECOLOGY AND EVOLUTION
LA  - English
KW  - camera trap
KW  - deep learning
KW  - neural network
KW  - transfer learning
KW  - wildlife ecology
KW  - NETWORKS
AB  - Camera traps have become an extensively utilized tool in ecological research, but the manual processing of images created by a network of camera traps rapidly becomes an overwhelming task, even for small camera trap studies. We used transfer learning to create convolutional neural network (CNN) models for identification and classification. By utilizing a small dataset with an average of 275 labeled images per species class, the model was able to distinguish between species and remove false triggers. We trained the model to detect 17 object classes with individual species identification, reaching an accuracy up to 92% and an average F1 score of 85%. Previous studies have suggested the need for thousands of images of each object class to reach results comparable to those achieved by human observers; however, we show that such accuracy can be achieved with fewer images. With transfer learning and an ongoing camera trap study, a deep learning model can be successfully created by a small camera trap study. A generalizable model produced from an unbalanced class set can be utilized to extract trap events that can later be confirmed by human processors.
AD  - Univ South Carolina UofSC, Dept Biol Sci, Columbia, SC 29208 USAAD  - South Carolina Army Natl Guard Environm Off, Eastover, SC USAFU  - Samuel Freeman Charitable Trust; University of South Carolina Honors College; University of South Carolina Office of Research
FX  - Samuel Freeman Charitable Trust; University of South Carolina Honors College; South Carolina Army National Guard; University of South Carolina Office of Research
CR  - Abadi M., 2015, TENSORFLOW LARGE SCA, V1
CR  - Alexander JS, 2016, BIOL CONSERV, V197, P27, DOI 10.1016/j.biocon.2016.02.023
CR  - Almond REA, 2018, LIVING PLANET REPORT
CR  - Chitwood MC, 2020, DIVERSITY-BASEL, V12, DOI 10.3390/d12090341
CR  - Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
CR  - Deepak S, 2019, COMPUT BIOL MED, V111, DOI 10.1016/j.compbiomed.2019.103345
CR  - Edwards S, 2016, J ARID ENVIRON, V124, P304, DOI 10.1016/j.jaridenv.2015.09.009
CR  - Ferreira-Rodriguez N, 2019, MAMMAL RES, V64, P155, DOI 10.1007/s13364-018-00414-1
CR  - Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
CR  - Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
CR  - Han DM, 2018, EXPERT SYST APPL, V95, P43, DOI 10.1016/j.eswa.2017.11.028
CR  - Jiang, 2019, PATTERN RECOGN, P394
CR  - Jiménez Carlos F., 2010, Rev. peru biol., V17, P191
CR  - KARANTH KU, 1995, BIOL CONSERV, V71, P333, DOI 10.1016/0006-3207(94)00057-W
CR  - Kolbert E, 2014, 6 EXTINCTION UNNATUR
CR  - Krasin I, 2017, DATASET
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Mccallum J, 2013, MAMMAL REV, V43, P196, DOI 10.1111/j.1365-2907.2012.00216.x
CR  - Newey S, 2015, AMBIO, V44, pS624, DOI 10.1007/s13280-015-0713-1
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Parsons AW, 2017, J MAMMAL, V98, P1547, DOI 10.1093/jmammal/gyx128
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Shao L, 2015, IEEE T NEUR NET LEAR, V26, P1019, DOI 10.1109/TNNLS.2014.2330900
CR  - Shi ZH, 2019, MULTIMED TOOLS APPL, V78, P1017, DOI 10.1007/s11042-018-6082-6
CR  - Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
CR  - Silveira L, 2003, BIOL CONSERV, V114, P351, DOI 10.1016/S0006-3207(03)00063-6
CR  - Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Swati ZNK, 2019, COMPUT MED IMAG GRAP, V75, P34, DOI 10.1016/j.compmedimag.2019.05.001
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tzutalin, 2015, LAB GIT COD
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Wolf C, 2006, INT J DOC ANAL RECOG, V8, P280, DOI 10.1007/s10032-006-0014-0
CR  - Xie M., 2015, ARXIV151000098
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - SEP
PY  - 2021
VL  - 11
IS  - 17
SP  - 12051
EP  - 12063
DO  - 10.1002/ece3.7970
AN  - WOS:000680141800001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Chen, RL
AU  - Little, R
AU  - Mihaylova, L
AU  - Delahay, R
AU  - Cox, R
TI  - Wildlife surveillance using deep learning methods
T2  - ECOLOGY AND EVOLUTION
LA  - English
KW  - automatic image recognition
KW  - bovine tuberculosis
KW  - convolutional neural networks
KW  - deep learning
KW  - wildlife monitoring
KW  - BADGERS MELES-MELES
KW  - BOVINE TUBERCULOSIS
KW  - MYCOBACTERIUM-BOVIS
KW  - TRANSMISSION
KW  - BEHAVIOR
KW  - BIOSECURITY
KW  - CATTLE
KW  - BAITS
AB  - Wildlife conservation and the management of human-wildlife conflicts require cost-effective methods of monitoring wild animal behavior. Still and video camera surveillance can generate enormous quantities of data, which is laborious and expensive to screen for the species of interest. In the present study, we describe a state-of-the-art, deep learning approach for automatically identifying and isolating species-specific activity from still images and video data. We used a dataset consisting of 8,368 images of wild and domestic animals in farm buildings, and we developed an approach firstly to distinguish badgers from other species (binary classification) and secondly to distinguish each of six animal species (multiclassification). We focused on binary classification of badgers first because such a tool would be relevant to efforts to manage Mycobacterium bovis (the cause of bovine tuberculosis) transmission between badgers and cattle. We used two deep learning frameworks for automatic image recognition. They achieved high accuracies, in the order of 98.05% for binary classification and 90.32% for multiclassification. Based on the deep learning framework, a detection process was also developed for identifying animals of interest in video footage, which to our knowledge is the first application for this purpose. The algorithms developed here have wide applications in wildlife monitoring where large quantities of visual data require screening for certain species.
AD  - Univ Sheffield, Dept Automat Control & Syst Engn, Sheffield, S Yorkshire, EnglandAD  - Univ Sheffield, Dept Geog, Sheffield, S Yorkshire, EnglandAD  - Anim & Plant Hlth Agcy, Natl Wildlife Management Ctr, Woodchester Pk, Glos, EnglandC3  - University of SheffieldC3  - University of SheffieldC3  - Animal & Plant Health Agency UKFU  - Department of Food and Rural Affairs [SE3295]; ESRC [ES/K009753/1] Funding Source: UKRI
FX  - Department of Food and Rural Affairs, Grant/Award Number: SE3295
CR  - Bjorklund BM, 2017, TROP MED INFECT DIS, V2, DOI 10.3390/tropicalmed2030040
CR  - Burghardt T, 2006, IEE P-VIS IMAGE SIGN, V153, P305, DOI 10.1049/ip-vis:20050052
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Chen RF, 2018, PROCEEDINGS OF 2018 IEEE 7TH DATA DRIVEN CONTROL AND LEARNING SYSTEMS CONFERENCE (DDCLS), P565, DOI 10.1109/DDCLS.2018.8515929
CR  - Dalal N., 2005, IEEE COMPUTER SOC C, P886, DOI 10.1109/CVPR.2005.177
CR  - Defra, 2014, STRAT ACH OFF BOV TU
CR  - Defra, 2015, GREEN BRIDG SAF TRAV
CR  - Defra, 2018, Q PUBL NAT STAT INC
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Donahue J, 2014, PR MACH LEARN RES, V32
CR  - Drewe JA, 2013, EPIDEMIOL INFECT, V141, P1467, DOI 10.1017/S0950268813000691
CR  - Enticott G, 2012, GEOGR J, V178, P327, DOI 10.1111/j.1475-4959.2012.00475.x
CR  - Garnett BT, 2002, P ROY SOC B-BIOL SCI, V269, P1487, DOI 10.1098/rspb.2002.2072
CR  - GODFRAY HCJ, 2013, P R SOC BIOL SCI, V280, P1
CR  - Gomez A., 2016, ARXIV160306169
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Gunn GJ, 2008, PREV VET MED, V84, P310, DOI 10.1016/j.prevetmed.2007.12.003
CR  - Hsing PY, 2018, REMOTE SENS ECOL CON, V4, P361, DOI 10.1002/rse2.84
CR  - Judge J, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0028941
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Little RA, 2019, FRONT VET SCI, V6, DOI 10.3389/fvets.2019.00081
CR  - Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
CR  - Martin C., 2013, P 40 ANN W PROT REL P 40 ANN W PROT REL, P1
CR  - MUIRHEAD RH, 1974, VET REC, V95, P552, DOI 10.1136/vr.95.24.552
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Nouvellet P, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0061588, 10.1371/currents.outbreaks.097a904d3f3619db2fe78d24bc776098]
CR  - Payne A, 2016, EUR J WILDLIFE RES, V62, P33, DOI 10.1007/s10344-015-0970-0
CR  - PTES, 2018, ROAD TUNN WILDL
CR  - Robertson A, 2017, VET REC, V180, DOI 10.1136/vr.103819
CR  - Robertson A, 2016, PREV VET MED, V135, P95, DOI 10.1016/j.prevetmed.2016.11.007
CR  - Robertson A, 2015, EUR J WILDLIFE RES, V61, P263, DOI 10.1007/s10344-014-0896-y
CR  - Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
CR  - Schneider S., 2018, ARXIV180310842
CR  - Spampinato C, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2015-1
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Tolhurst BA, 2009, APPL ANIM BEHAV SCI, V117, P103, DOI 10.1016/j.applanim.2008.10.009
CR  - Ward AI, 2006, ANIM SCI, V82, P767, DOI 10.1017/ASC2006102
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
CR  - Zhu H, 2017, IEEE T INTELL TRANSP, V18, P2584, DOI 10.1109/TITS.2017.2658662
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - SEP
PY  - 2019
VL  - 9
IS  - 17
SP  - 9453
EP  - 9466
DO  - 10.1002/ece3.5410
AN  - WOS:000481630400001
N1  - Times Cited in Web of Science Core Collection:  12
Total Times Cited:  12
Cited Reference Count:  42
ER  -

TY  - BOOK
AU  - Botella, C
AU  - Joly, A
AU  - Bonnet, P
AU  - Monestiez, P
AU  - Munoz, F
ED  - Joly, A
ED  - Vrochidis, S
ED  - Karatzas, K
ED  - Karppinen, A
ED  - Bonnet, P
TI  - A Deep Learning Approach to Species Distribution Modelling
T2  - MULTIMEDIA TOOLS AND APPLICATIONS FOR ENVIRONMENTAL & BIODIVERSITY INFORMATICS
LA  - English
AB  - Species distribution models (SDM) are widely used for ecological research and conservation purposes. Given a set of species occurrence, the aim is to infer its spatial distribution over a given territory. Because of the limited number of occurrences of specimens, this is usually achieved through environmental niche modeling approaches, i.e. by predicting the distribution in the geographic space on the basis of a mathematical representation of their known distribution in environmental space (= realized ecological niche). The environment is in most cases represented by climate data (such as temperature, and precipitation), but other variables such as soil type or land cover can also be used. In this paper, we propose a deep learning approach to the problem in order to improve the predictive effectiveness. Non-linear prediction models have been of interest for SDM for more than a decade but our study is the first one bringing empirical evidence that deep, convolutional and multilabel models might participate to resolve the limitations of SDM. Indeed, the main challenge is that the realized ecological niche is often very different from the theoretical fundamental niche, due to environment perturbation history, species propagation constraints and biotic interactions. Thus, the realized abundance in the environmental feature space can have a very irregular shape that can be difficult to capture with classical models. Deep neural networks on the other side, have been shown to be able to learn complex non-linear transformations in a wide variety of domains. Moreover, spatial patterns in environmental variables often contains useful information for species distribution but are usually not considered in classical models. Our study shows empirically how convolutional neural networks efficiently use this information and improve prediction performance.
AD  - INRIA Sophia Antipolis, ZENITH Team, LIRMM, UMR 5506, CC 477, Montpellier, FranceAD  - INRA, UMR AMAP, Montpellier, FranceAD  - Univ Montpellier, AMAP, CIRAD, CNRS,INRA,IRD, Montpellier, FranceAD  - INRA, BioSP, Site Agroparc, Avignon, FranceAD  - Inria ZENITH Team, Montpellier, FranceAD  - CIRAD, UMR AMAP, Montpellier, FranceAD  - Univ Grenoble Alpes, St Martin Dheres, FranceC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Universite Paul-ValeryC3  - Universite Perpignan Via DomitiaC3  - Universite de MontpellierC3  - CIRADC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Institut de Recherche pour le Developpement (IRD)C3  - Universite de MontpellierC3  - INRAEC3  - Centre National de la Recherche Scientifique (CNRS)C3  - CIRADC3  - INRAEC3  - Institut de Recherche pour le Developpement (IRD)C3  - Universite de MontpellierC3  - INRAEC3  - CIRADC3  - Centre National de la Recherche Scientifique (CNRS)C3  - Institut de Recherche pour le Developpement (IRD)C3  - Universite de MontpellierC3  - Communaute Universite Grenoble AlpesC3  - UDICE-French Research UniversitiesC3  - Universite Grenoble Alpes (UGA)CR  - BERMAN M, 1992, J R STAT SOC C-APPL, V41, P31
CR  - Chen Tianqi, 2015, ARXIV151201274
CR  - Dutreve B., 2016, INPN DONNEES FLORE C, DOI [10.15468/omae84accessedviaGBIF.orgon201708-30, DOI 10.15468/0MAE84]
CR  - Elith J, 2006, ECOGRAPHY, V29, P129, DOI 10.1111/j.2006.0906-7590.04596.x
CR  - Fithian W, 2013, ANN APPL STAT, V7, P1917, DOI 10.1214/13-AOAS667
CR  - FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Hastie T.J., 1990, GEN ADDITIVE MODELS, DOI DOI 10.1214/SS/1177013604
CR  - HUTCHINSON GE, 1957, COLD SPRING HARB SYM, V22, P415, DOI 10.1101/SQB.1957.022.01.039
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Karger D. N., 2016, CHELSEA CLIMATOLOGIE
CR  - Karger DN, 2016, ARXIV160700217
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Leathwick JR, 2006, ECOL MODEL, V199, P188, DOI 10.1016/j.ecolmodel.2006.05.022
CR  - LECUN Y, 1989, CONNECTIONISM IN PERSPECTIVE, P143
CR  - Lek S, 1996, ECOL MODEL, V90, P39, DOI 10.1016/0304-3800(95)00142-5
CR  - Nair V., 2010, P 27 INT C INT C MAC
CR  - Panagos P, 2006, GEO CONNEXION, V5, P32
CR  - Panagos P, 2012, LAND USE POLICY, V29, P329, DOI 10.1016/j.landusepol.2011.07.003
CR  - Phillips S., 2017, ECOGRAPHY
CR  - Phillips SJ, 2006, ECOL MODEL, V190, P231, DOI 10.1016/j.ecolmodel.2005.03.026
CR  - Phillips SJ, 2004, P 21 INT C MACHINE L, DOI [10.1016/j.newast.2003.12.006, DOI 10.1145/1015330.1015412]
CR  - Phillips SJ, 2008, ECOGRAPHY, V31, P161, DOI 10.1111/j.0906-7590.2008.5203.x
CR  - RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
CR  - Thuiller W, 2003, GLOBAL CHANGE BIOL, V9, P1353, DOI 10.1046/j.1365-2486.2003.00666.x
CR  - VanLiedekerke M, 2006, ESDBV2 RASTER LIBRAR
CR  - Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x
CR  - Zomer RJ, 2007, TREES WATER SMALLHOL
CR  - Zorner RJ, 2008, AGR ECOSYST ENVIRON, V126, P67, DOI 10.1016/j.agee.2008.01.014
PU  - SPRINGER
PI  - NEW YORK
PA  - 233 SPRING STREET, NEW YORK, NY 10013, UNITED STATES
PY  - 2018
SP  - 169
EP  - 199
DO  - 10.1007/978-3-319-76445-0_10
DO  - 10.1007/978-3-319-76445-0
AN  - WOS:000449980400011
N1  - Times Cited in Web of Science Core Collection:  13
Total Times Cited:  14
Cited Reference Count:  29
ER  -

TY  - JOUR
AU  - Delplanque, A
AU  - Foucher, S
AU  - Lejeune, P
AU  - Linchant, J
AU  - Theau, J
TI  - Multispecies detection and identification of African mammals in aerial imagery using convolutional neural networks
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - African mammals
KW  - CNNs
KW  - deep learning
KW  - multispecies
KW  - UAV
KW  - wildlife monitoring
AB  - Survey and monitoring of wildlife populations are among the key elements in nature conservation. The use of unmanned aerial vehicles and light aircrafts as aerial image acquisition systems is growing, as they are cheaper alternatives to traditional census methods. However, the manual localization and identification of species within imagery can be time-consuming and complex. Object detection algorithms, based on convolutional neural networks (CNNs), have shown a good capacity for animal detection. Nevertheless, most of the work has focused on binary detection cases (animal vs. background). The main objective of this study is to compare three recent detection algorithms to detect and identify African mammal species based on high-resolution aerial images. We evaluated the performance of three multi-class CNN algorithms: Faster-RCNN, Libra-RCNN and RetinaNet. Six species were targeted: topis (Damaliscus lunatus jimela), buffalos (Syncerus caffer), elephants (Loxodonta africana), kobs (Kobus kob), warthogs (Phacochoerus africanus) and waterbucks (Kobus ellipsiprymnus). The best model was then applied to a case study using an independent dataset. The best model was the Libra-RCNN, with the best mean average precision (0.80 +/- 0.02), the lowest degree of interspecies confusion (3.5 +/- 1.4%) and the lowest false positive per true positive ratio (1.7 +/- 0.2) on the test set. This model was able to detect and correctly identify 73% of all individuals (1115), find 43 individuals of species other than those targeted and detect 84 missed individuals on our independent UAV dataset, with an average processing speed of 12 s/image. This model showed better detection performance than previous studies dealing with similar habitats. It was able to differentiate six animal species in nadir aerial images. Although limitations were observed with warthog identification and individual detection in herds, this model can save time and can perform precise surveys in open savanna.
AD  - ULiege, Gembloux Agrobio Tech, TERRA Teaching & Res Ctr Forest Life, 2 Passage Deportes, B-5030 Gembloux, BelgiumAD  - Comp Res Inst Montreal, 405 Ogilvy Ave, Montreal, PQ H3N 1M3, CanadaAD  - Univ Sherbrooke, Dept Appl Geomat, 2500 Blvd Univ, Sherbrooke, PQ J1K 2R1, CanadaAD  - McGill Univ, Stewart Biol, Quebec Ctr Biodivers Sci QCBS, Montreal, PQ H3A 1B1, CanadaC3  - University of SherbrookeC3  - McGill UniversityFU  - Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.-FNRS); Ministere de l'Economie et de l'Innovation (MEI) of the province of Quebec; European Union (EU) [DCI-ENV/2012/309-143]; Center for International Forestry Research (CIFOR); R&D office, the ICCN (Institut Congolais pour la Conservation de la Nature)
FX  - The work of Alexandre DELPLANQUE was supported by the Fund for Research Training in Industry and Agriculture (FRIA, F.R.S.-FNRS). This project was financed in part by the Ministere de l'Economie et de l'Innovation (MEI) of the province of Quebec. We would like to thank the teams involved in the collection of the aerial images in the different areas and the observers who produced the ground truth associated with these images. Special thanks go to all those involved in the acquisition of data in the DRC under the Forest and Climate Change in Congo project (FCCC/2014-2016) funded by the European Union (EU, Grant Number DCI-ENV/2012/309-143) and granted by the Center for International Forestry Research (CIFOR). Data acquisition in the field was greatly facilitated by the support of R&D office, the ICCN (Institut Congolais pour la Conservation de la Nature), African Parks Network (Garamba NP) and the Virunga Foundation. We would also like to thank Simon LHOEST and Cedric VERMEULEN for reviewing the manuscript and bringing their expertise to the discussion of the results.
CR  - Barbedo JGA, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245436
CR  - Beery S, 2020, IEEE WINT CONF APPL, P852, DOI 10.1109/WACV45572.2020.9093570
CR  - Ceballos G, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1400253
CR  - Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
CR  - Chen K., 2019, ARXIV PREPRINT ARXIV
CR  - Craigie ID, 2010, BIOL CONSERV, V143, P2221, DOI 10.1016/j.biocon.2010.06.007
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Foucher S, 2021, GEOMATICS, V1, P34, DOI [10.3390/geomatics1010004, DOI 10.3390/GEOMATICS1010004]
CR  - Gaidet-Drapier N, 2006, BIODIVERS CONSERV, V15, P735, DOI 10.1007/s10531-004-1063-7
CR  - He K, 2016, ARXIV160305027, DOI DOI 10.1109/CVPR.2016.90
CR  - Hetem Robyn S, 2014, Temperature (Austin), V1, P115, DOI 10.4161/temp.29651
CR  - Isbell F, 2017, NATURE, V546, P65, DOI 10.1038/nature22899
CR  - JACHMANN H, 1991, AFR J ECOL, V29, P188, DOI 10.1111/j.1365-2028.1991.tb01001.x
CR  - Joubert, 2019, ZENODO, DOI [10.5281/zenodo.3234780, DOI 10.5281/ZENODO.3234780]
CR  - Joubert, 2019, P IEEE C COMP VIS PA, P48
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Lacher TE, 2019, J MAMMAL, V100, P942, DOI 10.1093/jmammal/gyy183
CR  - Lin T.-Y., 2017, ARXIV161203144, P2117, DOI DOI 10.1109/CVPR.2017.106
CR  - Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
CR  - Linchant J, 2015, INT ARCH PHOTOGRAMM, V40-3, P379, DOI 10.5194/isprsarchives-XL-3-W3-379-2015
CR  - Linchant J, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0206413
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Mayaux P, 2004, J BIOGEOGR, V31, P861, DOI 10.1111/j.1365-2699.2004.01073.x
CR  - Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
CR  - Norton-Griffiths M, 1978, COUNTING ANIMALS REV
CR  - Oksuz K, 2021, IEEE T PATTERN ANAL, V43, P3388, DOI 10.1109/TPAMI.2020.2981890
CR  - Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
CR  - Peng JB, 2020, ISPRS J PHOTOGRAMM, V169, P364, DOI 10.1016/j.isprsjprs.2020.08.026
CR  - Petersen T., 2020, LIVING PLANET REPORT
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
CR  - Ribera J, 2019, PROC CVPR IEEE, P6472, DOI 10.1109/CVPR.2019.00664
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Soviany P, 2019, INT SYMP SYMB NUMERI, P209, DOI 10.1109/SYNASC.2018.00041
CR  - Stocker T. F., 2013, SUMMARY POLICYMAKERS, DOI [DOI 10.1017/CBO9781107415324, 10.1017/CBO9781107415324.005, DOI 10.1017/CBO9781107415324.004]
CR  - Thuiller W, 2006, GLOBAL CHANGE BIOL, V12, P424, DOI 10.1111/j.1365-2486.2006.01115.x
CR  - Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
CR  - Watts AC, 2010, J WILDLIFE MANAGE, V74, P1614, DOI 10.2193/2009-425
CR  - Witmer GW, 2005, WILDLIFE RES, V32, P259, DOI 10.1071/WR04003
CR  - Xu BB, 2020, COMPUT ELECTRON AGR, V171, DOI 10.1016/j.compag.2020.105300
CR  - Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
CR  - Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - APR
PY  - 2022
VL  - 8
IS  - 2
SP  - 166
EP  - 179
DO  - 10.1002/rse2.234
AN  - WOS:000683285800001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  43
ER  -

TY  - JOUR
AU  - Nepovinnykh, E
AU  - Chelak, I
AU  - Lushpanov, A
AU  - Eerola, T
AU  - Kalviainen, H
AU  - Chirkova, O
TI  - Matching individual Ladoga ringed seals across short-term image sequences
T2  - MAMMALIAN BIOLOGY
LA  - English
KW  - Animal re-identification
KW  - Convolutional neural networks
KW  - Instance segmentation
KW  - Ladoga ringed seal
KW  - Photo-identification
KW  - PUSA-HISPIDA-LADOGENSIS
KW  - IDENTIFICATION
AB  - Automated wildlife reidentification has attracted increasing attention in recent years as it provides a non-invasive tool to identify and to track individual wild animals over time. In this paper, the first steps are taken towards the automatic photo-identification of the Ladoga ringed seals (Pusa hispida ladogensis). A method is proposed that takes a sequence of images, each containing multiple individuals as the input, and produces cropped images of seals grouped based on one certain individual per group. The method starts by detecting each seal from the images and proceeds to matching the individual seals between the images. It is shown that high grouping accuracy can be obtained with a general-purpose image retrieval method on an image sequence taken from the same location within a relatively short period of time. Each resulting group contains multiple images of one individual with slightly different variations, for example, in pose and illumination. Utilizing these images simultaneously provides more information for the individual re-identification compared to the traditional approach, i.e., which utilizes just one image at a time. It is further demonstrated that a convolutional neural network based method can be used to extract the unique pelage patterns of the seals despite the low contrast. Finally, a method is proposed and experiments with the novel Ladoga ringed seals data are carried out to provide a proof-of-concept for the individual re-identification.
AD  - Lappeenranta Lahti Univ Technol LUT, Comp Vis & Pattern Recognit Lab, Dept Computat Engn, Sch Engn Sci, POB 20, Lappeenranta 53851, FinlandAD  - Peter Great St Petersburg Polytech Univ, Dept Artificial Intelligence Inst Comp Sci & Tech, Polytech Skaya 29, St Petersburg 195251, RussiaAD  - Southern Fed Univ, Dept Comp Sci & Computat Expt, Rostov Na Donu 344006, RussiaAD  - Interreg Charitable Publ Org Biologists Nat Conse, 24 Line 3-7, St Petersburg 199106, RussiaC3  - Peter the Great St. Petersburg Polytechnic UniversityC3  - Southern Federal UniversityFU  - LUT University; European Union [KS1549]; Russian Federation [KS1549]; Republic of Finland via The South-East Finland-Russia CBC 2014-2020 Programme [KS1549]; Raija ja Ossi Tuuliaisen Saatio foundation
FX  - Open Access funding provided by LUT University (previously Lappeenranta University of Technology (LUT)). The research is a part of the CoExist project (Project ID: KS1549) funded by the European Union, the Russian Federation, and the Republic of Finland via The South-East Finland-Russia CBC 2014-2020 Programme. We would also like to thank the Raija ja Ossi Tuuliaisen Saatio foundation for additional financial support.
CR  - Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]
CR  - Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
CR  - Berger-Wolf T, 2015, BLOOMB DAT GOOD EXCH, V2
CR  - Berger-Wolf T.Y., 2017, ARXIV PREPRINT ARXIV
CR  - Bouma S, 2018, 2018 INT C IM VIS CO, P16, DOI [10.1109/IVCNZ.2018.8634778, DOI 10.1109/IVCNZ.2018.8634778]
CR  - Burghardt T, 2006, IEE P-VIS IMAGE SIGN, V153, P305, DOI 10.1049/ip-vis:20050052
CR  - Chehrsimin T, 2018, IET COMPUT VIS, V12, P146, DOI 10.1049/iet-cvi.2017.0082
CR  - Chelak I, 2021, ARXIV210513979
CR  - Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511
CR  - Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
CR  - Cheng XH, 2020, IEEE IMAGE PROC, P2231, DOI 10.1109/ICIP40778.2020.9190667
CR  - Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
CR  - Deb D, 2018, INT CONF BIOMETR THE
CR  - Dunbar SG, 2021, J EXP MAR BIOL ECOL, V535, DOI 10.1016/j.jembe.2020.151490
CR  - Girshick R., 2014, P IEEE C COMP VIS PA, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Gromov VV, 2021, P C MAR MAMM HOL
CR  - He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
CR  - Holmberg Jason, 2009, Endangered Species Research, V7, P39, DOI 10.3354/esr00186
CR  - Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
CR  - Kellenberger B, 2019, IEEE T GEOSCI REMOTE, V57, P9524, DOI 10.1109/TGRS.2019.2927393
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Kumar, 2019, CONVERGANCE ADAM
CR  - Kunnasranta M, 2021, BIOL CONSERV, V253, DOI 10.1016/j.biocon.2020.108908
CR  - Law H, 2020, INT J COMPUT VISION, V128, P642, DOI 10.1007/s11263-019-01204-1
CR  - Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
CR  - Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
CR  - Liu C, 2019, IEEE INT CONF COMP V, P315, DOI 10.1109/ICCVW.2019.00042
CR  - Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
CR  - Liu N, 2019, IEEE INT CONF COMP V, P286, DOI 10.1109/ICCVW.2019.00038
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
CR  - Loshchilov I., 2019, P ICLR
CR  - Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
CR  - Lushpanov A, 2020, THESIS LAPPEENRANTA
CR  - Mikolajczyk K, 2007, IEEE I CONF COMP VIS, P337
CR  - Mishchuk A, 2018, ARXIV170510872
CR  - Moskvyak O, 2020, IEEE WINT CONF APPL, P12, DOI 10.1109/WACVW50321.2020.9096932
CR  - Nepovinnykh E, 2020, IEEE WINT CONF APPL, P25, DOI 10.1109/WACVW50321.2020.9096935
CR  - Nepovinnykh E, 2018, LECT NOTES COMPUT SC, V11182, P211, DOI 10.1007/978-3-030-01449-0_18
CR  - Ng Tony, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P253, DOI 10.1007/978-3-030-58595-2_16
CR  - Parham J, 2017, AAAI SPRING S SERIES
CR  - Parham J, 2018, IEEE WINT CONF APPL, P1075, DOI 10.1109/WACV.2018.00123
CR  - Park Heebok, 2019, Journal of Ecology and Environment, V43, P39, DOI 10.1186/s41610-019-0138-z
CR  - Perronnin F., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383266
CR  - Perronnin F, 2010, PROC CVPR IEEE, P3384, DOI 10.1109/CVPR.2010.5540009
CR  - Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
CR  - Quinby BM, 2021, ENVIRON ENTOMOL, V50, P238, DOI 10.1093/ee/nvaa139
CR  - Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
CR  - Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
CR  - RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
CR  - Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
CR  - Shuyuan Li, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P2590, DOI 10.1145/3394171.3413569
CR  - Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
CR  - Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
CR  - Stergiou A., 2021, P IEEE CVF INT C COM, P10357
CR  - Tolias G., 2016, ARXIV151105879
CR  - Trukhanova IS, 2013, ARCTIC, V66, P417
CR  - Trukhanova Irina S., 2013, Russian Journal of Theriology, V12, P41
CR  - Ulyanov D, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P1250
CR  - Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
CR  - Zavialkin D, 2020, THESIS LAPPEENRANTA
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
CR  - Zhelezniakov A, 2015, LECT NOTES COMPUT SC, V9475, P227, DOI 10.1007/978-3-319-27863-6_21
PU  - SPRINGER HEIDELBERG
PI  - HEIDELBERG
PA  - TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
DO  - 10.1007/s42991-022-00229-3
AN  - WOS:000782164900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  68
ER  -

TY  - JOUR
AU  - Chalmers, C
AU  - Fergus, P
AU  - Montanez, CAC
AU  - Longmore, SN
AU  - Wich, SA
TI  - Video analysis for the detection of animals using convolutional neural networks and consumer-grade drones
T2  - JOURNAL OF UNMANNED VEHICLE SYSTEMS
LA  - English
KW  - conservation
KW  - deep learning
KW  - convolutional neural networks
KW  - inferencing
KW  - drone technology
KW  - WILDLIFE RESEARCH
KW  - DEEP
KW  - CNN
AB  - Determining animal distribution and density is important in conservation. The process is both time-consuming and labour-intensive. Drones have been used to help mitigate human-intensive tasks by covering large geographical areas over a much shorter timescale. In this paper we investigate this idea further using a proof of concept to detect rhinos and cars from drone footage. The proof of concept utilises off-the-shelf technology and consumer-grade drone hardware. The study demonstrates the feasibility of using machine learning (ML) to automate routine conservation tasks, such as animal detection and tracking. The prototype has been developed using a DJI Mavic Pro 2 and tested over a global system for mobile communications (GSM) network. The Faster-RCNN Resnet 101 architecture is used for transfer learning. Inference is performed with a frame sampling technique to address the required trade-off between precision, processing speed, and live video feed synchronisation. Inference models are hosted on a web platform and video streams from the drone (using OcuSync) are transmitted to a real-time messaging protocol (RTMP) server for subsequent classification. During training, the best model achieves a mean average precision (mAP) of 0.83 intersection over union (@IOU) 0.50 and 0.69 @IOU 0.75, respectively. On testing the system in Knowsley Safari our prototype was able to achieve the following: sensitivity (Sen), 0.91 (0.869, 0.94); specificity (Spec), 0.78 (0.74, 0.82); and an accuracy (ACC), 0.84 (0.81, 0.87) when detecting rhinos, and Sen, 1.00 (1.00, 1.00); Spec, 1.00 (1.00, 1.00); and an ACC, 1.00 (1.00, 1.00) when detecting cars.
AD  - Liverpool John Moores Univ, Sch Comp Sci, Liverpool L2 2QP, Merseyside, EnglandAD  - Liverpool John Moores Univ, Astrophys Res Inst, Liverpool L3 5RF, Merseyside, EnglandAD  - Liverpool John Moores Univ, Sch Biol & Environm Sci, Liverpool L2 2QP, Merseyside, EnglandC3  - Liverpool John Moores UniversityC3  - Liverpool John Moores UniversityC3  - Liverpool John Moores UniversityFU  - Research Council UK (RCUK) Science and Technology Facilities Council (STFC) [ST/R002673/1]
FX  - The authors would like to thank Naomi Davies at Knowsley Safari for co-ordinating the field trial and providing access to the rhino enclosure. This research was supported by the Research Council UK (RCUK) Science and Technology Facilities Council (STFC) through grant ST/R002673/1.
CR  - Agapito L., LECT NOTES COMPUTER, V8925
CR  - Ba L.J., 2013, P 26 INT C NEUR INF, P3084
CR  - Banerjee DS, 2016, INT CONF CLOUD COMP, P144, DOI [10.1109/CloudCom.2016.0036, 10.1109/CloudCom.2016.33]
CR  - Bondi E, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P7741
CR  - Bondi Elizabeth, 2019, P77
CR  - Buckland S.T., 2001, pi
CR  - Buckland S.T., 2004, ADV DISTANCE SAMPLIN
CR  - Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
CR  - Christie KS, 2016, FRONT ECOL ENVIRON, V14, P242, DOI 10.1002/fee.1281
CR  - Commercial Software Engineering (CSE) group at Microsoft, 2020, VOTT VIS OBJ TAGG TO
CR  - Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
CR  - Fang YF, 2016, PROCEDIA COMPUT SCI, V92, P13, DOI 10.1016/j.procs.2016.07.316
CR  - Fergus P., CONSERVATION
CR  - Hazelwood K, 2018, INT S HIGH PERF COMP, P620, DOI 10.1109/HPCA.2018.00059
CR  - Hensel M, 2017, ADV NEUR IN, V30
CR  - Iwamura S., 2017, ADV NEURAL INFORM PR, P435
CR  - Jakobs S., 2019, ATZHEAVY DUTY WORLDW, V12, P44, DOI [10.1007/s41321-019-0024-8, DOI 10.1007/S41321-019-0024-8]
CR  - King DB, 2015, ACS SYM SER, V1214, P1
CR  - Lamba A, 2019, CURR BIOL, V29, pR977, DOI 10.1016/j.cub.2019.08.016
CR  - LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
CR  - Lee J, 2017, 2017 FIRST IEEE INTERNATIONAL CONFERENCE ON ROBOTIC COMPUTING (IRC), P36, DOI 10.1109/IRC.2017.77
CR  - Lim K, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173317
CR  - Lin T.-Y., ND COCO COMMON OBJEC
CR  - Longmore SN, 2017, INT J REMOTE SENS, V38, P2623, DOI 10.1080/01431161.2017.1280639
CR  - Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
CR  - Martinez P, 2019, AUTOMAT CONSTR, V97, P151, DOI 10.1016/j.autcon.2018.10.021
CR  - Maxwell S, 2016, NATURE, V536, P143, DOI 10.1038/536143a
CR  - Nichols JD, 2006, TRENDS ECOL EVOL, V21, P668, DOI 10.1016/j.tree.2006.08.007
CR  - Peng JB, 2020, ISPRS J PHOTOGRAMM, V169, P364, DOI 10.1016/j.isprsjprs.2020.08.026
CR  - Rampasek L, 2016, CELL SYST, V2, P12, DOI 10.1016/j.cels.2016.01.009
CR  - Saria S, 2018, PLOS MED, V15, DOI 10.1371/journal.pmed.1002721
CR  - Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
CR  - Talukdar J, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND INTEGRATED NETWORKS (SPIN), P78, DOI 10.1109/SPIN.2018.8474198
CR  - van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
CR  - Wich SA, 2018, CONSERVATION DRONES: MAPPING AND MONITORING BIODIVERSITY, P1, DOI 10.1093/oso/9780198787617.001.0001
PU  - CANADIAN SCIENCE PUBLISHING
PI  - OTTAWA
PA  - 65 AURIGA DR, SUITE 203, OTTAWA, ON K2E 7W6, CANADA
DA  - JUN
PY  - 2021
VL  - 9
IS  - 2
SP  - 112
EP  - 127
DO  - 10.1139/juvs-2020-0018
AN  - WOS:000695453500003
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  35
ER  -

TY  - JOUR
AU  - Florath, J
AU  - Keller, S
TI  - Supervised Machine Learning Approaches on Multispectral Remote Sensing Data for a Combined Detection of Fire and Burned Area
T2  - REMOTE SENSING
LA  - English
KW  - remote sensing
KW  - classification
KW  - burned area mapping
KW  - fire detection
KW  - deep learning
KW  - Sentinel-2 images
KW  - self-organizing maps
KW  - undersampling
KW  - imbalanced dataset
KW  - convolutional neural network
KW  - DETECTION ALGORITHM
KW  - CLASSIFICATION
KW  - IMAGES
KW  - RESOLUTION
KW  - MULTISENSOR
KW  - VALIDATION
KW  - COLLECTION
KW  - PRODUCT
KW  - SCARS
KW  - ASTER
AB  - Bushfires pose a severe risk, among others, to humans, wildlife, and infrastructures. Rapid detection of fires is crucial for fire-extinguishing activities and rescue missions. Besides, mapping burned areas also supports evacuation and accessibility to emergency facilities. In this study, we propose a generic approach for detecting fires and burned areas based on machine learning (ML) approaches and remote sensing data. While most studies investigated either the detection of fires or mapping burned areas, we addressed and evaluated, in particular, the combined detection on three selected case study regions. Multispectral Sentinel-2 images represent the input data for the supervised ML models. First, we generated the reference data for the three target classes, burned, unburned, and fire, since no reference data were available. Second, the three regional fire datasets were preprocessed and divided into training, validation, and test subsets according to a defined schema. Furthermore, an undersampling approach ensured the balancing of the datasets. Third, seven selected supervised classification approaches were used and evaluated, including tree-based models, a self-organizing map, an artificial neural network, and a one-dimensional convolutional neural network (1D-CNN). All selected ML approaches achieved satisfying classification results. Moreover, they performed a highly accurate fire detection, while separating burned and unburned areas was slightly more challenging. The 1D-CNN and extremely randomized tree were the best-performing models with an overall accuracy score of 98% on the test subsets. Even on an unknown test dataset, the 1D-CNN achieved high classification accuracies. This generalization is even more valuable for any use-case scenario, including the organization of fire-fighting activities or civil protection. The proposed combined detection could be extended and enhanced with crowdsourced data in further studies.
AD  - Karlsruhe Inst Technol, Inst Photogrammetry & Remote Sensing, D-76131 Karlsruhe, GermanyC3  - Helmholtz AssociationC3  - Karlsruhe Institute of TechnologyFU  - KIT-Publication; Karlsruhe Institute of Technology; head of the Institute of Photogrammetry and Remote Sensing at the Karlsruhe Institute of Technology
FX  - We thank Stefan Hinz, head of the Institute of Photogrammetry and Remote Sensing at the Karlsruhe Institute of Technology, for the funding of this work. We acknowledge support by the KIT-Publication Fund of the Karlsruhe Institute of Technology
CR  - Abadi M., 2016, 12 USENIX S OPERATIN, P265
CR  - Axel AC, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10030371
CR  - Ban YF, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-019-56967-x
CR  - Barducci A, 2002, INFRARED PHYS TECHN, V43, P119, DOI 10.1016/S1350-4495(02)00129-9
CR  - Bastarrika A., 2018, REMOTE SENS-BASEL, DOI [10.20944/preprints201805.0480.v1, DOI 10.20944/PREPRINTS201805.0480.V1]
CR  - Bastarrika A, 2014, REMOTE SENS-BASEL, V6, P12360, DOI 10.3390/rs61212360
CR  - Bastarrika A, 2011, REMOTE SENS ENVIRON, V115, P1003, DOI 10.1016/j.rse.2010.12.005
CR  - Boer MM, 2020, NAT CLIM CHANGE, V10, P171, DOI 10.1038/s41558-020-0716-1
CR  - Bowman DMJS, 2009, SCIENCE, V324, P481, DOI 10.1126/science.1163886
CR  - Brand A., 2021, ISPRS INT ARCH PHOTO, V43, P47, DOI [10.5194/isprs-archives-XLIII-B3-2021-47-2021, DOI 10.5194/ISPRS-ARCHIVES-XLIII-B3-2021-47-2021]
CR  - Bruneau P, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13061153
CR  - Buchhorn M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12061044
CR  - Calle A, 2006, J GEOPHYS RES-BIOGEO, V111, DOI 10.1029/2005JG000116
CR  - Cervone G, 2016, INT J REMOTE SENS, V37, P100, DOI 10.1080/01431161.2015.1117684
CR  - Chanussot, 2020, HYPERSPECTRAL IMAGE, P187, DOI [10.1007/978-3-030-38617-7_7, DOI 10.1007/978-3-030-38617-7_7]
CR  - Chawla NV., 2004, ACM SIGKDD EXPLOR NE, V6, P1, DOI [10.1145/1007730.1007733, DOI 10.1145/1007730.1007733]
CR  - Chiaraviglio N, 2016, P IEEE INT C E-SCI, P414, DOI 10.1109/eScience.2016.7870928
CR  - Cicala L, 2018, IEEE INT C ENV ENG I, P1, DOI [10.1109/EE1.2018.8385269, DOI 10.1109/EE1.2018.8385269]
CR  - Comert R, 2019, INT J ENG GEOSCI, V4, P78, DOI 10.26833/ijeg.455595
CR  - Crowley MA, 2019, REMOTE SENS LETT, V10, P302, DOI 10.1080/2150704X.2018.1536300
CR  - Di Biase V, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050741
CR  - Engelbrecht J, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080764
CR  - Filipponi F, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11060622
CR  - Flasse SP, 1996, INT J REMOTE SENS, V17, P419, DOI 10.1080/01431169608949018
CR  - Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
CR  - Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451
CR  - Gargiulo M, 2019, PR ELECTROMAGN RES S, P418, DOI 10.1109/PIERS-Spring46901.2019.9017857
CR  - Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1
CR  - Ghaffarian S, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11202427
CR  - Giglio L, 2003, REMOTE SENS ENVIRON, V87, P273, DOI 10.1016/S0034-4257(03)00184-6
CR  - Giglio L, 2008, REMOTE SENS ENVIRON, V112, P3055, DOI 10.1016/j.rse.2008.03.003
CR  - Giglio L, 2016, REMOTE SENS ENVIRON, V178, P31, DOI 10.1016/j.rse.2016.02.054
CR  - Gitas IZ, 2004, REMOTE SENS ENVIRON, V92, P409, DOI 10.1016/j.rse.2004.06.006
CR  - Goodenough DG, 2011, CAN J REMOTE SENS, V37, P500
CR  - Grivei AC, 2020, 2020 13TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS (COMM), P189, DOI 10.1109/COMM48946.2020.9141999
CR  - Guindos-Rojas F, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050789
CR  - Guth J, 2019, NAT HAZARDS, V97, P979, DOI 10.1007/s11069-019-03672-7
CR  - Hansen MC, 2000, INT J REMOTE SENS, V21, P1331, DOI 10.1080/014311600210209
CR  - Henry MC, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11020104
CR  - HINTON GE, 1989, ARTIF INTELL, V40, P185, DOI 10.1016/0004-3702(89)90049-0
CR  - Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619
CR  - Kattenborn T, 2021, ISPRS J PHOTOGRAMM, V173, P24, DOI 10.1016/j.isprsjprs.2020.12.010
CR  - Keller S., 2019, ISPRS ANN PHOTOGRAMM, V4, P615, DOI [10.5194/isprs-annals-IV-2-W5-615-2019, DOI 10.5194/ISPRS-ANNALS-IV-2-W5-615-2019]
CR  - Kim HC, 2002, LECT NOTES COMPUT SC, V2388, P397
CR  - Knopp L, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152422
CR  - KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
CR  - Koltunov A, 2016, REMOTE SENS ENVIRON, V184, P436, DOI 10.1016/j.rse.2016.07.021
CR  - Kruspe A., 2021, 210812251 ARXIV
CR  - Kumar SS, 2018, INT J DIGIT EARTH, V11, P154, DOI 10.1080/17538947.2017.1391341
CR  - Kurnaz B., 2020, FOREST FIRE AREA DET
CR  - Kurum M, 2015, IEEE GEOSCI REMOTE S, V12, P1091, DOI 10.1109/LGRS.2014.2382716
CR  - Lasaponara R, 2020, IEEE GEOSCI REMOTE S, V17, P854, DOI 10.1109/LGRS.2019.2934503
CR  - Lentile LB, 2006, INT J WILDLAND FIRE, V15, P319, DOI 10.1071/WF05097
CR  - Li LS, 2018, J MACH LEARN RES, V18
CR  - Libonati R, 2015, REMOTE SENS-BASEL, V7, P15782, DOI 10.3390/rs71115782
CR  - Liu LF, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18093169
CR  - Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
CR  - Mallinis G, 2012, INT J REMOTE SENS, V33, P4408, DOI 10.1080/01431161.2011.648284
CR  - MATSON M, 1987, INT J REMOTE SENS, V8, P961, DOI 10.1080/01431168708954740
CR  - Michael Y, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10091479
CR  - Mithal V, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10010069
CR  - Mitrakis NE, 2012, INT J IMAGE DATA FUS, V3, P299, DOI 10.1080/19479832.2011.635604
CR  - Mitri G.H., 2002, FOREST FIRE RES WILD, P1
CR  - Mosley, 2013, BALANCED APPROACH MU
CR  - NASA, 2021, MODIS SPEC
CR  - Oliva P, 2015, REMOTE SENS ENVIRON, V160, P144, DOI 10.1016/j.rse.2015.01.010
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Pereira AA, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111161
CR  - PEREIRA MC, 1993, INT J REMOTE SENS, V14, P2061, DOI 10.1080/01431169308954022
CR  - Polychronaki A, 2010, INT J REMOTE SENS, V31, P1113, DOI 10.1080/01431160903334497
CR  - Ramo R, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111193
CR  - Rashkovetsky D, 2021, IEEE J-STARS, V14, P7001, DOI 10.1109/JSTARS.2021.3093625
CR  - Rauste Y, 1997, INT J REMOTE SENS, V18, P2641, DOI 10.1080/014311697217512
CR  - Riese F.M., 2019, DOI 10.5281/zenodo.2609130
CR  - Riese FM, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010007
CR  - Riese FM, 2018, INT GEOSCI REMOTE SE, P6151, DOI 10.1109/IGARSS.2018.8517812
CR  - Robert, 2003, P WORKSH LEARN IMB D, P11
CR  - ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
CR  - Ruecker G., 2000, INT ARCH PHOTOGRAMM, V33, P1286
CR  - San-Miguel-Ayanz J, 2005, NAT HAZARDS, V35, P361, DOI 10.1007/s11069-004-1797-2
CR  - Schroeder W, 2008, REMOTE SENS ENVIRON, V112, P2711, DOI 10.1016/j.rse.2008.01.005
CR  - Schroeder W, 2016, REMOTE SENS ENVIRON, V185, P210, DOI 10.1016/j.rse.2015.08.032
CR  - Schroeder W, 2014, REMOTE SENS ENVIRON, V143, P85, DOI 10.1016/j.rse.2013.12.008
CR  - Sharples JJ, 2016, CLIMATIC CHANGE, V139, P85, DOI 10.1007/s10584-016-1811-1
CR  - Siegert F, 2000, REMOTE SENS ENVIRON, V72, P64, DOI 10.1016/S0034-4257(99)00092-9
CR  - Stroppiana D, 2012, ISPRS J PHOTOGRAMM, V69, P88, DOI 10.1016/j.isprsjprs.2012.03.001
CR  - Stroppiana D, 2002, REMOTE SENS ENVIRON, V82, P21, DOI 10.1016/S0034-4257(02)00021-4
CR  - THOMAS PJ, 1995, P SOC PHOTO-OPT INS, V2553, P104, DOI 10.1117/12.221349
CR  - Wei JJ, 2018, CAN J REMOTE SENS, V44, P447, DOI 10.1080/07038992.2018.1543022
CR  - Weiss GM, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P13
CR  - Wu Q., 2020, J OPEN SOURCE SOFTW, V5, P2305, DOI [10.21105/joss.02305, DOI 10.21105/JOSS.02305]
CR  - Xie ZX, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10121992
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - FEB
PY  - 2022
VL  - 14
IS  - 3
DO  - 10.3390/rs14030657
AN  - WOS:000756554800001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  92
ER  -

TY  - JOUR
AU  - Khalighifar, A
AU  - Brown, RM
AU  - Vallejos, JG
AU  - Peterson, AT
TI  - Deep learning improves acoustic biodiversity monitoring and new candidate forest frog species identification (genus Platymantis) in the Philippines
T2  - BIODIVERSITY AND CONSERVATION
LA  - English
KW  - Bioacoustics
KW  - Biodiversity inventory
KW  - Ceratobatrachidae
KW  - Convolutional neural networks
KW  - Inception v3
KW  - TensorFlow
KW  - Transfer learning
KW  - LUZON ISLAND
KW  - AMPHIBIA
KW  - RANIDAE
KW  - RECOGNITION
KW  - IMAGES
KW  - ANURA
KW  - TIME
AB  - One significant challenge to biodiversity assessment and conservation is persistent gaps in species diversity knowledge in Earth's most biodiverse areas. Monitoring devices that utilize species-specific advertisement calls show promise in overcoming challenges associated with lagging frog species discovery rates. However, these devices generate data at paces faster than it can be analyzed. As such, automated platforms capable of efficient data processing and accurate species-level identification are at a premium. In addressing this gap, we used TensorFlow Inception v3 to design a robust, automated species identification system for 41 Philippine frog species (genus Platymantis), utilizing single-note audio spectrograms. With this model, we explored two concepts: (1) performance of our deep-learning model in discriminating closely-related frog species based on images representing advertisement call notes, and (2) the potential of this platform to accelerate new species discovery. TensorFlow identified species with a similar to 94% overall correct identification rate. Incorporating distributional data increased the overall identification rate to similar to 99%. In applying TensorFlow to a dataset that included undescribed species in addition to known species, our model was able to differentiate undescribed species through variation in "certainty" rate; the overall certainty rate for undescribed species was 65.5% versus 83.6% for described species. This indicates that, in addition to discriminating recognized frog species, our model has the potential to flag possible new species. As such, this work represents a proof-of-concept for automated, accelerated detection of novel species using acoustic mate-recognition signals, that can be applied to other groups characterized by vibrational cues, seismic signals, and vibrational mate-recognition.
AD  - Univ Kansas, Biodivers Inst, Lawrence, KS 66045 USAAD  - Univ Kansas, Dept Ecol & Evolutionary Biol, Lawrence, KS 66045 USAAD  - Univ Missouri, Div Biol Sci, Columbia, MO 65211 USAAD  - Colorado State Univ, Warner Coll Nat Resources, Ft Collins, CO 80523 USAC3  - University of KansasC3  - University of KansasC3  - University of Missouri SystemC3  - University of Missouri ColumbiaC3  - Colorado State UniversityFU  - U. S. National Science Foundation [DEB 0073199, DEB 0743491]; NSF Thematic Collections Network (TCN) program [DEB 1304585]; NSF [DEB 1654388, 1557053]; KU Biodiversity Institute's Rudkin Research Exploration (REX) Fund; KU College of Liberal Arts and Sciences Docking Scholar Fund
FX  - Philippine Platymantis frog calls were collected with support from the U. S. National Science Foundation's former Doctoral Dissertation Improvement Grant (DEB 0073199; 2001-2003) and a Biotic Surveys and Inventories Grant (DEB 0743491; 2008-2012). Collection and co-curated voucher specimens and their associated digital media specimens, archival digitization, data verification, and online serving of digital media specimens was made possible by a NSF Thematic Collections Network (TCN) program Grant (DEB 1304585; 2013-2018). Recent extended specimen collection and curation has been supported by NSF DEB 1654388 and 1557053, with further support from the KU Biodiversity Institute's Rudkin Research Exploration (REX) Fund and KU College of Liberal Arts and Sciences Docking Scholar Fund. We thank A. Diesmos, J. Fernandez, C. Siler, and C. Meneses for help recording frogs.
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
CR  - Araya-Salas M, 2017, METHODS ECOL EVOL, V8, P184, DOI 10.1111/2041-210X.12624
CR  - Brabant R, 2018, BELG J ZOOL, V148, P119, DOI 10.26496/bjz.2018.21
CR  - Brown RM, 2007, COPEIA, P251
CR  - Brown RM, 2015, ZOOTAXA, V4048, P191, DOI 10.11646/zootaxa.4048.2.3
CR  - Brown RM, 2015, ZOOL J LINN SOC-LOND, V174, P130, DOI 10.1111/zoj.12232
CR  - Brown RM, 2013, ANNU REV ECOL EVOL S, V44, P411, DOI 10.1146/annurev-ecolsys-110411-160323
CR  - Brown Walter C., 1999, Proceedings of the California Academy of Sciences, V51, P449
CR  - Brown Walter C., 1997, Proceedings of the California Academy of Sciences, V50, P1
CR  - Brown WC, 1997, P BIOL SOC WASH, V110, P18
CR  - Brown WC, 1999, P BIOL SOC WASH, V112, P510
CR  - Bryan JE, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0069679
CR  - Chan KO, 2017, BIOL LETTERS, V13, DOI 10.1098/rsbl.2017.0299
CR  - Chen Z, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-14356-3
CR  - Diesmos A, 2015, P CALIFORNIA ACAD SC, V20, P457, DOI DOI 10.4038/JNSFSR.V34I2.2084
CR  - Feinberg JA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108213
CR  - Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
CR  - GERHARDT HC, 1994, ANNU REV ECOL SYST, V25, P293, DOI 10.1146/annurev.es.25.110194.001453
CR  - GERHARDT HC, 1978, J EXP BIOL, V74, P59
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Gonzalez-del-Pliego P, 2019, CURR BIOL, V29, P1557, DOI 10.1016/j.cub.2019.04.005
CR  - Gower D., 2012, BIOTIC EVOLUTION ENV, P348, DOI 10.1017/CBO9780511735882.016
CR  - Guirado E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-50795-9
CR  - Gurgel-Goncalves R, 2017, PEERJ, V5, DOI 10.7717/peerj.3040
CR  - Holmgren J, 2008, INT J REMOTE SENS, V29, P1537, DOI 10.1080/01431160701736471
CR  - Inger R. F., 1954, Fieldiana Zoology, V33, P181
CR  - Khalighifar A, 2019, J MED ENTOMOL, V56, P1404, DOI 10.1093/jme/tjz065
CR  - Klein D.J., 2015, BLOOMB DAT GOOD EXCH
CR  - Kohler J, 2017, ZOOTAXA, V4251, P1, DOI 10.11646/zootaxa.4251.1.1
CR  - Kohavi, 1995, INT JOINT C ART INT, V14, P1137, DOI DOI 10.1067/MOD.2000.109031
CR  - Kumar N, 2012, LECT NOTES COMPUT SC, V7573, P502, DOI 10.1007/978-3-642-33709-3_36
CR  - MacLeod N, 2010, NATURE, V467, P154, DOI 10.1038/467154a
CR  - Molinaro AM, 2005, BIOINFORMATICS, V21, P3301, DOI 10.1093/bioinformatics/bti499
CR  - Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
CR  - NARINS PM, 1977, ANIM BEHAV, V25, P615, DOI 10.1016/0003-3472(77)90112-9
CR  - Ramcharan A, 2017, FRONT PLANT SCI, V8, DOI 10.3389/fpls.2017.01852
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Rzanny M, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0245-8
CR  - Scheffers BR, 2013, BIOTROPICA, V45, P628, DOI 10.1111/btp.12042
CR  - Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
CR  - Siler CD, 2007, HERPETOLOGICA, V63, P351, DOI 10.1655/0018-0831(2007)63[351:ANSOPA]2.0.CO;2
CR  - Siler CD, 2010, ZOOTAXA, P49
CR  - Smith SW, 2019, J ELECTROCARDIOL, V52, P88, DOI 10.1016/j.jelectrocard.2018.11.013
CR  - Sueur J, 2008, BIOACOUSTICS, V18, P213, DOI 10.1080/09524622.2008.9753600
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Tapley B, 2018, BIOL CONSERV, V220, P209, DOI 10.1016/j.biocon.2018.01.022
CR  - Taylor E. H., 1923, Philippine Journal of Science, V22, P515
CR  - Tonini JFR, 2020, ECOL EVOL, V10, P3686, DOI 10.1002/ece3.6155
CR  - Vieites DR, 2009, P NATL ACAD SCI USA, V106, P8267, DOI 10.1073/pnas.0810821106
CR  - Vignal C, 2007, P ROY SOC B-BIOL SCI, V274, P479, DOI 10.1098/rspb.2006.3744
CR  - Wells K.D., 2010, ECOLOGY BEHAV AMPHIB
CR  - Wells KD, 2007, SPR HDB AUD, V28, P44
CR  - Wimmer J, 2013, ECOL APPL, V23, P1419, DOI 10.1890/12-2088.1
CR  - Zhao Z, 2017, ECOL INFORM, V39, P99, DOI 10.1016/j.ecoinf.2017.04.003
PU  - SPRINGER
PI  - DORDRECHT
PA  - VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS
DA  - MAR
PY  - 2021
VL  - 30
IS  - 3
SP  - 643
EP  - 657
DO  - 10.1007/s10531-020-02107-1
AN  - WOS:000607992700001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Maheswari, M
AU  - Josephine, MS
AU  - Jeyabalaraja, V
TI  - Customized deep neural network model for autonomous and efficient surveillance of wildlife in national parks
T2  - COMPUTERS & ELECTRICAL ENGINEERING
LA  - English
KW  - Deep neural network
KW  - DNN
KW  - Tensor flow
KW  - Custom detection model
KW  - Vehicle
KW  - Wild animals
KW  - Lion
KW  - Elephant
KW  - Car
KW  - Motorcycle
KW  - Tiger detection
AB  - As National parks are one of the best places of attraction to the tourists, the so-called abode of the animals has to be protected due to the frequent intervention of human beings. On the other hand, the lives of human beings have also to be insulated from the sudden and vulnerable attack of the animals. When it comes to 'life', it becomes a vital factor for humans and animals. Therefore, here arises a dire need to create a system which can protect the life and safety of human beings as well as animals. However, when it comes to mutual safety of both, there has not been an effective solution till date that can practically be implemented in a national park or a sanctuary. In this scenario, 'Video Object Identifying' using Machine Learning Technique has been proven to render promising results in terms of accuracy and performance. Classifying and identifying objects using Customized Convolutional Neural-Network Model in order to detect the multi-class objects (vehicle and animal) become the aim of this present research. In this research, five categories of data sets namely lion, elephant, tiger, car and motorcycle have been used for training and testing. The custom model for video object detection renders with an accuracy of 82% which proves its efficiency in detecting the multi class datasets.
AD  - Deemed Univ, Dr MGR Educ & Res Inst, Dept Informat Technol, Chennai 600095, Tamil Nadu, IndiaAD  - Deemed Univ, Dr MGR Educ & Res Inst, Dept Comp Applicat, Chennai 600095, Tamil Nadu, IndiaAD  - Velammal Engn Coll, Dept Comp Sci & Engn, Chennai 600066, Tamil Nadu, IndiaC3  - Velammal Engineering CollegeCR  - Abu MDA, 2019, INT J ENG RES TECHNO, V12, P563
CR  - Ahmed I, Sustain Cities Soc, V69, P1
CR  - Al-Azzo F, 2018, INT J ADV COMPUT SC, V9, P9
CR  - Anand Shaurabh, 2017, Journal of Asia-Pacific Biodiversity, V10, P154
CR  - Chalmers C, 2021, J UNMANNED VEH SYST, V9, P112, DOI 10.1139/juvs-2020-0018
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Dharmadhikari SC, 2020, J CRIT REV, V7, P1770
CR  - Ibraheam M, 2021, ARTIF INTELL, V2, P552
CR  - Jiao LC, 2019, IEEE ACCESS, V7, P128837, DOI 10.1109/ACCESS.2019.2939201
CR  - Kumar A, 2017, IJSRD INT J SCI RES, V5, P1762
CR  - Maheswari M, 2020, J GREEN ENG JGE, V10, P10114
CR  - Mihaescu RE, 2020, ALGORITHMS, V13, DOI 10.3390/a13120343
CR  - Moallem G., 2021, KNOWL-BASED SYST, V216, P1
CR  - Raja AA, 2018, INT RES J ENG TECHNO, V5, P36
CR  - Rezaei M, 2020, Appl Sci, V10, P1
CR  - Rodriguez-Conde I, 2021, SYSTEMS APPL SCI, V11, P1
CR  - Sharma KU, 2017, INT J COMPUT VIS ROB, V7, P196, DOI DOI 10.1504/IJCVR.2017.081234
CR  - Sharma SU, 2017, IEEE ACCESS, V5, P347, DOI 10.1109/ACCESS.2016.2642981
CR  - Tang P, 2020, IEEE T PATTERN ANAL, V42, P1272, DOI 10.1109/TPAMI.2019.2910529
CR  - The CN, 2018, P MATEC WEB C, V155, P1
CR  - Tiwari M, 2017, INT J COMPUT INTELL, V13, P745
CR  - Verma Gyanendra K., 2018, Proceedings of 2nd International Conference on Computer Vision & Image Processing. CVIP 2017. Advances in Intelligent Systems and Computing (704), P327, DOI 10.1007/978-981-10-7898-9_27
CR  - Zheng ZH, 2021, IEEE T CYBERNETICS, DOI 10.1109/TCYB.2021.3095305
CR  - Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
CR  - Zhu H, 2020, Appl Sci, V10, P1
CR  - Zhu XZ, 2018, PROC CVPR IEEE, P7210, DOI 10.1109/CVPR.2018.00753
PU  - PERGAMON-ELSEVIER SCIENCE LTD
PI  - OXFORD
PA  - THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND
DA  - MAY
PY  - 2022
VL  - 100
DO  - 10.1016/j.compeleceng.2022.107913
AN  - WOS:000793037900006
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  26
ER  -

TY  - JOUR
AU  - Kellenberger, B
AU  - Veen, T
AU  - Folmer, E
AU  - Tuia, D
TI  - 21 000 birds in 4.5 h: efficient large-scale seabird detection with machine learning
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - coastal birds
KW  - convolutional neural network
KW  - deep learning
KW  - remote sensing
KW  - unmanned aerial vehicle
KW  - wildlife census
KW  - BIODIVERSITY
KW  - INDICATORS
KW  - DRONES
KW  - IMAGES
KW  - DIET
AB  - We address the task of automatically detecting and counting seabirds in unmanned aerial vehicle (UAV) imagery using deep convolutional neural networks (CNNs). Our study area, the coast of West Africa, harbours significant breeding colonies of terns and gulls, which as top predators in the food web function as important bioindicators for the health of the marine ecosystem. Surveys to estimate breeding numbers have hitherto been carried out on foot, which is tedious, imprecise and causes disturbance. By using UAVs and CNNs that allow localizing tens of thousands of birds automatically, we show that all three limitations can be addressed elegantly. As we employ a lightweight CNN architecture and incorporate prior knowledge about the spatial distribution of birds within the colonies, we were able to reduce the number of bird annotations required for CNN training to just 200 examples per class. Our model obtains good accuracy for the most abundant species of royal terns (90% precision at 90% recall), but is less accurate for the rarer Caspian terns and gull species (60% precision at 68% recall, respectively 20% precision at 88% recall), which amounts to around 7% of all individuals present. In sum, our results show that we can detect and classify the majority of 21 000 birds in just 4.5 h, start to finish, as opposed to about 3 weeks of tediously identifying and labelling all birds by hand.
AD  - Wageningen Univ & Res, Lab Geoinformat Sci & Remote Sensing, Wageningen, NetherlandsAD  - EPFL, Environm Computat Sci & Earth Observat Lab ECEO, Sion, SwitzerlandAD  - Quest Univ, Squamish, BC, CanadaAD  - Aeria Solut Ltd, Squamish, BC, CanadaC3  - Wageningen University & ResearchFU  - NVIDIA Corporation
FX  - We also gratefully acknowledge the support of the NVIDIA Corporation with the donation of a Titan V GPU used for this research, as well as the Microsoft AI for Earth program and team.
CR  - Akcay HG, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10071207
CR  - Andrew ME, 2017, REMOTE SENS ECOL CON, V3, P66, DOI 10.1002/rse2.38
CR  - Baran E., 2001, Naga, V23, P4
CR  - Bengio Y., 2009, PROC ICML, P41
CR  - BESAG J, 1986, J R STAT SOC B, V48, P259
CR  - Borowicz A, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22313-w
CR  - Butchart SHM, 2010, SCIENCE, V328, P1164, DOI 10.1126/science.1187512
CR  - Camphuysen CJ, 2005, AFR J MAR SCI, V27, P427, DOI 10.2989/18142320509504101
CR  - Cardinale BJ, 2012, NATURE, V486, P59, DOI 10.1038/nature11148
CR  - Carney KM, 1999, WATERBIRDS, V22, P68, DOI 10.2307/1521995
CR  - COSTANZA R, 1993, AMBIO, V22, P88
CR  - Diagana C., 2004, MANUAL MONITORING SE
CR  - Edney AJ, 2021, IBIS, V163, P317, DOI 10.1111/ibi.12871
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Einoder LD, 2009, FISH RES, V95, P6, DOI 10.1016/j.fishres.2008.09.024
CR  - Food and Agriculture Organization (FAO), 2011, REV STATE WORLD MARI
CR  - Frederick PC, 2003, J FIELD ORNITHOL, V74, P281, DOI 10.1648/0273-8570-74.3.281
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
CR  - Gregory R.D., 2003, Ornis Hungarica, V12-13, P11
CR  - Hamilton G, 2020, BIOL CONSERV, V247, DOI 10.1016/j.biocon.2020.108598
CR  - He K., 2015, CVPR
CR  - Hodgson JC, 2016, SCI REP-UK, V6, DOI 10.1038/srep22574
CR  - Hong SJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071651
CR  - Huh M., ARXIV160808614
CR  - Ivosevic Bojana, 2015, Journal of Ecology and Environment, V38, P113, DOI 10.5141/ecoenv.2015.012
CR  - Jain R., 2019, ARXIV PREPRINT ARXIV
CR  - Kellenberger B, 2020, METHODS ECOL EVOL, V11, P1716, DOI 10.1111/2041-210X.13489
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Parsons M, 2008, ICES J MAR SCI, V65, P1520, DOI 10.1093/icesjms/fsn155
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rumelhart D. E., 1995, BACKPROPAGATION THEO, P1
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Schindler K, 2012, IEEE T GEOSCI REMOTE, V50, P4534, DOI 10.1109/TGRS.2012.2192741
CR  - Terletzky P.A., 2016, J SIGNAL INFORM PROC, V7, P123, DOI [10.4236/jsip.2016.73013, DOI 10.4236/JSIP.2016.73013]
CR  - Tuia D, 2018, IEEE T GEOSCI REMOTE, V56, P3277, DOI 10.1109/TGRS.2018.2797316
CR  - Ulyanov D., 2016, ARXIV160708022
CR  - Veen J, 2019, ARDEA, V107, P33, DOI 10.5253/arde.v107i1.a8
CR  - Veen J, 2018, ARDEA, V106, P5, DOI 10.5253/arde.v106i1.a7
CR  - Veen J, 2018, WATERBIRDS, V41, P295, DOI 10.1675/063.041.0309
CR  - Veen T., 2003, OISEAUX PISCIVORES C
CR  - Volpi M, 2017, IEEE T GEOSCI REMOTE, V55, P881, DOI 10.1109/TGRS.2016.2616585
CR  - Zhu S.-C., 2019, P IEEE C COMP VIS PA
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - SEP
PY  - 2021
VL  - 7
IS  - 3
SP  - 445
EP  - 460
DO  - 10.1002/rse2.200
AN  - WOS:000632679000001
N1  - Times Cited in Web of Science Core Collection:  4
Total Times Cited:  4
Cited Reference Count:  45
ER  -

TY  - CPAPER
AU  - Munian, Y
AU  - Martinez-Molina, A
AU  - Alamaniotis, M
ED  - Bhat, CR
TI  - Design and Implementation of a Nocturnal Animal Detection Intelligent System in Transportation Applications
T2  - INTERNATIONAL CONFERENCE ON TRANSPORTATION AND DEVELOPMENT 2021: TRANSPORTATION OPERATIONS, TECHNOLOGIES, AND SAFETY
LA  - English
CP  - ASCE International Conference on Transportation and Development (ICTD) - Transportation Operations, Technologies, and Safety
AB  - Wildlife vehicle collision, commonly called roadkill, is a nascent threat to both humans and wild animals. The collision results in property damage, injuries, death, and financial losses to society and mankind. An automobile system is integrated with alert notification, image processing, and machine learning models. This study explores a newer dimension for wild animal detection and signals the driver during active nocturnal hours. The intelligent system uses histogram of oriented gradients (HOG), which extracts the essential thermography image features; next, the extracted features are fed to the pre-trained, convolutional neural network (1D-CNN). This intelligent system has been tested on a set of real scenarios and gives approximately 91% and 92% accuracy in the alert notification and detection of the wild animals in the transportation road system in the city of San Antonio, TX, USA. This proposed system will contribute to the reduction of vehicle collisions caused by wild animals.
AD  - Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USAAD  - Univ Texas San Antonio, Dept Architecture, San Antonio, TX USAC3  - University of Texas SystemC3  - University of Texas at San Antonio (UTSA)C3  - University of Texas SystemC3  - University of Texas at San Antonio (UTSA)CR  - Benten A, 2018, ACCIDENT ANAL PREV, V120, P64, DOI 10.1016/j.aap.2018.08.003
CR  - Christiansen P, 2014, SENSORS-BASEL, V14, P13778, DOI 10.3390/s140813778
CR  - Gkritza K, 2014, J TRANSP ENG, V140, DOI 10.1061/(ASCE)TE.1943-5436.0000629
CR  - Munian Yuvaraj, 2020, 2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA), DOI 10.1109/IISA50023.2020.9284365
CR  - Peeters J, 2018, OPTIM ENG, V19, P163, DOI 10.1007/s11081-017-9368-z
CR  - Santhi V, 2017, ADV CIV IND ENG BOOK, P1, DOI 10.4018/978-1-5225-2423-6
CR  - Sawyer H, 2016, WILDLIFE SOC B, V40, P211, DOI 10.1002/wsb.650
CR  - Sibanda V, 2019, PROC CIRP, V84, P755, DOI 10.1016/j.procir.2019.04.175
CR  - Zhou D., 2012, P INT C IM PROC COMP, V2, P969
PU  - AMER SOC CIVIL ENGINEERS
PI  - NEW YORK
PA  - UNITED ENGINEERING CENTER, 345 E 47TH ST, NEW YORK, NY 10017-2398 USA
PY  - 2021
SP  - 438
EP  - 449
AN  - WOS:000711337600038
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  9
ER  -

TY  - JOUR
AU  - Yang, DQ
AU  - Li, T
AU  - Liu, MT
AU  - Li, XW
AU  - Chen, BH
TI  - A systematic study of the class imbalance problem: Automatically identifying empty camera trap images using convolutional neural networks
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Camera trap images
KW  - Class imbalance
KW  - Convolutional neural networks
KW  - Deep learning
KW  - Image classification
AB  - Camera traps, which are widely used in wildlife surveys, often produce massive images, and many of them are empty images not contain animals. Using the deep learning model to automatically identify the empty camera trap images can reduce the workload of manual classification significantly. However, the performance of deep learning models is easily affected by the class imbalance problem of training datasets, which is a common problem for actual wildlife survey projects. Almost all previous studies on empty image recognition used down sampling or oversampling methods to eliminate the effect of class imbalance on the performance of deep learning classifiers. The class imbalance problem has been systematically studied in the field of traditional image recognition, yet very limited research is available in the context of identifying camera trap images taken from highly cluttered natural scenes. This study systematically studied the impact of class imbalance on model performance when using a deep learning model to identify empty camera trap images. Then we proposed the construction method of training sets of the deep learning model when the data set has different class imbalance levels. Based on results from our experiments we concluded that (i) the class imbalance showed little effect on the performance of the model when the empty image ratio (EIR) in the data set was between 10% and 70%, so the training sets can be randomly built without changing the class distribution; (ii) we recommended using over sampling to partially eliminate class imbalance to reduce omission errors when the EIR of the data set exceeded 70%; (iii) when the EIRs of the training set and the test set were close, the overall error, omission error, and commission error of the model were relatively smaller, and the model tended to achieve a better overall performance; (iv) the omission and commission errors can be adjusted by changing the percentage of empty images in the training set.
AD  - Dali Univ, Dept Math & Comp Sci, Dali 671003, Yunnan, Peoples R ChinaAD  - Dali Univ, Data Secur & Applicat Innovat Team, Dali 671003, Yunnan, Peoples R ChinaC3  - Dali UniversityC3  - Dali UniversityFU  - National Natural ScienceFoundation of China [31960119]; Yunnan Provincial Science and Technology Department University Joint Project [2017FH001-027]; Innovative Project of Dali University [ZKLX2020308]
FX  - We appreciate the support of the National Natural ScienceFoundation of China (31960119) and the Yunnan Provincial Science and Technology Department University Joint Project (2017FH001-027) and the Innovative Project of Dali University (ZKLX2020308) .
CR  - abak M., 2019, MACHINE LEARNING CLA
CR  - Bennin KE, 2018, PROCEEDINGS 2018 IEEE/ACM 40TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING (ICSE), P699, DOI 10.1145/3180155.3182520
CR  - Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
CR  - Chang J-R, 2015, ARXIV151102583
CR  - Chawla NV, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P875, DOI 10.1007/978-0-387-09823-4_45
CR  - Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Dertien JS, 2017, J WILDLIFE MANAGE, V81, P1457, DOI 10.1002/jwmg.21308
CR  - Diaz-Pulido Angélica, 2011, Mastozool. neotrop., V18, P63
CR  - Drummond C., 2003, PROC WORKSHOP LEARN, V11, P1
CR  - Duarte A, 2017, J WILDLIFE MANAGE, V81, P182, DOI 10.1002/jwmg.21146
CR  - Frey S, 2017, REMOTE SENS ECOL CON, V3, P123, DOI 10.1002/rse2.60
CR  - Havaei M, 2017, MED IMAGE ANAL, V35, P18, DOI 10.1016/j.media.2016.05.004
CR  - He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
CR  - He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
CR  - Herna, 2004, SIGKDD EXPLORATIONS, V6, P30, DOI [https://doi.org/10.1145/1007730.1007736, DOI 10.1145/1007730.1007736, 10.1145/1007730.1007736]
CR  - Jaccard N, 2017, J X-RAY SCI TECHNOL, V25, P323, DOI 10.3233/XST-16199
CR  - Japkowicz N, 2000, NEURAL COMPUT, V12, P531, DOI 10.1162/089976600300015691
CR  - Japkowicz N., 2002, Intelligent Data Analysis, V6, P429
CR  - Johnson JM, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0192-5
CR  - Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
CR  - Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
CR  - Krizhevsky A, 2009, LEARNING MULTIPLE LA, P1
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Lee H, 2006, LECT NOTES COMPUT SC, V4233, P21
CR  - Li FM, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P761, DOI 10.1109/ICIVC.2017.7984657
CR  - Liu XY, 2009, IEEE T SYST MAN CY B, V39, P539, DOI 10.1109/TSMCB.2008.2007853
CR  - Marouf M., 2020, P 3 INT C COMP MATH, P1, DOI [10.1109/iCoMET48670.2020.9073878, DOI 10.1109/ICOMET48670.2020.9073878]
CR  - Matwin, 1997, P 14 INT C MACH LEAR, P179, DOI DOI 10.1016/J.INS.2014.08.051
CR  - Mazurowski MA, 2008, NEURAL NETWORKS, V21, P427, DOI 10.1016/j.neunet.2007.12.031
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Rich LN, 2019, J WILDLIFE MANAGE, V83, P855, DOI 10.1002/jwmg.21654
CR  - Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29
CR  - Steenweg R, 2016, BIOL CONSERV, V201, P192, DOI 10.1016/j.biocon.2016.06.020
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yousif H, 2017, IEEE INT SYMP CIRC S
CR  - Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
CR  - Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
CR  - Zhou ZH, 2006, IEEE T KNOWL DATA EN, V18, P63, DOI 10.1109/TKDE.2006.17
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - SEP
PY  - 2021
VL  - 64
DO  - 10.1016/j.ecoinf.2021.101350
AN  - WOS:000691767700003
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  43
ER  -

TY  - JOUR
AU  - Clapham, M
AU  - Miller, E
AU  - Nguyen, M
AU  - Van Horn, RC
TI  - Multispecies facial detection for individual identification of wildlife: a case study across ursids
T2  - MAMMALIAN BIOLOGY
LA  - English
KW  - Bears
KW  - Deep learning
KW  - Face recognition
KW  - Individual ID
KW  - Machine learning
KW  - Ursidae
KW  - CAPTURE-RECAPTURE
KW  - ANDEAN BEARS
KW  - PHOTOGRAPHS
KW  - DENSITY
AB  - To address biodiversity decline in the era of big data, replicable methods of data processing are needed. Automated methods of individual identification (ID) via computer vision are valuable in conservation research and wildlife management. Rapid and systematic methods of image processing and analysis are fundamental to an ever-growing need for effective conservation research and practice. Bears (ursids) are an interesting test system for examining computer vision techniques for wildlife, as they have variable facial morphology, variable presence of individual markings, and are challenging to research and monitor. We leveraged existing imagery of bears living under human care to develop a multispecies bear face detector, a critical part of individual ID pipelines. We compared its performance across species and on a pre-existing wild brown bear Ursus arctos dataset (BearID), to examine the robustness of convolutional neural networks trained on animals under human care. Using the multispecies bear face detector and retrained sub-applications of BearID, we prototyped an end-to-end individual ID pipeline for the declining Andean bear Tremarctos ornatus. Our multispecies face detector had an average precision of 0.91-1.00 across all eight bear species, was transferable to images of wild brown bears (AP = 0.93), and correctly identified individual Andean bears in 86% of test images. These preliminary results indicate that a multispecies-trained network can detect faces of a single species sufficiently to achieve high-performance individual classification, which could speed-up the transferability and application of automated individual ID to a wider range of taxa.
AD  - Univ Victoria, Dept Geog, 3800 Finnerty Rd, Victoria, BC V8P 5C2, CanadaAD  - BearID Project, Sooke, BC, CanadaAD  - San Diego Zoo Wildlife Alliance, San Diego, CA USAC3  - University of VictoriaFU  - Microsoft AI for Earth program; Natural Sciences and Engineering Research Council of Canada [CRDPJ 523329-18, ALLRP 559534-20]; San Diego Zoo Wildlife Alliance; Google for Nonprofits; Knight Inlet Lodge; Wild Bear Lodge
FX  - Cloud computing (Microsoft Azure) credits for this project were granted by Microsoft AI for Earth program. MC is supported by the Natural Sciences and Engineering Research Council of Canada (CRDPJ 523329-18; ALLRP 559534-20), in combination with industry partners (Knight Inlet Lodge and Wild Bear Lodge) and Nanwakolas Council. RCVH is supported by the San Diego Zoo Wildlife Alliance. Google for Nonprofits funded Google Workspace which helped to facilitate this collaborative project.
CR  - Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
CR  - Appleton RD, 2018, J ZOOL, V305, P196, DOI 10.1111/jzo.12553
CR  - Araujo G, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-53718-w
CR  - Beery S, 2019, ARXIV190405916
CR  - Beery S, 2019, ARXIV190706772
CR  - Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
CR  - Berger-Wolf T.Y., 2017, ARXIV PREPRINT ARXIV
CR  - Brust CA, 2017, IEEE INT CONF COMP V, P2820, DOI 10.1109/ICCVW.2017.333
CR  - Buehler P, 2019, ECOL INFORM, V50, P191, DOI 10.1016/j.ecoinf.2019.02.003
CR  - Chen P, 2020, ECOL EVOL, V10, P3561, DOI 10.1002/ece3.6152
CR  - Choo YR, 2020, GLOB ECOL CONSERV, V24, DOI 10.1016/j.gecco.2020.e01294
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Clapham M, 2020, ECOL EVOL, V10, P12883, DOI 10.1002/ece3.6840
CR  - Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
CR  - Dalal N., 2005, IEEE COMPUTER SOC C, P886, DOI 10.1109/CVPR.2005.177
CR  - Deb D, 2018, INT CONF BIOMETR THE
CR  - Dharaiya N, 2020, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2020-1.RLTS.T13143A166519315.en, DOI 10.2305/IUCN.UK.2020-1.RLTS.T13143A166519315.EN]
CR  - Dirzo R, 2014, SCIENCE, V345, P401, DOI 10.1126/science.1251817
CR  - Ditria EM, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.00429
CR  - Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
CR  - Garshelis D, 2020, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2020-3.RLTS.T22824A166528664.en, DOI 10.2305/IUCN.UK.2020-3.RLTS.T22824A166528664.EN]
CR  - Guo ST, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101412
CR  - Higashide D, 2012, J ZOOL, V288, P199, DOI 10.1111/j.1469-7998.2012.00942.x
CR  - Hughes B, 2017, INT J COMPUT VISION, V122, P542, DOI 10.1007/s11263-016-0961-y
CR  - Johansson O, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63367-z
CR  - Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
CR  - Kelly MJ, 2008, NORTHEAST NAT, V15, P249, DOI 10.1656/1092-6194(2008)15[249:CTOCTS]2.0.CO;2
CR  - Khan MH, 2020, PROC CVPR IEEE, P6937, DOI 10.1109/CVPR42600.2020.00697
CR  - King D. E., 2015, ARXIV PREPRINT ARXIV
CR  - King DE, 2009, J MACH LEARN RES, V10, P1755
CR  - Korschens M, 2018, ARXIV181204418
CR  - Kutschera VE, 2014, MOL BIOL EVOL, V31, P2004, DOI 10.1093/molbev/msu186
CR  - Lahoz-Monfort JJ, 2019, BIOSCIENCE, V69, P823, DOI 10.1093/biosci/biz090
CR  - Loos A, 2018, EUR SIGNAL PR CONF, P1805, DOI 10.23919/EUSIPCO.2018.8553439
CR  - Loos A, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-49
CR  - Miele V, 007377 BIORXIV, V2020, DOI [10.1101/2020.03.25.007377, DOI 10.1101/2020.03.25.007377]
CR  - Molina S, 2017, URSUS, V28, P117, DOI 10.2192/URSU-D-16-00030.1
CR  - Morrell N, 2021, GLOB ECOL CONSERV, V26, DOI 10.1016/j.gecco.2021.e01473
CR  - Ngoprasert D, 2012, URSUS, V23, P117, DOI 10.2192/URSUS-D-11-00009.1
CR  - Nipko RB, 2020, WILDLIFE SOC B, V44, P424, DOI 10.1002/wsb.1086
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Penteriani V., 2020, BEARS OF THE WORLD, DOI [10.1017/9781108692571, DOI 10.1017/9781108692571]
CR  - Penteriani V, 2020, URSUS, V31, DOI 10.2192/URSUS-D-19-00027.1
CR  - Ramsey AB, 2019, WILDLIFE RES, V46, P326, DOI 10.1071/WR18049
CR  - Ravoor PC, 2020, COMPUT SCI REV, V38, DOI 10.1016/j.cosrev.2020.100289
CR  - Reyes Adriana, 2017, Therya, V8, P83, DOI 10.12933/therya-17-453
CR  - Rodríguez Daniel, 2020, Pap. Avulsos Zool., V60, pe20206030, DOI 10.11606/1807-0205/2020.60.30
CR  - Schneider S, 2020, IEEE WINT CONF APPL, P44, DOI 10.1109/WACVW50321.2020.9096925
CR  - Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
CR  - Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
CR  - Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
CR  - Scotson L, 2017, IUCN RED LIST THREAT, V2017, DOI [10.2305/IUCN.UK.2017-3.RLTS. T9760A45033547.en, DOI 10.2305/IUCN.UK.2017-3.RLTS.T9760A45033547.EN, 10.2305/IUCN.UK.2017-3.RLTS.T9760A45033547.en]
CR  - Shimozuru M, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176251
CR  - Steenweg R, 2017, FRONT ECOL ENVIRON, V15, P26, DOI 10.1002/fee.1448
CR  - Van Horn RC, 2015, PEERJ, V3, DOI 10.7717/peerj.1042
CR  - Van Horn RC, 2014, WILDLIFE BIOL, V20, P291, DOI 10.2981/wlb.00023
CR  - Velez-Liendo X, 2017, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2017-3.RLTS.T22066A45034047.en, DOI 10.2305/IUCN.UK.2017-3.RLTS.T22066A45034047.EN]
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Yoshizaki J, 2009, ECOLOGY, V90, P3, DOI 10.1890/08-0304.1
CR  - Zheng X, 2016, J ZOOL, V300, P247, DOI 10.1111/jzo.12377
PU  - SPRINGER HEIDELBERG
PI  - HEIDELBERG
PA  - TIERGARTENSTRASSE 17, D-69121 HEIDELBERG, GERMANY
DO  - 10.1007/s42991-021-00168-5
AN  - WOS:000782162400001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  60
ER  -

TY  - JOUR
AU  - Seydi, ST
AU  - Hasanlou, M
AU  - Chanussot, J
TI  - DSMNN-Net: A Deep Siamese Morphological Neural Network Model for Burned Area Mapping Using Multispectral Sentinel-2 and Hyperspectral PRISMA Images
T2  - REMOTE SENSING
LA  - English
KW  - deep learning
KW  - PRISMA
KW  - burned area
KW  - Sentinel-2
KW  - morphological operator
KW  - convolutional neural network
KW  - CLASSIFICATION
KW  - LANDSAT
KW  - REGION
KW  - MSI
AB  - Wildfires are one of the most destructive natural disasters that can affect our environment, with significant effects also on wildlife. Recently, climate change and human activities have resulted in higher frequencies of wildfires throughout the world. Timely and accurate detection of the burned areas can help to make decisions for their management. Remote sensing satellite imagery can have a key role in mapping burned areas due to its wide coverage, high-resolution data collection, and low capture times. However, although many studies have reported on burned area mapping based on remote sensing imagery in recent decades, accurate burned area mapping remains a major challenge due to the complexity of the background and the diversity of the burned areas. This paper presents a novel framework for burned area mapping based on Deep Siamese Morphological Neural Network (DSMNN-Net) and heterogeneous datasets. The DSMNN-Net framework is based on change detection through proposing a pre/post-fire method that is compatible with heterogeneous remote sensing datasets. The proposed network combines multiscale convolution layers and morphological layers (erosion and dilation) to generate deep features. To evaluate the performance of the method proposed here, two case study areas in Australian forests were selected. The framework used can better detect burned areas compared to other state-of-the-art burned area mapping procedures, with a performance of >98% for overall accuracy index, and a kappa coefficient of >0.9, using multispectral Sentinel-2 and hyperspectral PRISMA image datasets. The analyses of the two datasets illustrate that the DSMNN-Net is sufficiently valid and robust for burned area mapping, and especially for complex areas.
AD  - Univ Tehran, Coll Engn, Sch Surveying & Geospatial Engn, Tehran 1417466191, IranAD  - Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100094, Peoples R ChinaAD  - Univ Grenoble Alpes, GIPSA Lab, Grenoble INP, CNRS, F-38000 Grenoble, FranceC3  - University of TehranC3  - Chinese Academy of SciencesC3  - UDICE-French Research UniversitiesC3  - Communaute Universite Grenoble AlpesC3  - Institut National Polytechnique de GrenobleC3  - Universite Grenoble Alpes (UGA)C3  - Centre National de la Recherche Scientifique (CNRS)FU  - AXA fund; MIAI @ Grenoble Alpes [ANR-19-P3IA-0003]
FX  - The authors would like to thank the European Space Agency and the Italian Space Agency for providing the datasets. We thank the anonymous reviewers for their valuable comments on our manuscript. This research was partially supported by the AXA fund for research and by MIAI @ Grenoble Alpes, (ANR-19-P3IA-0003).
CR  - Arabi ME, 2018, INT GEOSCI REMOTE SE, P5041, DOI 10.1109/IGARSS.2018.8518178
CR  - Ban YF, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-019-56967-x
CR  - Castillo EB, 2020, ISPRS INT J GEO-INF, V9, DOI 10.3390/ijgi9100564
CR  - Belenguer-Plomer MA, 2021, REMOTE SENS ENVIRON, V260, DOI 10.1016/j.rse.2021.112468
CR  - Chen YS, 2016, IEEE T GEOSCI REMOTE, V54, P6232, DOI 10.1109/TGRS.2016.2584107
CR  - Chiang SH, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19245423
CR  - Chowdhury S., ARXIV210808952, V2021
CR  - Danfeng Hong, 2021, 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, P1245, DOI 10.1109/IGARSS47720.2021.9554255
CR  - de Bem PP, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12162576
CR  - De Luca G, 2021, GISCI REMOTE SENS, V58, P516, DOI 10.1080/15481603.2021.1907896
CR  - DeLancey ER, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010002
CR  - Dhaka VS, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144749
CR  - Donezar U, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11222607
CR  - Duane A, 2021, CLIMATIC CHANGE, V165, DOI 10.1007/s10584-021-03066-4
CR  - ElGharbawi T, 2021, ISPRS J PHOTOGRAMM, V173, P1, DOI 10.1016/j.isprsjprs.2021.01.001
CR  - Farasin A, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10124332
CR  - Franchi G, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107246
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Grivei AC, 2020, 2020 13TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS (COMM), P189, DOI 10.1109/COMM48946.2020.9141999
CR  - Gu YF, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3084-1
CR  - Guarini R, 2018, INT GEOSCI REMOTE SE, P179, DOI 10.1109/IGARSS.2018.8517785
CR  - Hao DL, 2018, IEEE T GEOSCI REMOTE, V56, P3903, DOI 10.1109/TGRS.2018.2816015
CR  - Haque M.K., 2021, J ENV PROT, V12, P391, DOI DOI 10.4236/JEP.2021.126024
CR  - Hashemi-Beni L, 2021, IEEE J-STARS, V14, P2127, DOI 10.1109/JSTARS.2021.3051873
CR  - He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
CR  - Hu XK, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13081509
CR  - Islam MA, 2021, IEEE T NEUR NET LEAR, V32, P4826, DOI 10.1109/TNNLS.2020.3025723
CR  - Klemen M., 2020, ARXIV201112432
CR  - Knopp L, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12152422
CR  - Lestari AI, 2021, 2021 IEEE 11TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P52, DOI 10.1109/CCWC51732.2021.9376117
CR  - Li Y, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13173436
CR  - Li ZW, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3084827
CR  - Lima TA, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080961
CR  - Limonova EE, 2021, IEEE ACCESS, V9, P97569, DOI 10.1109/ACCESS.2021.3094484
CR  - Liu SC, 2020, EUR J REMOTE SENS, V53, P104, DOI 10.1080/22797254.2020.1738900
CR  - Liu SC, 2017, IEEE GEOSCI REMOTE S, V14, P324, DOI 10.1109/LGRS.2016.2639540
CR  - Liu WQ, 2021, INFORM SCIENCES, V581, P655, DOI 10.1016/j.ins.2021.10.022
CR  - Lizundia-Loiola J, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13214295
CR  - Llorens R, 2021, INT J APPL EARTH OBS, V95, DOI 10.1016/j.jag.2020.102243
CR  - Loizzo R, 2018, INT GEOSCI REMOTE SE, P175, DOI 10.1109/IGARSS.2018.8518512
CR  - Lu L, 2019, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-3-030-13969-8
CR  - Ma A., 2021, REMOTE SENS ENVIRON, P265
CR  - Mardian J, 2021, REMOTE SENS ENVIRON, V255, DOI 10.1016/j.rse.2021.112292
CR  - Masek JG, 2020, REMOTE SENS ENVIRON, V248, DOI 10.1016/j.rse.2020.111968
CR  - Mondal R, 2021, MORPHOLOGICAL NETWOR
CR  - Moya L, 2021, IEEE T GEOSCI REMOTE, V59, P8288, DOI 10.1109/TGRS.2020.3046004
CR  - Munoz DF, 2021, SCI TOTAL ENVIRON, V782, DOI 10.1016/j.scitotenv.2021.146927
CR  - Ngadze F, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0232962
CR  - Nogueira K, 2021, IEEE ACCESS, V9, P114308, DOI 10.1109/ACCESS.2021.3104405
CR  - Nohrstedt D, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-020-20435-2
CR  - Nolde M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12132162
CR  - Palaiologou P, 2021, FIRE-BASEL, V4, DOI 10.3390/fire4020018
CR  - Pandey PC, 2021, GEOCARTO INT, V36, P957, DOI 10.1080/10106049.2019.1629647
CR  - Pinto MM, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13091608
CR  - Pouyap M., 2021, J SIGNAL INF PROCESS, V12, P71
CR  - PROY C, 1989, REMOTE SENS ENVIRON, V30, P21, DOI 10.1016/0034-4257(89)90044-8
CR  - Quintano C, 2018, INT J APPL EARTH OBS, V64, P221, DOI 10.1016/j.jag.2017.09.014
CR  - Raja G., 2021, P IEEE INT C COMM WO, P1
CR  - Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/neco_a_00990, 10.1162/NECO_a_00990]
CR  - Roy DP, 2021, INT J APPL EARTH OBS, V96, DOI 10.1016/j.jag.2020.102271
CR  - Roy DP, 2019, REMOTE SENS ENVIRON, V231, DOI 10.1016/j.rse.2019.111254
CR  - Roy S.K., 2021, IEEE T GEOSCI ELECT
CR  - Salehi SSM, 2017, LECT NOTES COMPUT SC, V10541, P379, DOI 10.1007/978-3-319-67389-9_44
CR  - Serra-Burriel F., 2021, REMOTE SENS ENVIRON, P265
CR  - Seydi S., 2019, INT ARCH PHOTOGRAMM
CR  - Seydi ST, 2021, IEEE J-STARS, V14, P10941, DOI 10.1109/JSTARS.2021.3123163
CR  - Seydi ST, 2021, MEASUREMENT, V186, DOI 10.1016/j.measurement.2021.110137
CR  - Seydi ST, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13020220
CR  - Shen Y., 2019, ARXIV190901532
CR  - Sun YL, 2021, PATTERN RECOGN, V109, DOI 10.1016/j.patcog.2020.107598
CR  - Syifa M, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12040623
CR  - Vangi E, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21041182
CR  - Wambugu N, 2021, INT J APPL EARTH OBS, V103, DOI 10.1016/j.jag.2021.102515
CR  - Wu CY, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13050905
CR  - Xu LL, 2014, REMOTE SENS ENVIRON, V141, P14, DOI 10.1016/j.rse.2013.10.012
CR  - Xulu S, 2021, ISPRS INT J GEO-INF, V10, DOI 10.3390/ijgi10080511
CR  - Yang L, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13173394
CR  - Yu CY, 2020, IEEE J-STARS, V13, P2485, DOI 10.1109/JSTARS.2020.2983224
CR  - Yu QT, 2021, INT J APPL EARTH OBS, V102, DOI 10.1016/j.jag.2021.102404
CR  - Yu Y., INT J APPL EARTH OBS, V104
CR  - Zhan Y, 2017, IEEE GEOSCI REMOTE S, V14, P1845, DOI 10.1109/LGRS.2017.2738149
CR  - Zhang PZ, 2021, REMOTE SENS ENVIRON, V261, DOI 10.1016/j.rse.2021.112467
CR  - Zhang Q, 2021, REMOTE SENS ENVIRON, V264, DOI 10.1016/j.rse.2021.112575
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - DEC
PY  - 2021
VL  - 13
IS  - 24
DO  - 10.3390/rs13245138
AN  - WOS:000771728900024
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  83
ER  -

TY  - JOUR
AU  - Lostanlen, V
AU  - Salamon, J
AU  - Farnsworth, A
AU  - Kelling, S
AU  - Bello, JP
TI  - Robust sound event detection in bioacoustic sensor networks
T2  - PLOS ONE
LA  - English
KW  - SIGNAL-PROCESSING RESEARCH
KW  - ADAPTIVE NEURAL-NETWORK
KW  - ONSET DETECTION
KW  - BIRD MIGRATION
KW  - FLIGHT CALLS
KW  - WEATHER
KW  - CLASSIFICATION
KW  - DIVERSITY
KW  - ECOLOGY
KW  - MODELS
AB  - Bioacoustic sensors, sometimes known as autonomous recording units (ARUs), can record sounds of wildlife over long periods of time in scalable and minimally invasive ways. Deriving per-species abundance estimates from these sensors requires detection, classification, and quantification of animal vocalizations as individual acoustic events. Yet, variability in ambient noise, both over time and across sensors, hinders the reliability of current automated systems for sound event detection (SED), such as convolutional neural networks (CNN) in the time-frequency domain. In this article, we develop, benchmark, and combine several machine listening techniques to improve the generalizability of SED models across heterogeneous acoustic environments. As a case study, we consider the problem of detecting avian flight calls from a ten-hour recording of nocturnal bird migration, recorded by a network of six ARUs in the presence of heterogeneous background noise. Starting from a CNN yielding state-of-the-art accuracy on this task, we introduce two noise adaptation techniques, respectively integrating short-term (60 ms) and long-term (30 min) context. First, we apply per-channel energy normalization (PCEN) in the time-frequency domain, which applies short-term automatic gain control to every subband in the mel-frequency spectrogram. Secondly, we replace the last dense layer in the network by a context-adaptive neural network (CA-NN) layer, i.e. an affine layer whose weights are dynamically adapted at prediction time by an auxiliary network taking long-term summary statistics of spectrotemporal features as input. We show that PCEN reduces temporal overfitting across dawn vs. dusk audio clips whereas context adaptation on PCEN-based summary statistics reduces spatial overfitting across sensor locations. Moreover, combining them yields state-of-the-art results that are unmatched by artificial data augmentation alone. We release a pre-trained version of our best performing system under the name of BirdVoxDetect, a ready-to-use detector of avian flight calls in field recordings.
AD  - Cornell Univ, Cornell Lab Ornithol, Ithaca, NY 14850 USAAD  - NYU, Mus & Audio Res Lab, New York, NY 10003 USAAD  - NYU, Ctr Urban Sci & Progress, New York, NY 10003 USAC3  - Cornell UniversityC3  - New York UniversityC3  - New York UniversityFU  - National Science Foundation [1633259, 1633206]; Leon Levy Foundation; Google faculty awards
FX  - This research was supported by the National Science Foundation (grants 1633259 to JPB and 1633206 to SK and AF), the Leon Levy Foundation, and Google faculty awards to SK and JPB (https://ai.google/research/outreach/facultyresearch-awards/recipients/).The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.
CR  - Andersen J, 2015, PROC INT CONF INTELL, P1
CR  - Bairlein F, 2016, SCIENCE, V354, P547, DOI 10.1126/science.aah6647
CR  - Battenberg E, 2017, 170504400 ARXIV
CR  - Bauer S, 2017, BIOSCIENCE, V67, P912, DOI 10.1093/biosci/bix074
CR  - Baumgartner MF, 2013, J ACOUST SOC AM, V134, P1814, DOI 10.1121/1.4816406
CR  - Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
CR  - Blair RB, 1996, ECOL APPL, V6, P506, DOI 10.2307/2269387
CR  - Blumstein DT, 2011, J APPL ECOL, V48, P758, DOI 10.1111/j.1365-2664.2011.01993.x
CR  - Brumm H, 2017, METHODS ECOL EVOL, V8, P1617, DOI 10.1111/2041-210X.12766
CR  - Cakir E, 2017, EUR SIGNAL PR CONF, P1744, DOI 10.23919/EUSIPCO.2017.8081508
CR  - Chollet F., 2018, KERAS V2 0 0
CR  - Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
CR  - Delcroix M, 2018, IEEE-ACM T AUDIO SPE, V26, P895, DOI 10.1109/TASLP.2018.2798821
CR  - Delcroix M, 2016, INTERSPEECH, P1573, DOI 10.21437/Interspeech.2016-203
CR  - Delcroix M, 2016, INT CONF ACOUST SPEE, P5270, DOI 10.1109/ICASSP.2016.7472683
CR  - Delcroix M, 2015, INT CONF ACOUST SPEE, P4535, DOI 10.1109/ICASSP.2015.7178829
CR  - Devault TL, 2011, WILDLIFE SOC B, V35, P394, DOI 10.1002/wsb.75
CR  - Dokter AM, 2018, NAT ECOL EVOL, V2, P1603, DOI 10.1038/s41559-018-0666-4
CR  - Drewitt AL, 2006, IBIS, V148, P29, DOI 10.1111/j.1474-919X.2006.00516.x
CR  - Efford MG, 2009, ECOLOGY, V90, P2676, DOI 10.1890/08-1735.1
CR  - Ellis, 2018, COMPUTATIONAL ANAL S, P373, DOI DOI 10.1007/978-3-319-63450-0_13
CR  - Ellis D., 2018, COMPUTATIONAL ANAL S, P13, DOI [10.1007/978-3-319-63450-0_2, DOI 10.1007/978-3-319-63450-0.]
CR  - Evans William R., 2005, Passenger Pigeon, V67, P15
CR  - Farnsworth A, 2005, AUK, V122, P733, DOI 10.1642/0004-8038(2005)122[0733:FCATVF]2.0.CO;2
CR  - Farnsworth A, 2016, ECOL APPL, V26, P752, DOI 10.1890/15-0023
CR  - Farnsworth A, 2014, AI MAG, V35, P31, DOI 10.1609/aimag.v35i2.2527
CR  - Fiedler Wolfgang, 2009, Ringing & Migration, V24, P175
CR  - Fink D, 2014, AI MAG, V35, P19, DOI 10.1609/aimag.v35i2.2533
CR  - Fink D, 2010, ECOL APPL, V20, P2131, DOI 10.1890/09-1340.1
CR  - Franceschi JY, 2018, P INT C ART INT STAT, P1280
CR  - Glotin, 2016, 2016 IEEE 26 INT WOR, P1, DOI [DOI 10.1109/MLSP.2016.7738875, 10.1109/MLSP.2016.7738875]
CR  - Gordo O, 2007, CLIM RES, V35, P37, DOI 10.3354/cr00713
CR  - Grill T, 2017, EUR SIGNAL PR CONF, P1764, DOI 10.23919/EUSIPCO.2017.8081512
CR  - Ha D., 2017, P INT C LEARN REPR, P1
CR  - Hecht J, 2016, IEEE SPECTRUM, P11
CR  - Heinicke S, 2015, METHODS ECOL EVOL, V6, P753, DOI 10.1111/2041-210X.12384
CR  - Hobson KA, 2002, WILDLIFE SOC B, V30, P709
CR  - Hopcroft J. E., 1973, SIAM Journal on Computing, V2, P225, DOI 10.1137/0202019
CR  - Huemmer C, 2017, INT CONF ACOUST SPEE, P4875, DOI 10.1109/ICASSP.2017.7953083
CR  - Jia Xu, 2016, NEURIPS, P667
CR  - Joly Alexis, 2017, Experimental IR Meets Multilinguality, Multimodality, and Interaction. 8th International Conference of the CLEF Association, CLEF 2017. Proceedings: LNCS 10456, P255, DOI 10.1007/978-3-319-65813-1_24
CR  - Kaewtip K, 2016, J ACOUST SOC AM, V140, P3691, DOI 10.1121/1.4966592
CR  - Kahl S, 2018, C LABS EV FOR
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Klapuri A, 1999, INT CONF ACOUST SPEE, P3089, DOI 10.1109/ICASSP.1999.757494
CR  - Knight EC, 2019, BIOACOUSTICS, V28, P539, DOI 10.1080/09524622.2018.1503971
CR  - Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899
CR  - Krstulovic S., 2018, AUDIO EVENT RECOGNIT, P335
CR  - Laiolo P, 2010, BIOL CONSERV, V143, P1635, DOI 10.1016/j.biocon.2010.03.025
CR  - Lanzone M, 2009, AUK, V126, P511, DOI 10.1525/auk.2009.08187
CR  - Li D, 2017, INT SYM COMPUT INTEL, P338, DOI 10.1109/ISCID.2017.51
CR  - Loss SR, 2015, ANNU REV ECOL EVOL S, V46, P99, DOI 10.1146/annurev-ecolsys-112414-054133
CR  - Lostanlen V, 2017, P IEEE C AC SPEECH S, P266
CR  - Lostanlen V, 2019, IEEE SIGNAL PROC LET, V26, P39, DOI 10.1109/LSP.2018.2878620
CR  - Mack C, 2015, IEEE SPECTRUM, V52, P31, DOI 10.1109/MSPEC.2015.7065415
CR  - Marcarini M, 2008, INT CONF ACOUST SPEE, P2029, DOI 10.1109/ICASSP.2008.4518038
CR  - Marques TA, 2013, BIOL REV, V88, P287, DOI 10.1111/brv.12001
CR  - McCallum JC, 2017, GRAPH MEMORY PRICES
CR  - McFee B., 2015, 16 INT SOC MUS INF R, P248
CR  - McFee B, 2019, IEEE SIGNAL PROC MAG, V36, P128, DOI 10.1109/MSP.2018.2875349
CR  - Merchant ND, 2015, METHODS ECOL EVOL, V6, P257, DOI 10.1111/2041-210X.12330
CR  - Millet J, 2019, INT CONF ACOUST SPEE, P5831, DOI 10.1109/ICASSP.2019.8682324
CR  - Mydlarz C, 2017, APPL ACOUST, V117, P207, DOI 10.1016/j.apacoust.2016.06.010
CR  - Naguib M, 2003, J ACOUST SOC AM, V113, P1749, DOI 10.1121/1.1539050
CR  - Nieukirk SL, 2012, J ACOUST SOC AM, V131, P1102, DOI 10.1121/1.3672648
CR  - Oliver RY, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aaq1084
CR  - Pamula H, 2017, POSTEPY AKUSTYKI, P149
CR  - Pellegrini T, 2017, EUR SIGNAL PR CONF, P1734, DOI 10.23919/EUSIPCO.2017.8081506
CR  - Pijanowski BC, 2011, BIOSCIENCE, V61, P203, DOI 10.1525/bio.2011.61.3.6
CR  - Raffel C., 2014, P 15 INT SOC MUS INF
CR  - Ross SRPJ, 2018, ECOL RES, V33, P135, DOI 10.1007/s11284-017-1509-5
CR  - Salamon J, 2017, IEEE WORK APPL SIG, P344, DOI 10.1109/WASPAA.2017.8170052
CR  - Salamon J, 2017, INT CONF ACOUST SPEE, P141, DOI 10.1109/ICASSP.2017.7952134
CR  - Salamon J, 2017, IEEE SIGNAL PROC LET, V24, P279, DOI 10.1109/LSP.2017.2657381
CR  - Salamon J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0166866
CR  - Salamon J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1041, DOI 10.1145/2647868.2655045
CR  - Schluter J, 2018, C LABS EV FOR CLEF
CR  - Schluter J., 2015, P 16 INT SOC MUS INF, P121, DOI 10.5281/zenodo.1417745
CR  - Schluter J, 2018, P C INT SOC MUS INF
CR  - Schwarz A, 2015, INT CONF ACOUST SPEE, P4380, DOI 10.1109/ICASSP.2015.7178798
CR  - Segura-Garcia J, 2015, IEEE SENS J, V15, P836, DOI 10.1109/JSEN.2014.2356342
CR  - Shamoun-Baranes J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0160106
CR  - Shan C, 2018, ARXIV180310916 ARXIV
CR  - SHONFIELD J, 2017, AVIAN CONSERV ECOL, V12, DOI DOI 10.5751/ACE-
CR  - Stewart FEC, 2018, ECOSPHERE, V9, DOI 10.1002/ecs2.2112
CR  - Stowell D, 2018, METHODS ECOLOGY EVOL
CR  - Stowell D., 2018, COMPUTATIONAL ANAL S, P303
CR  - Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
CR  - Sullivan BL, 2014, BIOL CONSERV, V169, P31, DOI 10.1016/j.biocon.2013.11.003
CR  - Ulloa JS, 2018, ECOL INDIC, V90, P346, DOI 10.1016/j.ecolind.2018.03.026
CR  - Van Doren BM, 2018, SCIENCE, V361, P1115, DOI 10.1126/science.aat7526
CR  - Van Doren BM, 2017, P NATL ACAD SCI USA, V114, P11175, DOI 10.1073/pnas.1708574114
CR  - Wang YX, 2017, INT CONF ACOUST SPEE, P5670, DOI 10.1109/ICASSP.2017.7953242
CR  - Warren PS, 2006, ANIM BEHAV, V71, P491, DOI 10.1016/j.anbehav.2005.07.014
CR  - Wilson SJ, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01248-130204
CR  - Yang Zhilin, 2018, P INT C LEARN REPR I
CR  - Zhao ZX, 2014, J PHYS OCEANOGR, V44, P2763, DOI 10.1175/JPO-D-14-0040.1
CR  - Zinemanas P, 2019, PROC CONF OPEN INNOV, P533, DOI 10.23919/FRUCT.2019.8711906
PU  - PUBLIC LIBRARY SCIENCE
PI  - SAN FRANCISCO
PA  - 1160 BATTERY STREET, STE 100, SAN FRANCISCO, CA 94111 USA
DA  - OCT 24
PY  - 2019
VL  - 14
IS  - 10
DO  - 10.1371/journal.pone.0214168
AN  - WOS:000532631800003
N1  - Times Cited in Web of Science Core Collection:  23
Total Times Cited:  23
Cited Reference Count:  98
ER  -

TY  - JOUR
AU  - Kellenberger, B
AU  - Marcos, D
AU  - Lobry, S
AU  - Tuia, D
TI  - Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning
T2  - IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING
LA  - English
KW  - Animals
KW  - Detectors
KW  - Data models
KW  - Unmanned aerial vehicles
KW  - Adaptation models
KW  - Biological system modeling
KW  - Predictive models
KW  - Active Learning (AL)
KW  - animal census
KW  - convolutional neural networks
KW  - domain adaptation
KW  - object detection
KW  - Optimal Transport (OT)
KW  - unmanned aerial vehicles
AB  - We present an Active Learning (AL) strategy for reusing a deep Convolutional Neural Network (CNN)-based object detector on a new data set. This is of particular interest for wildlife conservation: given a set of images acquired with an Unmanned Aerial Vehicle (UAV) and manually labeled ground truth, our goal is to train an animal detector that can be reused for repeated acquisitions, e.g., in follow-up years. Domain shifts between data sets typically prevent such a direct model application. We thus propose to bridge this gap using AL and introduce a new criterion called Transfer Sampling (TS). TS uses Optimal Transport (OT) to find corresponding regions between the source and the target data sets in the space of CNN activations. The CNN scores in the source data set are used to rank the samples according to their likelihood of being animals, and this ranking is transferred to the target data set. Unlike conventional AL criteria that exploit model uncertainty, TS focuses on very confident samples, thus allowing quick retrieval of true positives in the target data set, where positives are typically extremely rare and difficult to find by visual inspection. We extend TS with a new window cropping strategy that further accelerates sample retrieval. Our experiments show that with both strategies combined, less than half a percent of oracle-provided labels are enough to find almost 80% of the animals in challenging sets of UAV images, beating all baselines by a margin.
AD  - Wageningen Univ, Lab GeoInformat Sci & Remote Sensing, NL-6708 PB Wageningen, NetherlandsC3  - Wageningen University & ResearchFU  - Swiss National Science Foundation [PP00P2_150593]
FX  - This work was supported by the Swiss National Science Foundation under Grant PP00P2_150593. (Corresponding author: Devis Tuia.)
CR  - BAYLISS P, 1989, AUST WILDLIFE RES, V16, P651
CR  - Cai WB, 2013, IEEE DATA MINING, P51, DOI 10.1109/ICDM.2013.104
CR  - CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
CR  - Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
CR  - Cuturi M., 2013, NEURIPS, P2292, DOI DOI 10.5555/2999792.2999868
CR  - Gal Y, 2017, PR MACH LEARN RES, V70
CR  - Hodgson AB, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059561, 10.1371/journal.pone.0079556]
CR  - Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
CR  - Johannesson GT, 2015, NEW DIRECT TOUR ANAL, P1
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Kao CC, 2019, LECT NOTES COMPUT SC, V11366, P506, DOI 10.1007/978-3-030-20876-9_32
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Luo T, 2005, J MACH LEARN RES, V6, P589
CR  - Norton-Griffiths M., 1978, SERENGETI ECOLOGICAL, V1
CR  - Ofli F, 2016, BIG DATA-US, V4, P47, DOI 10.1089/big.2014.0064
CR  - Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Santoro Adam, 2016, ARXIV160506065
CR  - Schohn G., 2000, ICML
CR  - Settles Burr, 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI DOI 10.2200/S00429ED1V01Y201207AIM018
CR  - Tuia D, 2016, IEEE GEOSC REM SEN M, V4, P41, DOI 10.1109/MGRS.2016.2548504
CR  - Tuia D, 2013, IEEE T GEOSCI REMOTE, V51, P872, DOI 10.1109/TGRS.2012.2203605
CR  - Tuia D, 2011, IEEE J-STSP, V5, P606, DOI 10.1109/JSTSP.2011.2139193
CR  - Ulyanov D., 2016, ARXIV160708022
CR  - van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
CR  - Yang Z, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115989
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
DA  - DEC
PY  - 2019
VL  - 57
IS  - 12
SP  - 9524
EP  - 9533
DO  - 10.1109/TGRS.2019.2927393
AN  - WOS:000505701800002
N1  - Times Cited in Web of Science Core Collection:  31
Total Times Cited:  33
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Tabak, MA
AU  - Norouzzadeh, MS
AU  - Wolfson, DW
AU  - Newton, EJ
AU  - Boughton, RK
AU  - Ivan, JS
AU  - Odell, EA
AU  - Newkirk, ES
AU  - Conrey, RY
AU  - Stenglein, J
AU  - Iannarilli, F
AU  - Erb, J
AU  - Brook, RK
AU  - Davis, AJ
AU  - Lewis, J
AU  - Walsh, DP
AU  - Beasley, JC
AU  - VerCauteren, KC
AU  - Clune, J
AU  - Miller, RS
TI  - Improving the accessibility and transferability of machine learning algorithms for identification of animals in camera trap images: MLWIC2
T2  - ECOLOGY AND EVOLUTION
LA  - English
KW  - computer vision
KW  - deep convolutional neural networks
KW  - image classification
KW  - machine learning
KW  - motion-activated camera
KW  - R package
KW  - remote sensing
KW  - species identification
AB  - Motion-activated wildlife cameras (or "camera traps") are frequently used to remotely and noninvasively observe animals. The vast number of images collected from camera trap projects has prompted some biologists to employ machine learning algorithms to automatically recognize species in these images, or at least filter-out images that do not contain animals. These approaches are often limited by model transferability, as a model trained to recognize species from one location might not work as well for the same species in different locations. Furthermore, these methods often require advanced computational skills, making them inaccessible to many biologists. We used 3 million camera trap images from 18 studies in 10 states across the United States of America to train two deep neural networks, one that recognizes 58 species, the "species model," and one that determines if an image is empty or if it contains an animal, the "empty-animal model." Our species model and empty-animal model had accuracies of 96.8% and 97.3%, respectively. Furthermore, the models performed well on some out-of-sample datasets, as the species model had 91% accuracy on species from Canada (accuracy range 36%-91% across all out-of-sample datasets) and the empty-animal model achieved an accuracy of 91%-94% on out-of-sample datasets from different continents. Our software addresses some of the limitations of using machine learning to classify images from camera traps. By including many species from several locations, our species model is potentially applicable to many camera trap studies in North America. We also found that our empty-animal model can facilitate removal of images without animals globally. We provide the trained models in an R package (MLWIC2: Machine Learning for Wildlife Image Classification in R), which contains Shiny Applications that allow scientists with minimal programming experience to use trained models and train new models in six neural network architectures with varying depths.
AD  - Quantitat Sci Consulting LLC, Laramie, WY 82072 USAAD  - Univ Wyoming, Dept Zool & Physiol, Laramie, WY 82071 USAAD  - Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USAAD  - Univ Minnesota, Dept Fisheries Wildlife & Conservat Biol, Minnesota Cooperat Fish & Wildlife Res Unit, St Paul, MN 55108 USAAD  - Ontario Minist Nat Resources & Forestry, Wildlife Res & Monitoring Sect, Peterborough, ON, CanadaAD  - Univ Florida, Range Cattle Res & Educ Ctr, Wildlife Ecol & Conservat, Ona, FL USAAD  - Colorado Pk & Wildlife, Ft Collins, CO USAAD  - Wisconsin Dept Nat Resources, Madison, WI USAAD  - Univ Minnesota, Conservat Sci Grad Program, St Paul, MN 55108 USAAD  - Minnesota Dept Nat Resources, Forest Wildlife Populat & Res Grp, Grand Rapids, MN USAAD  - Univ Saskatchewan, Dept Anim & Poultry Sci, Saskatoon, SK, CanadaAD  - USDA, Natl Wildlife Res Ctr, Ft Collins, CO USAAD  - Arizona State Univ, Coll Integrat Sci & Arts, Mesa, AZ USAAD  - US Geol Survey, Natl Wildlife Hlth Ctr, Madison, WI USAAD  - Univ Georgia, Savannah River Ecol Lab, Warnell Sch Forestry & Nat Resources, Aiken, SC USAAD  - US Anim & Plant Hlth Inspect Serv, Natl Wildlife Res Ctr, USDA, Ft Collins, CO USAAD  - OpenAI, San Francisco, CA USAAD  - USDA, Ctr Epidemiol & Anim Hlth, Ft Collins, CO USAC3  - University of WyomingC3  - University of WyomingC3  - University of Minnesota SystemC3  - University of Minnesota Twin CitiesC3  - Ministry of Natural Resources & ForestryC3  - State University System of FloridaC3  - University of FloridaC3  - University of Minnesota SystemC3  - University of Minnesota Twin CitiesC3  - University of SaskatchewanC3  - United States Department of Agriculture (USDA)C3  - Arizona State UniversityC3  - United States Department of the InteriorC3  - United States Geological SurveyC3  - United States Department of Energy (DOE)C3  - Savannah River Ecology LaboratoryC3  - University System of GeorgiaC3  - University of GeorgiaC3  - United States Department of Agriculture (USDA)C3  - United States Department of Agriculture (USDA)FU  - DOE [DE-EM0004391]; USFWS Pittman-Robertson Wildlife Restoration Program; Wisconsin Department of Natural Resources
FX  - Contributions of JCB were partially supported by the DOE under Award Number DE-EM0004391 to the University of Georgia Research Foundation. Support for this research was provided by the USFWS Pittman-Robertson Wildlife Restoration Program and Wisconsin Department of Natural Resources. For supplying camera trap images, we thank USDA Forest Service: Rocky Mountain Research station; Montana Fish, Wildlife and Parks; Wyoming Game and Fish Department; Washington Department of Fish and Wildlife; Idaho Department of Fish and Game; and Woodland Park Zoo.
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Advanced Research Computing Center, 2018, TET COMP ENV INT X86, DOI [10.15786/M2FY47, DOI 10.15786/M2FY47]
CR  - Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
CR  - Beery S., 2020, P IEE CVF C COMP VIS, P13075
CR  - Beery S., 2019, EFFICIENT PIPELINE C
CR  - Beery Sara, 2018, P EUR C COMP VIS ECC, P456
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Guillera-Arroita G, 2017, METHODS ECOL EVOL, V8, P1081, DOI 10.1111/2041-210X.12743
CR  - Harvey Phil, 2016, EXIFTOOL
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - McIntyre T, 2020, WILDLIFE RES, V47, P177, DOI 10.1071/WR19040
CR  - Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
CR  - Norouzzadeh M. S., 2019, ARXIV191009716CSEESS
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
CR  - Royle JA, 2006, ECOLOGY, V87, P835, DOI 10.1890/0012-9658(2006)87[835:GSOMAF]2.0.CO;2
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Tabak M. A., 2020, DRYAD, DOI [10.5061/dryad.x95x69pfx, DOI 10.5061/DRYAD.X95X69PFX]
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Terry JCD, 2020, METHODS ECOL EVOL, V11, P303, DOI 10.1111/2041-210X.13335
CR  - Tobler MW, 2015, J APPL ECOL, V52, P413, DOI 10.1111/1365-2664.12399
CR  - Wei WD, 2020, ECOL INFORM, V55, DOI 10.1016/j.ecoinf.2019.101021
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yousif H., 2019, IEEE T CIRCUITS SYST
CR  - Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - OCT
PY  - 2020
VL  - 10
IS  - 19
SP  - 10374
EP  - 10383
DO  - 10.1002/ece3.6692
AN  - WOS:000569520700001
N1  - Times Cited in Web of Science Core Collection:  8
Total Times Cited:  8
Cited Reference Count:  26
ER  -

TY  - JOUR
AU  - Zhou, ML
AU  - Elmore, JA
AU  - Samiappan, S
AU  - Evans, KO
AU  - Pfeiffer, MB
AU  - Blackwell, BF
AU  - Iglay, RB
TI  - Improving Animal Monitoring Using Small Unmanned Aircraft Systems (sUAS) and Deep Learning Networks
T2  - SENSORS
LA  - English
KW  - drone
KW  - RPA
KW  - UAV
KW  - UVS
KW  - CNN
KW  - ResNet
KW  - machine learning
KW  - WILDLIFE RESEARCH
KW  - AERIAL VEHICLE
KW  - BACKPROPAGATION
KW  - GRADIENT
KW  - IMAGES
KW  - RATES
KW  - UAS
AB  - In recent years, small unmanned aircraft systems (sUAS) have been used widely to monitor animals because of their customizability, ease of operating, ability to access difficult to navigate places, and potential to minimize disturbance to animals. Automatic identification and classification of animals through images acquired using a sUAS may solve critical problems such as monitoring large areas with high vehicle traffic for animals to prevent collisions, such as animal-aircraft collisions on airports. In this research we demonstrate automated identification of four animal species using deep learning animal classification models trained on sUAS collected images. We used a sUAS mounted with visible spectrum cameras to capture 1288 images of four different animal species: cattle (Bos taurus), horses (Equus caballus), Canada Geese (Branta canadensis), and white-tailed deer (Odocoileus virginianus). We chose these animals because they were readily accessible and white-tailed deer and Canada Geese are considered aviation hazards, as well as being easily identifiable within aerial imagery. A four-class classification problem involving these species was developed from the acquired data using deep learning neural networks. We studied the performance of two deep neural network models, convolutional neural networks (CNN) and deep residual networks (ResNet). Results indicate that the ResNet model with 18 layers, ResNet 18, may be an effective algorithm at classifying between animals while using a relatively small number of training samples. The best ResNet architecture produced a 99.18% overall accuracy (OA) in animal identification and a Kappa statistic of 0.98. The highest OA and Kappa produced by CNN were 84.55% and 0.79 respectively. These findings suggest that ResNet is effective at distinguishing among the four species tested and shows promise for classifying larger datasets of more diverse animals.
AD  - Mississippi State Univ, Geosyst Res Inst, Oxford, MS 39762 USAAD  - Mississippi State Univ, Dept Wildlife Fisheries & Aquaculture, Box 9690, Oxford, MS 39762 USAAD  - USDA, Anim & Plant Hlth Inspect Serv, Wildlife Serv, Natl Wildlife Res Ctr,Ohio Field Stn, Sandusky, OH 44870 USAC3  - Mississippi State UniversityC3  - Mississippi State UniversityC3  - United States Department of Agriculture (USDA)FU  - U.S. Department of Agriculture Animal and Plant Health Inspection Service (USDA APHIS) [AP20WSNWRC00C010, AP20WSNWRC00C026, 692M15-19-T-00017, 692M15-19-F-00348]; Forest and Wildlife Research Center and College of Forest Resources at Mississippi State University
FX  - This work was funded by U.S. Department of Agriculture Animal and Plant Health Inspection Service (USDA APHIS; Cooperative Agreements AP20WSNWRC00C010 and AP20WSNWRC00C026) to Mississippi State University (R.B. Iglay, S. Samiappan, K. O. Evans) via Interagency Agreement between USDA APHIS and the Federal Aviation Administration (FAA IA No. 692M15-19-T-00017/Task Order No. 692M15-19-F-00348) for Task Order No. 2, Research Activities on Wildlife Hazards to Aviation. Additional support was provided by the Forest and Wildlife Research Center and College of Forest Resources at Mississippi State University.
CR  - AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
CR  - Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150
CR  - [Anonymous], Torchvision.transforms-Torchvision 0.10.0 documentation
CR  - Attoh-Okine NO, 1999, ADV ENG SOFTW, V30, P291, DOI 10.1016/S0965-9978(98)00071-4
CR  - Bennitt E, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-38610-x
CR  - Blackwell B., AVIAN SURVEY METHODS
CR  - Brownlee J., 2018, DIFFERENCE BATCH EPO
CR  - Buckland ST, 2012, J APPL ECOL, V49, P960, DOI 10.1111/j.1365-2664.2012.02150.x
CR  - Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
CR  - Chabot D, 2015, J UNMANNED VEH SYST, V3, P137, DOI 10.1139/juvs-2015-0021
CR  - Chaganti SY., 2020, 2020 INT C COMP SCI, P1
CR  - Cho K., 2011, P 28 INT C INT C MAC, P105
CR  - Christie KS, 2016, FRONT ECOL ENVIRON, V14, P242, DOI 10.1002/fee.1281
CR  - DeVault T., WILDLIFE AIRPORTS WI
CR  - DeVault TL, 2018, WILDLIFE SOC B, V42, P94, DOI 10.1002/wsb.859
CR  - Dolbeer R.A., WILDLIFE STRIKES CIV
CR  - Frederick P.C., 2003, J FIELD ORNITHOL
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Han XZ, 2020, 2020 5TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (ICCIA 2020), P76, DOI 10.1109/ICCIA49625.2020.00022
CR  - Hayes Madeline C., 2021, Ornithological Applications, V123, pduab022
CR  - Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
CR  - Hodgson JC, 2016, SCI REP-UK, V6, DOI 10.1038/srep22574
CR  - Hong SJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19071651
CR  - Hubbard S, 2018, J UNMANNED VEH SYST, V6, P1, DOI 10.1139/juvs-2016-0020
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Kastner, 2014, 2014 IEEE 11 C MOB, DOI 10.1109/mass.2014.48
CR  - Keshari R, 2018, PROC CVPR IEEE, P9349, DOI 10.1109/CVPR.2018.00974
CR  - Lawrence S, 2000, IEEE IJCNN, P114, DOI 10.1109/IJCNN.2000.857823
CR  - Li Y., ARXIV190704595
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Liu L, 2021, POSTGRAD MED, V133, P265, DOI 10.1080/00325481.2020.1803666
CR  - Liu T., ARXIV150601195
CR  - Lyons MB, 2019, METHODS ECOL EVOL, V10, P1024, DOI 10.1111/2041-210X.13194
CR  - Manohar N, 2016, 2016 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P156, DOI 10.1109/ICACCI.2016.7732040
CR  - McEvoy JF, 2016, PEERJ, V4, DOI 10.7717/peerj.1831
CR  - Mikolajczyk Agnieszka, 2018, 2018 International Interdisciplinary PhD Workshop (IIPhDW), P117, DOI 10.1109/IIPHDW.2018.8388338
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Perez L, 2017, ARXIV171204621
CR  - Pfeiffer MB, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0206599
CR  - Pimm SL, 2015, TRENDS ECOL EVOL, V30, P685, DOI 10.1016/j.tree.2015.08.008
CR  - QUENOUILLE MH, 1956, BIOMETRIKA, V43, P353
CR  - Ratcliffe N, 2015, J UNMANNED VEH SYST, V3, P95, DOI 10.1139/juvs-2015-0006
CR  - Reintsma KM, 2018, WATERBIRDS, V41, P326, DOI 10.1675/063.041.0314
CR  - Rush GP, 2018, ECOL EVOL, V8, P12322, DOI 10.1002/ece3.4495
CR  - Sasse DB, 2003, WILDLIFE SOC B, V31, P1015
CR  - Scholten CN, 2019, BIOL CONSERV, V233, P241, DOI 10.1016/j.biocon.2019.03.001
CR  - Seymour AC, 2017, SCI REP-UK, V7, DOI 10.1038/srep45127
CR  - Steele WK, 2021, WILDLIFE RES, V48, P422, DOI 10.1071/WR20127
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Thanapol Panissara, 2020, 2020 5th International Conference on Information Technology (InCIT), P300, DOI 10.1109/InCIT50588.2020.9310787
CR  - Viera AJ, 2005, FAM MED, V37, P360
CR  - Washburn BE, 2021, WILDLIFE SOC B, V45, P237, DOI 10.1002/wsb.1177
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Wilson DR, 2001, IEEE IJCNN, P115, DOI 10.1109/IJCNN.2001.939002
CR  - Wu Haibing, 2015, MAX POOLING DROPOUT
CR  - Zualkernan IA, 2020, 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT), P111, DOI 10.1109/GCAIOT51063.2020.9345858
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - SEP
PY  - 2021
VL  - 21
IS  - 17
DO  - 10.3390/s21175697
AN  - WOS:000695608700001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  56
ER  -

TY  - JOUR
AU  - Feng, LQ
AU  - Zhao, YQ
AU  - Sun, YC
AU  - Zhao, WX
AU  - Tang, JX
TI  - Action Recognition Using a Spatial-Temporal Network for Wild Felines
T2  - ANIMALS
LA  - English
KW  - wild feline action recognition
KW  - spatial temporal features
KW  - two-stream network
KW  - deep learning
KW  - BEHAVIOR
KW  - PATTERNS
AB  - Simple Summary
   Many wild felines are on the verge of extinction, and the monitoring of wildlife diversity is particularly important. Using surveillance videos of wild felines to monitor their behaviors has an auxiliary effect on the protection of wild felines. Through the actions of wild felines, such as standing, galloping, ambling, etc., their behaviors can be inferred and judged. Therefore, research on the action recognition of wild felines is of great significance to wildlife protection. The currently available methods are all aimed at experimental animals and design-specific feature descriptors for specific animals (such as color, texture, shape, edge, etc.), thus lacking flexibility and versatility. The proposed state-of-the-art algorithm using spatial-temporal networks combines skeleton features with outline features to automatically recognize the actions of wild felines. This model will be suitable for researchers of wild felines.
   Behavior analysis of wild felines has significance for the protection of a grassland ecological environment. Compared with human action recognition, fewer researchers have focused on feline behavior analysis. This paper proposes a novel two-stream architecture that incorporates spatial and temporal networks for wild feline action recognition. The spatial portion outlines the object region extracted by Mask region-based convolutional neural network (R-CNN) and builds a Tiny Visual Geometry Group (VGG) network for static action recognition. Compared with VGG16, the Tiny VGG network can reduce the number of network parameters and avoid overfitting. The temporal part presents a novel skeleton-based action recognition model based on the bending angle fluctuation amplitude of the knee joints in a video clip. Due to its temporal features, the model can effectively distinguish between different upright actions, such as standing, ambling, and galloping, particularly when the felines are occluded by objects such as plants, fallen trees, and so on. The experimental results showed that the proposed two-stream network model can effectively outline the wild feline targets in captured images and can significantly improve the performance of wild feline action recognition due to its spatial and temporal features.
AD  - Nanjing Forestry Univ, Coll Mech & Elect Engn, Nanjing 210037, Peoples R ChinaAD  - Kidswant Children Prod Co Ltd, Nanjing 211135, Peoples R ChinaC3  - Nanjing Forestry UniversityFU  - National Natural Science Fund [31200496]
FX  - This research was funded by the National Natural Science Fund, grant number No. 31200496.
CR  - Agbele T, 2019, PROCEDIA COMPUT SCI, V159, P1375, DOI 10.1016/j.procs.2019.09.308
CR  - Akcay HG, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10071207
CR  - Anderson DJ, 2014, NEURON, V84, P18, DOI 10.1016/j.neuron.2014.09.005
CR  - Atkinson T., 2018, Practical feline behaviour: understanding cat behaviour and improving welfare, P3, DOI 10.1079/9781780647838.0003
CR  - Biolatti C, 2016, APPL ANIM BEHAV SCI, V174, P173, DOI 10.1016/j.applanim.2015.11.017
CR  - Bod'ova K, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0193049
CR  - Bridle J. S., 1990, Neurocomputing, Algorithms, Architectures and Applications. Proceedings of the NATO Advanced Research Workshop, P227
CR  - Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
CR  - Chakravarty P, 2019, MOV ECOL, V7, DOI 10.1186/s40462-019-0172-6
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Cui ZM, 2020, EDUC MEAS-ISSUES PRA, V39, P7, DOI 10.1111/emip.12402
CR  - Dangtongdee K.D., 2018, PLANT IDENTIFICATION
CR  - Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
CR  - Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
CR  - Fraser A. F., 2012, Feline behaviour and welfare, P1, DOI 10.1079/9781845939267.0001
CR  - George G., 2018, INT J ENG SCI RES TE, V7, P548
CR  - Glorot X., 2011, AISTATS, P315, DOI DOI 10.1.1.208.6449
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Graving JM, 2019, ELIFE, V8, DOI 10.7554/eLife.47994
CR  - Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
CR  - Gu JQ, 2017, INT J AGR BIOL ENG, V10, P165, DOI 10.3965/j.ijabe.20171003.3080
CR  - [何东健 He Dongjian], 2016, [农业机械学报, Transactions of the Chinese Society for Agricultural Machinery], V47, P294
CR  - He K., 2015, CVPR
CR  - He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI 10.1109/ICCV.2017.322
CR  - Jaouedi N, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174944
CR  - Jiang ZH, 2017, ICPRAM: PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS, P259, DOI 10.5220/0006244602590269
CR  - Karen S., 2014, P CVPR, P23
CR  - Kingma DP, 2015, COMPUTER SCI, P1
CR  - Lee J, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16050631
CR  - Li, 2018, THESIS INNER MONGOLI
CR  - Li QJ, 2020, INT J REMOTE SENS, V41, P7145, DOI 10.1080/01431161.2020.1754495
CR  - Lin, 2018, P AAAI C ART INT U O, P32
CR  - Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
CR  - Lorbach M, 2019, MULTIMED TOOLS APPL, V78, P19787, DOI 10.1007/s11042-019-7169-4
CR  - Luo YC, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204379
CR  - Malhotra KR, 2018, IEEE COMPUT SOC CONF, P1944, DOI 10.1109/CVPRW.2018.00241
CR  - Marchant-Forde JN, 2015, FRONT VET SCI, V2, DOI 10.3389/fvets.2015.00016
CR  - Mench Joy, 1998, ILAR J, V39, P20
CR  - Nath T, 2019, NAT PROTOC, V14, P2152, DOI 10.1038/s41596-019-0176-0
CR  - Ndako JA, 2020, NEW MICROB NEW INFEC, V38, DOI 10.1016/j.nmni.2020.100795
CR  - Nguyen NG, 2019, PROCEEDINGS OF THE 12TH INTERNATIONAL JOINT CONFERENCE ON BIOMEDICAL ENGINEERING SYSTEMS AND TECHNOLOGIES, VOL 3 (BIOINFORMATICS), P270, DOI 10.5220/0007567602700275
CR  - Noda T, 2014, J EXP MAR BIOL ECOL, V451, P55, DOI 10.1016/j.jembe.2013.10.031
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
CR  - Pawara P., 2016, P 2016 IEEE S SER CO, P1, DOI 10.1109/ SSCI.2016.7850111
CR  - Pereira TD, 2019, NAT METHODS, V16, P117, DOI 10.1038/s41592-018-0234-5
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Romero-Ferrero F, 2019, NAT METHODS, V16, P179, DOI 10.1038/s41592-018-0295-5
CR  - Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
CR  - Shepherdson D.J., 1998, 2 NATURE ENV ENRICHM, P184
CR  - Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - SZEGEDY C, 2016, PROC CVPR IEEE, P2818, DOI DOI 10.1109/CVPR.2016.308
CR  - Tao CB, 2020, DISCRETE DYN NAT SOC, V2020, DOI 10.1155/2020/5916205
CR  - Vaz J, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0174711
CR  - Wark J.D., 2019, ANIM BEHAV COGN, V6, P158
CR  - Williams HJ, 2017, MOV ECOL, V5, DOI 10.1186/s40462-017-0097-x
CR  - Yun K, 2012, P IEEE COMP SOC C CO, P28, DOI DOI 10.1109/CVPRW.2012.6239234
CR  - Zhang H, 2020, J INEQUAL APPL, V2020, DOI 10.1186/s13660-020-02366-0
CR  - Zhang T, 2020, PATTERN RECOGN LETT, V132, P84, DOI 10.1016/j.patrec.2018.11.002
CR  - Zhang YP, 2020, PHYS MED BIOL, V65, DOI 10.1088/1361-6560/aba410
CR  - Zhao FN, 2020, APPL THERM ENG, V180, DOI 10.1016/j.applthermaleng.2020.115810
CR  - Zuffi S, 2017, PROC CVPR IEEE, P5524, DOI 10.1109/CVPR.2017.586
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - FEB
PY  - 2021
VL  - 11
IS  - 2
DO  - 10.3390/ani11020485
AN  - WOS:000622037400001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  66
ER  -

TY  - JOUR
AU  - Fairbrass, AJ
AU  - Firman, M
AU  - Williams, C
AU  - Brostow, GJ
AU  - Titheridge, H
AU  - Jones, KE
TI  - CityNet-Deep learning tools for urban ecoacoustic assessment
T2  - METHODS IN ECOLOGY AND EVOLUTION
LA  - English
KW  - acoustic indices
KW  - anthropogenic
KW  - biodiversity assessment
KW  - convolutional neural networks
KW  - ecoacoustics
KW  - machine learning
KW  - soundscapes
KW  - urban ecology
KW  - ACOUSTIC INDEXES
KW  - BIODIVERSITY
KW  - SOUNDSCAPE
KW  - IDENTIFICATION
KW  - LANDSCAPE
AB  - Cities support unique and valuable ecological communities, but understanding urban wildlife is limited due to the difficulties of assessing biodiversity. Ecoacoustic surveying is a useful way of assessing habitats, where biotic sound measured from audio recordings is used as a proxy for population abundance and/or activity. However, existing algorithms systematically over and underestimate measures of biotic activity in the presence of typical urban non-biotic sounds in recordings. We develop CityNet, a deep learning system using convolutional neural networks (CNNs), to measure audible biotic (CityBioNet) and anthropogenic (CityAnthroNet) acoustic activity in cities. The CNNs were trained on a large dataset of annotated audio recordings collected across Greater London, UK. Using a held-out test dataset, we compare the precision and recall of CityBioNet and CityAnthroNet separately to the best available alternative algorithms: four Acoustic Indices: Acoustic Complexity Index, Acoustic Diversity Index, Bioacoustic Index, and Normalised Difference Soundscape Index, and a state-of-the-art bird call detection CNN (bulbul). We also compare the effect of non-biotic sounds on the predictions of CityBioNet and bulbul. Finally we apply CityNet to describe acoustic patterns of the urban soundscape in two sites along an urbanisation gradient. CityBioNet was the best performing algorithm for measuring biotic activity in terms of precision and recall, followed by bulbul, whereas the Acoustic Indices performed worst. CityAnthroNet outperformed the Normalised Difference Soundscape Index, but by a smaller margin than CityBioNet achieved against the competing algorithms. The CityBioNet predictions were impacted by mechanical sounds, whereas air traffic and wind sounds influenced the bulbul predictions. Across an urbanisation gradient, we show that CityNet produced realistic daily patterns of biotic and anthropogenic acoustic activity from real-world urban audio data. Using CityNet, it is possible to automatically measure biotic and anthropogenic acoustic activity in cities from audio recordings. If embedded within an autonomous sensing system, CityNet could produce environmental data for cites at large-scales and facilitate investigation of the impacts of anthropogenic activities on wildlife. The algorithms, code and pretrained models are made freely available in combination with two expert-annotated urban audio datasets to facilitate automated environmental surveillance in cities.
AD  - UCL, Dept Civil Environm & Geomat Engn, Ctr Urban Sustainabil & Resilience, London, EnglandAD  - UCL, Dept Genet Evolut & Environm, Ctr Biodivers & Environm Res, London, EnglandAD  - Bat Conservat Trust, London, EnglandAD  - UCL, Dept Comp Sci, London, EnglandAD  - Zool Soc London, Inst Zool, London, EnglandC3  - University of LondonC3  - University College LondonC3  - University of LondonC3  - University College LondonC3  - University of LondonC3  - University College LondonC3  - Zoological Society of LondonFU  - Engineering and Physical Sciences Research Council [EP/G037698/1, EP/K015664/1]; EPSRC [EP/K015664/2, EP/K015664/1] Funding Source: UKRI; NERC [NE/P019013/1] Funding Source: UKRI
FX  - Engineering and Physical Sciences Research Council, Grant/Award Number: EP/G037698/1 and EP/K015664/1
CR  - Acevedo MA, 2009, ECOL INFORM, V4, P206, DOI 10.1016/j.ecoinf.2009.06.005
CR  - Aide TM, 2013, PEERJ, V1, DOI 10.7717/peerj.103
CR  - Al-Rfou R., 2016, THEANO PYTHON FRAMEW
CR  - Aronson MFJ, 2014, P ROY SOC B-BIOL SCI, V281, DOI 10.1098/rspb.2013.3330
CR  - Beninde J, 2015, ECOL LETT, V18, P581, DOI 10.1111/ele.12427
CR  - Boelman NT, 2007, ECOL APPL, V17, P2137, DOI 10.1890/07-0004.1
CR  - Chesmore ED, 2004, B ENTOMOL RES, V94, P319, DOI 10.1079/BER2004306
CR  - Crouse DL, 2017, LANCET PLANET HEALTH, V1, pE289, DOI 10.1016/S2542-5196(17)30118-3
CR  - Darras K, 2016, BIOL CONSERV, V201, P29, DOI 10.1016/j.biocon.2016.06.021
CR  - Deichmann JL, 2017, ECOL INDIC, V74, P39, DOI 10.1016/j.ecolind.2016.11.002
CR  - Depraetere M, 2012, ECOL INDIC, V13, P46, DOI 10.1016/j.ecolind.2011.05.006
CR  - Dieleman S., 2015, LASAGNE, DOI [10.5281/zenodo.27878, DOI 10.5281/ZENODO.27878]
CR  - Digby A, 2013, METHODS ECOL EVOL, V4, P675, DOI 10.1111/2041-210X.12060
CR  - Faeth SH, 2011, ANN NY ACAD SCI, V1223, P69, DOI 10.1111/j.1749-6632.2010.05925.x
CR  - Fairbrass AJ, 2017, ECOL INDIC, V83, P169, DOI 10.1016/j.ecolind.2017.07.064
CR  - Farinha-Marques P, 2011, INNOVATION-ABINGDON, V24, P247, DOI 10.1080/13511610.2011.592062
CR  - Fuller S, 2015, ECOL INDIC, V58, P207, DOI 10.1016/j.ecolind.2015.05.057
CR  - Gasc A, 2015, BIOL CONSERV, V191, P306, DOI 10.1016/j.biocon.2015.06.018
CR  - Gil D, 2014, AVIAN URBAN ECOLOGY: BEHAVIOURAL AND PHYSIOLOGICAL ADAPTATIONS, P69
CR  - Glotin, 2016, 2016 IEEE 26 INT WOR, P1, DOI [DOI 10.1109/MLSP.2016.7738875, 10.1109/MLSP.2016.7738875]
CR  - Grill T., 2017, 25 EUR SIGN PROC C E, DOI [10.23919/EUSIPCO.2017.8081512, DOI 10.23919/EUSIPCO.2017.8081512]
CR  - Hall DM, 2017, CONSERV BIOL, V31, P24, DOI 10.1111/cobi.12840
CR  - Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
CR  - Kasten EP, 2012, ECOL INFORM, V12, P50, DOI 10.1016/j.ecoinf.2012.08.001
CR  - Kohsaka R., 2013, URBANIZATION BIODIVE, P699, DOI [10.1007/978-94-007-7088-1_32, DOI 10.1007/978-94-007-7088-1]
CR  - Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lin TH, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04790-7
CR  - McFee B., 2015, P 14 PYTH SCI C, P18, DOI DOI 10.25080/MAJORA-7B98E3ED-003
CR  - Mesaros Annamaria, 2017, DCASE 2017 WORKSH DE
CR  - Natural England, 2016, LINKS NAT ENV MENT H
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Pieretti N, 2011, ECOL INDIC, V11, P868, DOI 10.1016/j.ecolind.2010.11.005
CR  - Pieretti N, 2013, J ACOUST SOC AM, V134, P891, DOI 10.1121/1.4807812
CR  - Pijanowski BC, 2011, BIOSCIENCE, V61, P203, DOI 10.1525/bio.2011.61.3.6
CR  - Python Software Foundation, 2016, PYTH LANG REF
CR  - R Core Team, 2017, R LANG ENV STAT COMP
CR  - Salamon J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1041, DOI 10.1145/2647868.2655045
CR  - Salamon Justin, 2017, 2017 IEEE WORKSH APP
CR  - Statistical CJ., 1992, CURR DIR PSYCHOL SCI, V1, P98, DOI [10.1111/1467-8721.ep10768783, DOI 10.1111/1467-8721.EP10768783]
CR  - Stowell D, 2014, PEERJ, V2, DOI 10.7717/peerj.488
CR  - Sueur J, 2015, BIOSEMIOTICS-NETH, V8, P493, DOI 10.1007/s12304-015-9248-x
CR  - Sueur J, 2014, ACTA ACUST UNITED AC, V100, P772, DOI 10.3813/AAA.918757
CR  - Sueur J, 2008, BIOACOUSTICS, V18, P213, DOI 10.1080/09524622.2008.9753600
CR  - Szewczak J. M., 2010, SONOBAT
CR  - Towsey M, 2014, ECOL INFORM, V21, P110, DOI 10.1016/j.ecoinf.2013.11.007
CR  - UN DESA, 2016, WORLDS CIT 2016 DAT
CR  - Villanueva-Rivera L. J., 2014, PACKAGE SOUNDECOLOGY
CR  - Villanueva-Rivera LJ, 2011, LANDSCAPE ECOL, V26, P1233, DOI 10.1007/s10980-011-9636-9
CR  - Walters Charlotte L., 2013, P479
CR  - Walters CL, 2012, J APPL ECOL, V49, P1064, DOI 10.1111/j.1365-2664.2012.02182.x
CR  - Wang Y.-X., 2018, ARXIV180105401
CR  - Wildlife Acoustics, 2017, KAL AN SOFTW
CR  - Zamora-Gutierrez V, 2016, METHODS ECOL EVOL, V7, P1082, DOI 10.1111/2041-210X.12556
CR  - Zanella A, 2014, IEEE INTERNET THINGS, V1, P22, DOI 10.1109/JIOT.2014.2306328
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - FEB
PY  - 2019
VL  - 10
IS  - 2
SP  - 186
EP  - 197
DO  - 10.1111/2041-210X.13114
AN  - WOS:000459020800003
N1  - Times Cited in Web of Science Core Collection:  19
Total Times Cited:  20
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Torney, CJ
AU  - Lloyd-Jones, DJ
AU  - Chevallier, M
AU  - Moyer, DC
AU  - Maliti, HT
AU  - Mwita, M
AU  - Kohi, EM
AU  - Hopcraft, GC
TI  - A comparison of deep learning and citizen science techniques for counting wildlife in aerial survey images
T2  - METHODS IN ECOLOGY AND EVOLUTION
LA  - English
KW  - citizen science
KW  - conservation
KW  - deep learning
KW  - monitoring
KW  - population ecology
KW  - surveys
KW  - SERENGETI WILDEBEEST
KW  - TRACKING
AB  - Fast and accurate estimates of wildlife abundance are an essential component of efforts to conserve ecosystems in the face of rapid environmental change. A widely used method for estimating species abundance involves flying aerial transects, taking photographs, counting animals within the images and then inferring total population size based on a statistical estimate of species density in the region. The intermediate task of manually counting the aerial images is highly labour intensive and is often the limiting step in making a population estimate. Here, we assess the use of two novel approaches to perform this task by deploying both citizen scientists and deep learning to count aerial images of the 2015 survey of wildebeest (Connochaetes taurinus) in Serengeti National Park, Tanzania. Through the use of the online platform Zooniverse, we collected multiple non-expert counts by citizen scientists and used three different aggregation methods to obtain a single count for the survey images. We also counted the images by developing a bespoke deep learning method via the use of a convolutional neural network. The results of both approaches were then compared. After filtering of the citizen science counts, both approaches provided highly accurate total estimates. The deep learning method was far faster and appears to be a more reliable and predictable approach; however, we note that citizen science volunteers played an important role when creating training data for the algorithm. Notably, our results show that accurate, species-specific, automated counting of aerial wildlife images is now possible.
AD  - Univ Glasgow, Sch Math & Stat, Glasgow, Lanark, ScotlandAD  - Univ Cape Town, DST NRF Ctr Excellence, FitzPatrick Inst African Ornithol, Rondebosch, South AfricaAD  - Field Museum Nat Hist, Integrated Res Ctr, Chicago, IL 60605 USAAD  - Tanzania Wildlife Res Inst, Arusha, TanzaniaAD  - Univ Glasgow, Inst Biodivers Anim Hlth & Comparat Med, Glasgow, Lanark, ScotlandC3  - University of GlasgowC3  - National Research Foundation - South AfricaC3  - University of Cape TownC3  - Field Museum of Natural History (Chicago)C3  - University of GlasgowFU  - European Union Horizon 2020 [641918]; James S. McDonnell Foundation
FX  - European Union Horizon 2020, Grant/Award Number: 641918; James S. McDonnell Foundation
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - BAJZAK D, 1990, WILDLIFE SOC B, V18, P125
CR  - Bruijning M, 2018, METHODS ECOL EVOL, V9, P965, DOI 10.1111/2041-210X.12975
CR  - Campbell K., 1995, P 11 AAVD ACVD M, P117
CR  - Chabot D, 2018, AVIAN CONSERV ECOL, V13, DOI 10.5751/ACE-01205-130115
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Chollet F., 2015, KERAS
CR  - Christin S., 2018, APPL DEEP LEARNING E
CR  - Condorcet M., 1976, ESSAY APPL MATH THEO
CR  - Dell AI, 2014, TRENDS ECOL EVOL, V29, P417, DOI 10.1016/j.tree.2014.05.004
CR  - Dill KA, 2012, SCIENCE, V338, P1042, DOI 10.1126/science.1219021
CR  - Dobson A, 2010, NATURE, V467, P272, DOI 10.1038/467272a
CR  - Ellwood ER, 2017, BIOL CONSERV, V208, P1, DOI 10.1016/j.biocon.2016.10.014
CR  - Estes R. D., 2014, GNUS WORLD SERENGETI
CR  - Forrester TD, 2017, BIOL CONSERV, V208, P98, DOI 10.1016/j.biocon.2016.06.025
CR  - Fryxell JM., 2015, SERENGETI 4 SUSTAINI, DOI [https://doi.org/10.7208/chicago/9780226196336.001.0001, DOI 10.7208/CHICAGO/9780226196336.001.0001]
CR  - Galton F, 1907, NATURE, V75, P450, DOI 10.1038/075450a0
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Harris Grant, 2009, Endangered Species Research, V7, P55, DOI 10.3354/esr00173
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Holdo RM, 2011, ANIMAL MIGRATION: A SYNTHESIS, P131
CR  - Holdo RM, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0016370
CR  - JOLLY G M, 1969, East African Agricultural and Forestry Journal, V34, P46
CR  - Kao AB, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2018.0130
CR  - Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
CR  - Laliberte AS, 2003, WILDLIFE SOC B, V31, P362
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Lintott CJ, 2008, MON NOT R ASTRON SOC, V389, P1179, DOI 10.1111/j.1365-2966.2008.13689.x
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Mac Aodha O, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005995
CR  - Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
CR  - MCNAUGHTON SJ, 1985, ECOL MONOGR, V55, P259, DOI 10.2307/1942578
CR  - McNeill S, 2011, INT GEOSCI REMOTE SE, P4312, DOI 10.1109/IGARSS.2011.6050185
CR  - Mduma SAR, 1999, J ANIM ECOL, V68, P1101, DOI 10.1046/j.1365-2656.1999.00352.x
CR  - MYERS JP, 1987, AM SCI, V75, P19
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - NORTON-GRIFFITHS M, 1973, East African Wildlife Journal, V11, P135
CR  - Redmon J., 2018, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.690
CR  - Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
CR  - Sauermann H, 2015, P NATL ACAD SCI USA, V112, P679, DOI 10.1073/pnas.1408907112
CR  - Schneider S., 2018, ARXIV180310842
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
CR  - Singh NJ, 2011, J APPL ECOL, V48, P35, DOI 10.1111/j.1365-2664.2010.01905.x
CR  - Subalusky AL, 2017, P NATL ACAD SCI USA, V114, P7647, DOI 10.1073/pnas.1614778114
CR  - Surowiecki J., 2005, WISDOM CROWDS
CR  - Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
CR  - Thirgood S, 2004, ANIM CONSERV, V7, P113, DOI 10.1017/S1367943004001404
CR  - Torney CJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0156342
CR  - Turchin P, 1999, OIKOS, V84, P153, DOI 10.2307/3546876
CR  - Valletta JJ, 2017, ANIM BEHAV, V124, P203, DOI 10.1016/j.anbehav.2016.12.005
CR  - Walters CJ., 1986, ADAPTIVE MANAGEMENT
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Wilcove DS, 2008, PLOS BIOL, V6, P1361, DOI 10.1371/journal.pbio.0060188
CR  - Xue YF, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9090878
CR  - Yang Z, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115989
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - JUN
PY  - 2019
VL  - 10
IS  - 6
SP  - 779
EP  - 787
DO  - 10.1111/2041-210X.13165
AN  - WOS:000470017200004
N1  - Times Cited in Web of Science Core Collection:  29
Total Times Cited:  29
Cited Reference Count:  60
ER  -

TY  - JOUR
AU  - Eikelboom, JAJ
AU  - Wind, J
AU  - van de Ven, E
AU  - Kenana, LM
AU  - Schroder, B
AU  - de Knegt, HJ
AU  - van Langevelde, F
AU  - Prins, HHT
TI  - Improving the precision and accuracy of animal population estimates with aerial image object detection
T2  - METHODS IN ECOLOGY AND EVOLUTION
LA  - English
KW  - computer vision
KW  - convolutional neural network
KW  - deep machine learning
KW  - drones
KW  - game census
KW  - image recognition
KW  - savanna
KW  - wildlife survey
KW  - AIRCRAFT
AB  - Animal population sizes are often estimated using aerial sample counts by human observers, both for wildlife and livestock. The associated methods of counting remained more or less the same since the 1970s, but suffer from low precision and low accuracy of population estimates. Aerial counts using cost-efficient Unmanned Aerial Vehicles or microlight aircrafts with cameras and an automated animal detection algorithm can potentially improve this precision and accuracy. Therefore, we evaluated the performance of the multi-class convolutional neural network RetinaNet in detecting elephants, giraffes and zebras in aerial images from two Kenyan animal counts. The algorithm detected 95% of the number of elephants, 91% of giraffes and 90% of zebras that were found by four layers of human annotation, of which it correctly detected an extra 2.8% of elephants, 3.8% giraffes and 4.0% zebras that were missed by all humans, while detecting only 1.6 to 5.0 false positives per true positive. Furthermore, the animal detections by the algorithm were less sensitive to the sighting distance than humans were. With such a high recall and precision, we posit it is feasible to replace manual aerial animal count methods (from images and/or directly) by only the manual identification of image bounding boxes selected by the algorithm and then use a correction factor equal to the inverse of the undercounting bias in the calculation of the population estimates. This correction factor causes the standard error of the population estimate to increase slightly compared to a manual method, but this increase can be compensated for when the sampling effort would increase by 23%. However, an increase in sampling effort of 160% to 1,050% can be attained with the same expenses for equipment and personnel using our proposed semi-automatic method compared to a manual method. Therefore, we conclude that our proposed aerial count method will improve the accuracy of population estimates and will decrease the standard error of population estimates by 31% to 67%. Most importantly, this animal detection algorithm has the potential to outperform humans in detecting animals from the air when supplied with images taken at a fixed rate.
AD  - Wageningen Univ & Res, Resource Ecol Grp, Wageningen, NetherlandsAD  - Jheronimus Acad Data Sci, Shertogenbosch, NetherlandsAD  - Kenya Wildlife Serv, Nairobi, KenyaAD  - Welgevonden Game Reserve, Vaalwater, South AfricaAD  - Univ KwaZulu Natal, Sch Life Sci, Westville Campus, Durban, South AfricaC3  - Wageningen University & ResearchC3  - University of Kwazulu NatalFU  - Nederlandse Organisatie voor Wetenschappelijk Onderzoek
FX  - Nederlandse Organisatie voor Wetenschappelijk Onderzoek, Grant/Award Number: Advanced Instrumentation for Wildlife Protection
CR  - Amazon Web Services, 2018, AM EC2 P3 INST
CR  - Andrew W, 2017, IEEE INT CONF COMP V, P2850, DOI 10.1109/ICCVW.2017.336
CR  - Bishir John W., 1996, P177
CR  - Buckland S.T., 2004, ADV DISTANCE SAMPLIN
CR  - CAUGHLEY G, 1976, J WILDLIFE MANAGE, V40, P290, DOI 10.2307/3800428
CR  - Chamoso P., 2014, AMBIENT INTELLIGENCE, P71, DOI DOI 10.1007/978-3-319-07596-9_8
CR  - Christie KS, 2016, FRONT ECOL ENVIRON, V14, P242, DOI 10.1002/fee.1281
CR  - Colefax AP, 2018, ICES J MAR SCI, V75, P1, DOI 10.1093/icesjms/fsx100
CR  - COOK RD, 1979, BIOMETRICS, V35, P735, DOI 10.2307/2530104
CR  - DAVIS DE, 1980, WILDLIFE MANAGEMENT, P221
CR  - De Bie S. D., 1983, AERIAL RESOURCE INVE
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - DHV Consulting Engineers, 1980, COUNTR AN RANG ASS P
CR  - Dunn WC, 2002, WILDLIFE SOC B, V30, P963
CR  - Eikelboom J. A. J., 2019, IMPROVING PRECISION, DOI [10.4121/uuid:ba99a206-3e5a-4673-b830-b5c866445b8c, DOI 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Fizyr, 2018, KER RETINANET 0 5 0
CR  - Fleming PJS, 2008, WILDLIFE RES, V35, P258, DOI 10.1071/WR07081
CR  - Hearne JW, 2000, ANN OPER RES, V95, P269, DOI 10.1023/A:1018950007473
CR  - Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
CR  - Jachmann H, 2002, J APPL ECOL, V39, P841, DOI 10.1046/j.1365-2664.2002.00752.x
CR  - Jachmann H., 2001, ESTIMATING ABUNDANCE
CR  - JOLLY G M, 1969, East African Agricultural and Forestry Journal, V34, P46
CR  - JOLLY G M, 1969, East African Agricultural and Forestry Journal, V34, P50
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - LERESCHE RE, 1974, J WILDLIFE MANAGE, V38, P175, DOI 10.2307/3800722
CR  - Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
CR  - Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Mwakiwa E, 2016, ENVIRON CONSERV, V43, P128, DOI 10.1017/S0376892915000296
CR  - Nichols JD, 2000, AUK, V117, P393, DOI 10.1642/0004-8038(2000)117[0393:ADOAFE]2.0.CO;2
CR  - Norton-Griffiths M, 1978, HDB AFRICAN WILDLIFE, V1
CR  - Rabe MJ, 2002, WILDLIFE SOC B, V30, P46
CR  - Redfern JV, 2002, S AFR J SCI, V98, P455
CR  - Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
CR  - Sirmacek B., 2012, P INT C ENV MOD SOFT, P1
CR  - STOTT RS, 1972, J WILDLIFE MANAGE, V36, P468, DOI 10.2307/3799077
CR  - van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
CR  - Van Lavieren L. P., 1982, INTRO TAKING FIELN 1
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - NOV
PY  - 2019
VL  - 10
IS  - 11
SP  - 1875
EP  - 1887
DO  - 10.1111/2041-210X.13277
AN  - WOS:000488346300001
N1  - Times Cited in Web of Science Core Collection:  32
Total Times Cited:  32
Cited Reference Count:  39
ER  -

TY  - JOUR
AU  - Patel, A
AU  - Cheung, L
AU  - Khatod, N
AU  - Matijosaitiene, I
AU  - Arteaga, A
AU  - Gilkey, JW
TI  - Revealing the Unknown: Real-Time Recognition of Galapagos Snake Species Using Deep Learning
T2  - ANIMALS
LA  - English
KW  - artificial intelligence (AI) platform
KW  - deep learning
KW  - Galapagos Islands
KW  - image classification
KW  - machine learning
KW  - Pseudalsophis
KW  - racer snake
KW  - region-based convolutional neural network (R-CNN)
KW  - snake species
AB  - Simple Summary The snakes in Galapagos are the least studied group of vertebrates in the archipelago. The conservation status of only four out of nine recognized species has been formally evaluated, and preliminary evidence suggests that some of the species may be entirely extinct on some islands. Moreover, nearly all park ranger reports and citizen/science photographic identifications of Galapagos snakes are spurious, given that the systematics of the snakes in the archipelago have just recently been clarified. Our solution is to provide park rangers and tourists with easily accessible applications for species identification in real time through automatic object recognition. We used deep learning algorithms on collected images of the snake species to develop the artificial intelligence platform, an application software, that is able to recognize a species of a snake using a user's uploaded image. The application software works in the following way: once a user uploads an image of a snake into the application, the algorithm processes it, classifies it into one of the nine snake species, gives the class of the predicted species, as well as educates users by providing them with information about the distribution, natural history, conservation, and etymology of the snake.
   Abstract Real-time identification of wildlife is an upcoming and promising tool for the preservation of wildlife. In this research project, we aimed to use object detection and image classification for the racer snakes of the Galapagos Islands, Ecuador. The final target of this project was to build an artificial intelligence (AI) platform, in terms of a web or mobile application, which would serve as a real-time decision making and supporting mechanism for the visitors and park rangers of the Galapagos Islands, to correctly identify a snake species from the user's uploaded image. Using the deep learning and machine learning algorithms and libraries, we modified and successfully implemented four region-based convolutional neural network (R-CNN) architectures (models for image classification): Inception V2, ResNet, MobileNet, and VGG16. Inception V2, ResNet and VGG16 reached an overall accuracy of 75%.
AD  - St Peters Univ, Data Sci Inst, Jersey City, NJ 07306 USAAD  - Kaunas Univ Technol, Inst Environm Engn, LT-44249 Kaunas, LithuaniaAD  - Vilnius Gediminas Tech Univ, Dept Informat Technol, LT-10223 Vilnius, LithuaniaAD  - Trop Herping, Quito 170150, EcuadorC3  - Kaunas University of TechnologyC3  - Vilnius Gediminas Technical UniversityCR  - Amir A, 2017, ADV INTELL SYST, V532, P52, DOI 10.1007/978-3-319-48517-1_5
CR  - [Anonymous], 2018, RESNET ALEXNET VGGNE
CR  - Arteaga A., 2019, REPTILES GALAPAGOS, P208
CR  - Chen X., 2017, IMPLEMENTATION FASTE
CR  - Cisneros-Heredia D.F., 2017, IUCN RED LIST THREAT, DOI 10.2305/IUCN.UK.2017-2.RLTS.T190539A54447664.en
CR  - Dangtongdee K.D., 2018, PLANT IDENTIFICATION
CR  - Fang W., 2019, IEEE ACCESS, V7
CR  - Fang W, 2019, NEUROCOMPUTING, V361, P85, DOI 10.1016/j.neucom.2019.05.095
CR  - Gao H., 2017, FASTER R CNN EXPLAIN
CR  - Girshick R., 2015, ICCV, P1440, DOI DOI 10.1109/ICCV.2015.169
CR  - Goswami S., 2018, DEEPER LOOK FASTER R
CR  - Hernandez-Serna A, 2014, PEERJ, V2, DOI 10.7717/peerj.563
CR  - Hollemans M, 2018, MOBILENET VERSION 2
CR  - Jegou S, 2017, 100 LAYERS TIRAMISU
CR  - Jordan J., 2018, OVERVIEW OBJECT DETE
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
CR  - Mangersnes R., GALAPAGOS RACER SNAK
CR  - Marquez C., 2017, IUCN RED LIST THREAT, DOI [10.2305/IUCN.UK.2017-2.RLTS.T190541A56253872.en, DOI 10.2305/IUCN.UK.2017-2.RLTS.T190541A56253872.EN]
CR  - Marquez C., 2016, IUCN RED LIST THREAT, DOI 10.2305/IUCN.UK.2016-1.RLTS.T190540A56267613.en
CR  - Mejia D., 2017, MONGABAY
CR  - Murphy J., 2016, OVERVIEW CONVOLUTION
CR  - Nigam I., 2016, EXPLORING STRUCTURE
CR  - Raj B., 2018, SIMPLE GUIDE VERSION
CR  - Redmon Joseph, 2016, YOU ONLY LOOK ONCE U
CR  - Ren S., 2016, FASTER R CNN REALTIM
CR  - Schramer T., 2016, GALAPAGOS RACER
CR  - Sergyan, 2007, P 5 SLOV HUNG JOINT
CR  - Simonyan K., 2015, P INT C LEARN REPR I
CR  - Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Zaher H, 2018, SYST BIODIVERS, V16, P614, DOI 10.1080/14772000.2018.1478910
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - MAY
PY  - 2020
VL  - 10
IS  - 5
DO  - 10.3390/ani10050806
AN  - WOS:000540228300057
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  33
ER  -

TY  - CPAPER
AU  - Favorskaya, M
AU  - Pakhirka, A
ED  - Rudas, IJ
ED  - Janos, C
ED  - Toro, C
ED  - Botzheim, J
ED  - Howlett, RJ
ED  - Jain, LC
TI  - Animal species recognition in the wildlife based on muzzle and shape features using joint CNN
T2  - KNOWLEDGE-BASED AND INTELLIGENT INFORMATION & ENGINEERING SYSTEMS (KES 2019)
LA  - English
CP  - 23rd KES International Conference on Knowledge-Based and Intelligent Information and Engineering Systems (KES)
KW  - Animal recognition
KW  - shape features
KW  - muzzle features;deep learning
KW  - NETWORK
AB  - Monitoring of animal behavior in the wild supposes the reliable techniques for their species recognition using, mainly, visual data captured by camera traps. In this paper, we propose to extent Convolutional Neural Network (CNN) VGG by three branches, two of which are VGG16 for the muzzle and part of shape recognition and one is VGG19 for the whole shape recognition. A necessity of such branched CNN structure is caused by great variety of the animal poses fixed by a camera trap. Also, here we met with an objective problem of the unbalanced dataset due to different behavior of animals in nature. Preliminary categorization procedure of images helps to obtain better recognition results. Experiments were conducted using the dataset obtained from Ergaki national park, Krasnoyarsky Kray, Russia, 2012-2018. The joint CNN shows good accuracy results on the balanced dataset achieving 80.6% Top-1 and 94.1% Top-5, respectively. In the case of the unbalanced training dataset, we obtained 38.7% Top-1 and 54.8% Top-5 accuracy. (C) 2019 The Authors. Published by Elsevier B.V.
AD  - Reshetnev Siberian State Univ Sci & Technol, 31 Krasnoyarsky Rabochy Ave, Krasnoyarsk 660037, RussiaC3  - Reshetnev Siberian State University of Science & TechnologyFU  - Russian Foundation for Basic Research, Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of Science [18-47-240001]
FX  - The reported study was funded by Russian Foundation for Basic Research, Government of Krasnoyarsk Territory, Krasnoyarsk Regional Fund of Science to the research project No. 18-47-240001.
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Diaz-Pulido, 2017, VISUAL COMPUT, P1
CR  - Duyck J, 2015, PATTERN RECOGN, V48, P1059, DOI 10.1016/j.patcog.2014.07.017
CR  - [Фаворская М.Н. Favorskaya M.N.], 2018, [Информационно-управляющие системы, Informatsionno-upravlyayushchie sistemy], P35, DOI 10.31799/1684-8853-2018-6-35-45
CR  - Favorskaya Margarita N., 2019, P 11 KES INT C INT D
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
CR  - Huang G, 2018, ANN OPER RES, P1, DOI DOI 10.1007/S10479-018-2973-1
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Li S, 2018, PATTERN RECOGN, V81, P294, DOI 10.1016/j.patcog.2018.03.035
CR  - Lin MH, 2014, ASIA-PAC EDUC RES, V23, P577, DOI 10.1007/s40299-013-0131-8
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - TensorFlow, IMAGE RECOGNITION
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zotin Alexander, 2019, 3 INT ISPRS WORKSH P
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - Radarweg 29, PO Box 211, AMSTERDAM, NETHERLANDS
PY  - 2019
VL  - 159
SP  - 933
EP  - 942
DO  - 10.1016/j.procs.2019.09.260
AN  - WOS:000571151500096
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  18
ER  -

TY  - JOUR
AU  - Madhusudhana, S
AU  - Shiu, Y
AU  - Klinck, H
AU  - Fleishman, E
AU  - Liu, XB
AU  - Nosal, EM
AU  - Helble, T
AU  - Cholewiak, D
AU  - Gillespie, D
AU  - Sirovic, A
AU  - Roch, MA
TI  - Improve automatic detection of animal call sequences with temporal context
T2  - JOURNAL OF THE ROYAL SOCIETY INTERFACE
LA  - English
KW  - passive acoustic monitoring
KW  - bioacoustics
KW  - machine learning
KW  - temporal context
KW  - robust automatic recognition
KW  - improved performance
KW  - WHALE ACOUSTIC PRESENCE
KW  - FIN WHALES
KW  - BALAENOPTERA-PHYSALUS
KW  - CLASSIFICATION
KW  - DEEP
KW  - BLUE
KW  - SONG
KW  - VOCALIZATIONS
KW  - CALIFORNIA
KW  - SOUNDS
AB  - Many animals rely on long-form communication, in the form of songs, for vital functions such as mate attraction and territorial defence. We explored the prospect of improving automatic recognition performance by using the temporal context inherent in song. The ability to accurately detect sequences of calls has implications for conservation and biological studies. We show that the performance of a convolutional neural network (CNN), designed to detect song notes (calls) in short-duration audio segments, can be improved by combining it with a recurrent network designed to process sequences of learned representations from the CNN on a longer time scale. The combined system of independently trained CNN and long short-term memory (LSTM) network models exploits the temporal patterns between song notes. We demonstrate the technique using recordings of fin whale (Balaenoptera physalus) songs, which comprise patterned sequences of characteristic notes. We evaluated several variants of the CNN + LSTM network. Relative to the baseline CNN model, the CNN + LSTM models reduced performance variance, offering a 9-17% increase in area under the precision-recall curve and a 9-18% increase in peak F1-scores. These results show that the inclusion of temporal information may offer a valuable pathway for improving the automatic recognition and transcription of wildlife recordings.
AD  - Cornell Univ, Cornell Lab Ornithol, K Lisa Yang Ctr Conservat Bioacoust, Ithaca, NY 14850 USAAD  - Oregon State Univ, Dept Fisheries Wildlife & Conservat Sci, Marine Mammal Inst, Corvallis, OR 97331 USAAD  - Oregon State Univ, Coll Earth Ocean & Atmospher Sci, Corvallis, OR 97331 USAAD  - San Diego State Univ, Dept Comp Sci, San Diego, CA 92182 USAAD  - Univ Hawaii Manoa, Dept Ocean & Resources Engn, Honolulu, HI 96822 USAAD  - US Navy, Naval Informat Warfare Ctr Pacific, San Diego, CA 92152 USAAD  - Natl Marine Fisheries Serv, Northeast Fisheries Sci Ctr, Natl Ocean & Atmospher Adm, Woods Hole, MA USAAD  - Univ St Andrews, Scottish Oceans Inst, Sea Mammal Res Unit, St Andrews, Fife, ScotlandAD  - Texas A&M Univ, Marine Biol Dept, Galveston, TX USAC3  - Cornell UniversityC3  - Oregon State UniversityC3  - Oregon State UniversityC3  - California State University SystemC3  - San Diego State UniversityC3  - University of Hawaii SystemC3  - University of Hawaii ManoaC3  - Naval Information Warfare Center PacificC3  - National Oceanic Atmospheric Admin (NOAA) - USAC3  - University of St AndrewsC3  - Texas A&M University SystemFU  - US Office of Naval Research [N00014-17-1-2867]
FX  - This work was supported by the US Office of Naval Research (grant no. N00014-17-1-2867).
CR  - Aguilar A, 2018, ENCYCLOPEDIA OF MARINE MAMMALS, 3RD EDITION, P368
CR  - Bee MA, 2008, J COMP PSYCHOL, V122, P235, DOI 10.1037/0735-7036.122.3.235
CR  - Bermant PC, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-48909-4
CR  - Bishop CM, 2006, PATTERN RECOGN
CR  - Bradbury JW., 2011, PRINCIPLES ANIMAL CO
CR  - Buchan SJ, 2019, ENDANGER SPECIES RES, V39, P135, DOI 10.3354/esr00956
CR  - Cakir E, 2017, IEEE-ACM T AUDIO SPE, V25, P1291, DOI 10.1109/TASLP.2017.2690575
CR  - Castellote M, 2012, MAR MAMMAL SCI, V28, P325, DOI 10.1111/j.1748-7692.2011.00491.x
CR  - Cho K., 2014, ARXIV PREPRINT ARXIV, DOI [DOI 10.3115/V1/W14-4012, 10.3115/v1/w14-4012]
CR  - Croll DA, 2002, NATURE, V417, P809, DOI 10.1038/417809a
CR  - Davis GE, 2020, GLOBAL CHANGE BIOL, V26, P4812, DOI 10.1111/gcb.15191
CR  - Delarue J, 2009, J ACOUST SOC AM, V125, P1774, DOI 10.1121/1.3068454
CR  - Garcia HA, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12020326
CR  - Glotin, 2016, 2016 IEEE 26 INT WOR, P1, DOI [DOI 10.1109/MLSP.2016.7738875, 10.1109/MLSP.2016.7738875]
CR  - Harris DV, 2018, J ACOUST SOC AM, V143, P2980, DOI 10.1121/1.5031111
CR  - Helble TA, 2020, FRONT MAR SCI, V7, DOI 10.3389/fmars.2020.587110
CR  - Himawan I, 2018, INTERSPEECH, P2107, DOI 10.21437/Interspeech.2018-1143
CR  - Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
CR  - Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI 10.1162/neco.1997.9.8.1735
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - Ibrahim AK, 2018, J ACOUST SOC AM, V144, pEL196, DOI 10.1121/1.5054911
CR  - Jiang JJ, 2019, APPL ACOUST, V150, P169, DOI 10.1016/j.apacoust.2019.02.007
CR  - Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236
CR  - Kingma DP, 2015, 3 INT C LEARN REPR S
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Leroy EC, 2018, ENDANGER SPECIES RES, V37, P289, DOI 10.3354/esr00927
CR  - Lin M, 2013, PROC 2 INT C LEARN R
CR  - Luo WY, 2019, J ACOUST SOC AM, V145, pEL7, DOI 10.1121/1.5085647
CR  - Madhusudhana S., 2018, P 2018 OCEANS MTS IE P 2018 OCEANS MTS IE, P1
CR  - Madhusudhana S., 2021, Zenodo ., DOI 10.5281/zenodo.4661494
CR  - Madhusudhana S., 2021, Zenodo ., DOI 10.5281/zenodo.4661565
CR  - Madhusudhana SK., 2019, J ACOUST SOC AM, V146, P2982, DOI [10.1121/1.5137323, DOI 10.1121/1.5137323]
CR  - Madhusudhana SK., 2015, THESIS CURTIN U PERT THESIS CURTIN U PERT
CR  - Malfante M., 2018, P 2018 OCEANS MTS IE P 2018 OCEANS MTS IE, P1
CR  - Matias L, 2015, J ACOUST SOC AM, V138, P504, DOI 10.1121/1.4922706
CR  - MCDONALD MA, 1995, J ACOUST SOC AM, V98, P712, DOI 10.1121/1.413565
CR  - McInnes L., 2018, AR XIV180203426V3, P33
CR  - Mellinger DK, 2000, J ACOUST SOC AM, V107, P3518, DOI 10.1121/1.429434
CR  - Oleson EM., 2016, J ACOUST SOC AM, V140, P3296, DOI [10.1121/1.4970481, DOI 10.1121/1.4970481]
CR  - Oleson EM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115678
CR  - Pereira A, 2020, J ACOUST SOC AM, V147, P2235, DOI 10.1121/10.0001066
CR  - Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838
CR  - Sciacca V, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0141838
CR  - Shabangu FW, 2020, ENDANGER SPECIES RES, V43, P21, DOI 10.3354/esr01050
CR  - Shiu Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57549-y
CR  - Siblini W., 2020, ADV INTELLIGENT DATA, V12080
CR  - Sirovic A, 2004, DEEP-SEA RES PT II, V51, P2327, DOI 10.1016/j.dsr2.2004.08.005
CR  - Sirovic A, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-09979-4
CR  - Sirovic A, 2015, ENDANGER SPECIES RES, V28, P61, DOI 10.3354/esr00676
CR  - Sirovic A, 2009, MAR MAMMAL SCI, V25, P125, DOI 10.1111/j.1748-7692.2008.00239.x
CR  - Thomas M., 2019, PROC EUROPEAN C MACH, P290
CR  - THOMPSON PO, 1992, J ACOUST SOC AM, V92, P3051, DOI 10.1121/1.404201
CR  - Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI [DOI 10.4018/978-1-60566-766-9.CH011, 10.4018/978-1-60566-766-9.ch011]
CR  - Usman AM, 2020, IEEE ACCESS, V8, P105181, DOI 10.1109/ACCESS.2020.3000477
CR  - WATKINS WA, 1987, J ACOUST SOC AM, V82, P1901, DOI 10.1121/1.395685
CR  - Watkins William A., 2000, Oceanography, V13, P62
CR  - Weirathmueller MJ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186127
CR  - Wiggins SM, 2007, 2007 SYMPOSIUM ON UNDERWATER TECHNOLOGY AND WORKSHOP ON SCIENTIFIC USE OF SUBMARINE CABLES AND RELATED TECHNOLOGIES, VOLS 1 AND 2, P594
CR  - Wilcock WSD, 2012, J ACOUST SOC AM, V132, P2408, DOI 10.1121/1.4747017
CR  - Zhang YJ, 2018, J ACOUST SOC AM, V144, P478, DOI 10.1121/1.5047743
PU  - ROYAL SOC
PI  - LONDON
PA  - 6-9 CARLTON HOUSE TERRACE, LONDON SW1Y 5AG, ENGLAND
DA  - JUL 21
PY  - 2021
VL  - 18
IS  - 180
DO  - 10.1098/rsif.2021.0297
AN  - WOS:000676307900001
N1  - Times Cited in Web of Science Core Collection:  3
Total Times Cited:  3
Cited Reference Count:  60
ER  -

TY  - CPAPER
AU  - Cheema, GS
AU  - Anand, S
ED  - Altun, Y
ED  - Das, K
ED  - Mielikainen, T
ED  - Malerba, D
ED  - Stefanowski, J
ED  - Read, J
ED  - Zitnik, M
ED  - Ceci, M
ED  - Dzeroski, S
TI  - Automatic Detection and Recognition of Individuals in Patterned Species
T2  - MACHINE LEARNING AND KNOWLEDGE DISCOVERY IN DATABASES, ECML PKDD 2017, PT III
LA  - English
CP  - European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD)
KW  - Animal biometrics
KW  - Wildlife monitoring
KW  - Detection
KW  - Recognition
KW  - Convolutional neural network
KW  - Computer vision
AB  - Visual animal biometrics is rapidly gaining popularity as it enables a non-invasive and cost-effective approach for wildlife monitoring applications. Widespread usage of camera traps has led to large volumes of collected images, making manual processing of visual content hard to manage. In this work, we develop a framework for automatic detection and recognition of individuals in different patterned species like tigers, zebras and jaguars. Most existing systems primarily rely on manual input for localizing the animal, which does not scale well to large datasets. In order to automate the detection process while retaining robustness to blur, partial occlusion, illumination and pose variations, we use the recently proposed Faster-RCNN object detection framework to efficiently detect animals in images. We further extract features from AlexNet of the animal's flank and train a logistic regression (or Linear SVM) classifier to recognize the individuals. We primarily test and evaluate our framework on a camera trap tiger image dataset that contains images that vary in overall image quality, animal pose, scale and lighting. We also evaluate our recognition system on zebra and jaguar images to show generalization to other patterned species. Our framework gives perfect detection results in camera trapped tiger images and a similar or better individual recognition performance when compared with state-of-the-art recognition techniques.
AD  - IIIT Delhi, New Delhi, IndiaC3  - Indraprastha Institute of Information Technology DelhiCR  - Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
CR  - Bolger DT, 2012, METHODS ECOL EVOL, V3, P813, DOI 10.1111/j.2041-210X.2012.00212.x
CR  - Burghardt T., 2004, EWIMT
CR  - Burghardt T, 2006, NEUREL 2006: EIGHT SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL ENGINEERING, PROCEEDINGS, P27
CR  - Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
CR  - Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
CR  - Crall JP, 2013, IEEE WORK APP COMP, P230, DOI 10.1109/WACV.2013.6475023
CR  - Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
CR  - Daugman J, 2004, IEEE T CIRC SYST VID, V14, P21, DOI 10.1109/TCSVT.2003.818350
CR  - Freytag A, 2016, LECT NOTES COMPUT SC, V9796, P51, DOI 10.1007/978-3-319-45886-1_5
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
CR  - Jain AK, 2000, IEEE T IMAGE PROCESS, V9, P846, DOI 10.1109/83.841531
CR  - Jiang XD, 2000, INT C PATT RECOG, P1038, DOI 10.1109/ICPR.2000.906252
CR  - KLINGEL H, 1974, Zeitschrift fuer Tierpsychologie, V36, P37
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lahiri M., 2011, P 1 ACM INT C MULT R, P6
CR  - Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
CR  - Mizroch S., 2003, MAR FISH REV, V65, P25
CR  - Norouzzadeh MS, 2017, ARXIV170305830
CR  - Prodger Phillip, 2009, DARWINS CAMERA ART P
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Scott D. K., 1978, RECOGNITION MARKING, P160
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Tisse C.-l., 2002, P VISION INTERFAC, P294
CR  - TURK MA, 1991, 1991 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, P586
CR  - Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
CR  - Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
PU  - SPRINGER INTERNATIONAL PUBLISHING AG
PI  - CHAM
PA  - GEWERBESTRASSE 11, CHAM, CH-6330, SWITZERLAND
PY  - 2017
VL  - 10536
SP  - 27
EP  - 38
DO  - 10.1007/978-3-319-71273-4_3
AN  - WOS:000443111100003
N1  - Times Cited in Web of Science Core Collection:  14
Total Times Cited:  16
Cited Reference Count:  31
ER  -

TY  - JOUR
AU  - Miao, ZQ
AU  - Gaynor, KM
AU  - Wang, JY
AU  - Liu, ZW
AU  - Muellerklein, O
AU  - Norouzzadeh, MS
AU  - McInturff, A
AU  - Bowie, RCK
AU  - Nathan, R
AU  - Yu, SX
AU  - Getz, WM
TI  - Insights and approaches using deep learning to classify wildlife
T2  - SCIENTIFIC REPORTS
LA  - English
AB  - The implementation of intelligent software to identify and classify objects and individuals in visual fields is a technology of growing importance to operatives in many fields, including wildlife conservation and management. To non-experts, the methods can be abstruse and the results mystifying. Here, in the context of applying cutting edge methods to classify wildlife species from camera-trap data, we shed light on the methods themselves and types of features these methods extract to make efficient identifications and reliable classifications. The current state of the art is to employ convolutional neural networks (CNN) encoded within deep-learning algorithms. We outline these methods and present results obtained in training a CNN to classify 20 African wildlife species with an overall accuracy of 87.5% from a dataset containing 111,467 images. We demonstrate the application of a gradient-weighted class-activation-mapping (Grad-CAM) procedure to extract the most salient pixels in the final convolution layer. We show that these pixels highlight features in particular images that in some cases are similar to those used to train humans to identify these species. Further, we used mutual information methods to identify the neurons in the final convolution layer that consistently respond most strongly across a set of images of one particular species. We then interpret the features in the image where the strongest responses occur, and present dataset biases that were revealed by these extracted features. We also used hierarchical clustering of feature vectors (i.e., the state of the final fully-connected layer in the CNN) associated with each image to produce a visual similarity dendrogram of identified species. Finally, we evaluated the relative unfamiliarity of images that were not part of the training set when these images were one of the 20 species "known" to our CNN in contrast to images of the species that were "unknown" to our CNN.
AD  - Univ Calif Berkeley, Dept Env Sci Pol & Manag, Berkeley, CA 94704 USAAD  - Univ Calif Berkeley, Int Comp Sci Inst, 1947 Ctr St, Berkeley, CA 94704 USAAD  - Univ Calif Berkeley, Vis Sci Grad Grp, Berkeley, CA 94704 USAAD  - Univ Wyoming, Dept Comp Sci, Laramie, WY 82071 USAAD  - Univ Calif Berkeley, Dept Integr Biol, Berkeley, CA USAAD  - Univ Calif Berkeley, Museum Vertebrate Zool, Berkeley, CA USAAD  - Hebrew Univ Jerusalem, Alexander Silberman Inst Life Sci, Dept EEB, Jerusalem, IsraelAD  - Univ KwaZulu Natal, Sch Math Sci, Durban, South AfricaC3  - University of California SystemC3  - University of California BerkeleyC3  - University of California SystemC3  - University of California BerkeleyC3  - University of California SystemC3  - University of California BerkeleyC3  - University of WyomingC3  - University of California SystemC3  - University of California BerkeleyC3  - University of California SystemC3  - University of California BerkeleyC3  - Hebrew University of JerusalemC3  - University of Kwazulu NatalFU  - NSF-GRFP; Rufford Foundation; Idea Wild; Explorers Club; UC Berkeley Center for African Studies; NSF EEID Grant [1617982]; BSF Grant [2015904]
FX  - Thanks to T. Gu, A. Ke, H. Rosen, A. Wu, C. Jurgensen, E. Lai, M. Levy, and E. Silverberg for annotating the images used in this study, and to everyone else involved in this project. Data collection was supported by J. Brashares and through grants to KMG from the NSF-GRFP, the Rufford Foundation, Idea Wild, the Explorers Club, and the UC Berkeley Center for African Studies. We are grateful for the support of Gorongosa National Park, especially M. Stalmans in permitting and facilitating this research. Z. M. was funded in part by NSF EEID Grant 1617982 to W.M.G., R.C.K.B. and R.N., and was also supported in part by BSF Grant 2015904 to R. N. and W. M. G. Thanks to Z. Beba, T. Easter, P. Hammond, Z. Melvin, L. Reiswig, and N. Schramm for participating in the feature survey.
CR  - BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
CR  - Bland LM, 2015, CONSERV BIOL, V29, P250, DOI 10.1111/cobi.12372
CR  - Caravaggi A, 2017, REMOTE SENS ECOL CON, V3, P109, DOI 10.1002/rse2.48
CR  - Chattopadhyay Prithvijit, 2017, ARXIV170805122
CR  - Crisci C, 2012, ECOL MODEL, V240, P113, DOI 10.1016/j.ecolmodel.2012.03.001
CR  - Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hoque S., 2011, INT J BIOSCIENCE BIO, V3, P45
CR  - Kampichler C, 2010, ECOL INFORM, V5, P441, DOI 10.1016/j.ecoinf.2010.06.003
CR  - Kitchin R, 2014, BIG DATA SOC, V1, DOI 10.1177/2053951714528481
CR  - Kuhl HS, 2013, TRENDS ECOL EVOL, V28, P432, DOI 10.1016/j.tree.2013.02.013
CR  - Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
CR  - Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
CR  - MacKay D. J. C., 2002, INFORM THEORY INFERE
CR  - Malisiewicz T., 2009, P 22 INT C NEUR INF, P1222
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Rahman DA, 2017, ORYX, V51, P665, DOI 10.1017/S0030605316000429
CR  - Rangel TF, 2012, NAT CONSERVACAO, V10, P119, DOI 10.4322/natcon.2012.030
CR  - Rokach L, 2005, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, P321, DOI 10.1007/0-387-25465-X_15
CR  - Sejnowski TJ, 2016, COMPUT NEUROSCI-MIT, pIX
CR  - Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Springenberg J.T., 2014, ARXIV ABS14126806
CR  - Tabak M. A., 2018, BIORXIV, DOI [10.1101/346809, DOI 10.1101/346809]
CR  - Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
CR  - Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
CR  - van Horn G, 2017, ARXIV170901450
CR  - Vinyals O., 2016, 30 ANN C NEURAL INFO, V29, P3630
CR  - Waldchen J, 2018, METHODS ECOL EVOL, V9, P2216, DOI 10.1111/2041-210X.13075
CR  - Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808
CR  - Zhou B., 2014, OBJECT DETECTORS EME
PU  - NATURE PORTFOLIO
PI  - BERLIN
PA  - HEIDELBERGER PLATZ 3, BERLIN, 14197, GERMANY
DA  - MAY 31
PY  - 2019
VL  - 9
DO  - 10.1038/s41598-019-44565-w
AN  - WOS:000469752800015
N1  - Times Cited in Web of Science Core Collection:  23
Total Times Cited:  23
Cited Reference Count:  35
ER  -

TY  - CPAPER
AU  - Marsden, M
AU  - McGuinness, K
AU  - Little, S
AU  - Keogh, CE
AU  - O'Connor, NE
A1  - IEEE
TI  - People, Penguins and Petri Dishes: Adapting Object Counting Models To New Visual Domains And Object Types Without Forgetting
T2  - 2018 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION (CVPR)
LA  - English
CP  - 31st IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
AB  - In this paper we propose a technique to adapt a convolutional neural network (CNN) based object counter to additional visual domains and object types while still preserving the original counting function. Domain-specific normalisation and scaling operators are trained to allow the model to adjust to the statistical distributions of the various visual domains. The developed adaptation technique is used to produce a singular patch-based counting regressor capable of counting various object types including people, vehicles, cell nuclei and wildlife. As part of this study a challenging new cell counting dataset in the context of tissue culture and patient diagnosis is constructed. This new collection, referred to as the Dublin Cell Counting (DCC) dataset, is the first of its kind to be made available to the wider computer vision community. State-of-the-art object counting performance is achieved in both the Shanghaitech (parts A and B) and Penguins datasets while competitive performance is observed on the TRANCOS and Modified Bone Marrow (MBM) datasets, all using a shared counting model.
AD  - Dublin City Univ, Insight Ctr Data Analyt, Dublin, IrelandAD  - Univ Coll Dublin, Sch Med, Conway Inst, Dublin, IrelandC3  - Dublin City UniversityC3  - University College DublinFU  - Irish Research Council; Science Foundation Ireland (SFI) [SFI/12/RC/2289, 15/SIRG/3283]
FX  - This publication has emanated from research conducted with the financial support of the Irish Research Council and Science Foundation Ireland (SFI) under grant numbers SFI/12/RC/2289 and 15/SIRG/3283. A special thanks is also given to the Cummin's group in the School of Medicine, University College Dublin for making the DCC dataset images available to the wider research community.
CR  - Abadi Martin, 2015, TENSORFLOW LARGE SCA
CR  - Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30
CR  - Bengio Y., 2010, P 13 INT C ARTIFICIA, P249, DOI DOI 10.1177/1753193409103364.
CR  - Bilen H., 2017, ARXIV170107275
CR  - Chan AB, 2012, IEEE T IMAGE PROCESS, V21, P2160, DOI 10.1109/TIP.2011.2172800
CR  - Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
CR  - Chollet F., 2015, KERAS
CR  - Cohen J.P., 2017, ARXIV170308710
CR  - Cohen JP, 2017, IEEE INT CONF COMP V, P18, DOI 10.1109/ICCVW.2017.9
CR  - Deng J., 2009, CVPR09
CR  - Duchi J, 2011, J MACH LEARN RES, V12, P2121
CR  - Ge Weina, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2913, DOI 10.1109/CVPRW.2009.5206621
CR  - Guerrero-Gomez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
CR  - He K., 2016, 2016 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2016.90
CR  - Hu YC, 2016, J VIS COMMUN IMAGE R, V38, P530, DOI 10.1016/j.jvcir.2016.03.021
CR  - Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
CR  - Ioffe S., 2015, BATCH NORMALIZATION, P448
CR  - Kang D., 2017, ARXIV170510118
CR  - Lempitsky Victor, 2010, ADV NEURAL INFORM PR, P1324
CR  - Lin SF, 2001, IEEE T SYST MAN CY A, V31, P645, DOI 10.1109/3468.983420
CR  - Liu TL, 2014, INT CONF INFO SCI, P100, DOI 10.1109/ICIST.2014.6920341
CR  - Loy CC, 2013, IEEE I CONF COMP VIS, P2256, DOI 10.1109/ICCV.2013.270
CR  - Marsden Mark, 2017, INT C COMP VIS THEOR
CR  - Onoro-Rubio D, 2016, LECT NOTES COMPUT SC, V9911, P615, DOI 10.1007/978-3-319-46478-7_38
CR  - Rebuffi Sylvestre-Alvise, 2017, 2017 C NEUR INF PROC
CR  - Sam Deepak Babu, 2017, IEEE C COMP VIS PATT, V1, P6
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Weyand, 2017, ARXIV170404861
CR  - Wu B, 2005, IEEE I CONF COMP VIS, P90
CR  - Xie WD, 2018, COMP M BIO BIO E-IV, V6, P283, DOI 10.1080/21681163.2016.1149104
CR  - Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
CR  - Zhang Shanghang, 2017, ARXIV170709476
CR  - Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2018
SP  - 8070
EP  - 8079
DO  - 10.1109/CVPR.2018.00842
AN  - WOS:000457843608025
N1  - Times Cited in Web of Science Core Collection:  22
Total Times Cited:  23
Cited Reference Count:  33
ER  -

TY  - JOUR
AU  - Knight, EC
AU  - Solymos, P
AU  - Scott, C
AU  - Bayne, EM
TI  - Validation prediction: a flexible protocol to increase efficiency of automated acoustic processing for wildlife research
T2  - ECOLOGICAL APPLICATIONS
LA  - English
KW  - autonomous recording unit (ARU)
KW  - bioacoustic
KW  - bird
KW  - machine learning
KW  - passive acoustic monitoring
KW  - recognizer
KW  - signal processing
KW  - BAT ECHOLOCATION CALLS
KW  - IDENTIFICATION
KW  - RECORDINGS
KW  - FOREST
KW  - BIRDS
KW  - TOOL
AB  - Automated recognition is increasingly used to extract species detections from audio recordings; however, the time required to manually review each detection can be prohibitive. We developed a flexible protocol called "validation prediction" that uses machine learning to predict whether recognizer detections are true or false positives and can be applied to any recognizer type, ecological application, or analytical approach. Validation prediction uses a predictable relationship between recognizer score and the energy of an acoustic signal but can also incorporate any other ecological or spectral predictors (e.g., time of day, dominant frequency) that will help separate true from false-positive recognizer detections. First, we documented the relationship between recognizer score and the energy of an acoustic signal for two different recognizer algorithm types (hidden Markov models and convolutional neural networks). Next, we demonstrated our protocol using a case study of two species, the Common Nighthawk (Chordeiles minor) and Ovenbird (Seiurus aurocapilla). We reduced the number of detections that required validation by 75.7% and 42.9%, respectively, while retaining at least 98% of the true-positive detections. Validation prediction substantially improves the efficiency of using automated recognition on acoustic data sets. Our method can be of use to wildlife monitoring and research programs and will facilitate using automated recognition to mine bioacoustic data sets.
AD  - Univ Alberta, Biol Sci Ctr CW405, Dept Biol Sci, Edmonton, AB, CanadaAD  - Bishon House, Bishopstone HR4 7HZ, Hereford, EnglandC3  - University of AlbertaFU  - Natural Sciences and Engineering Research Council of Canada [44660-12] Funding Source: Medline; Canadian Oilsands Innovation Alliance Funding Source: Medline; Alberta Biodiversity Monitoring Institute Funding Source: Medline; Alberta Conservation Association [030-00-90-273] Funding Source: Medline
CR  - Araya-Salas M, 2017, METHODS ECOL EVOL, V8, P184, DOI 10.1111/2041-210X.12624
CR  - Balantic C, 2019, ECOL APPL, V29, DOI 10.1002/eap.1854
CR  - Balantic CM, 2020, BIOACOUSTICS, V29, P296, DOI 10.1080/09524622.2019.1605309
CR  - Barre K, 2019, METHODS ECOL EVOL, V10, P1171, DOI 10.1111/2041-210X.13198
CR  - Borker AL, 2014, CONSERV BIOL, V28, P1100, DOI 10.1111/cobi.12264
CR  - Cakir E, 2017, EUR SIGNAL PR CONF, P1744, DOI 10.23919/EUSIPCO.2017.8081508
CR  - Campos-Cerqueira M, 2016, METHODS ECOL EVOL, V7, P1340, DOI 10.1111/2041-210X.12599
CR  - Chambert T, 2018, METHODS ECOL EVOL, V9, P560, DOI 10.1111/2041-210X.12910
CR  - Charchuk C, 2018, FOREST ECOL MANAG, V407, P9, DOI 10.1016/j.foreco.2017.10.033
CR  - Darras K, 2018, METHODS ECOL EVOL, V9, P1928, DOI 10.1111/2041-210X.13031
CR  - Elith J, 2008, J ANIM ECOL, V77, P802, DOI 10.1111/j.1365-2656.2008.01390.x
CR  - Gibb R., 2018, METHODS ECOLOGY EVOL, V3, P992
CR  - Gradisek A, 2017, BIOACOUSTICS, V26, P63, DOI 10.1080/09524622.2016.1190946
CR  - Jaiswara R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075930
CR  - Joshi KA, 2017, EMU, V117, P233, DOI 10.1080/01584197.2017.1298970
CR  - Knight EC, 2019, BIOACOUSTICS, V28, P539, DOI 10.1080/09524622.2018.1503971
CR  - Knight EC, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-01114-120214
CR  - Marques TA, 2013, BIOL REV, V88, P287, DOI 10.1111/brv.12001
CR  - Marques TA, 2009, J ACOUST SOC AM, V125, P1982, DOI 10.1121/1.3089590
CR  - Noda JJ, 2016, APPL SCI-BASEL, V6, DOI 10.3390/app6120443
CR  - Petruskova T, 2016, METHODS ECOL EVOL, V7, P274, DOI 10.1111/2041-210X.12496
CR  - Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
CR  - R Core Team, 2017, R LAN ENV STAT COMP
CR  - Rebbeck M, 2008, IBIS LOND 1859, V143, P468
CR  - Russo D, 2016, ECOL INDIC, V66, P598, DOI 10.1016/j.ecolind.2016.02.036
CR  - Rydell J, 2017, ECOL INDIC, V78, P416, DOI 10.1016/j.ecolind.2017.03.023
CR  - Shonfield J, 2018, J RAPTOR RES, V52, P42, DOI 10.3356/JRR-17-52.1
CR  - Shonfield J, 2017, AVIAN CONSERV ECOL, V12, DOI 10.5751/ACE-00974-120114
CR  - Solymos P, 2018, CONDOR, V120, P765, DOI 10.1650/CONDOR-18-32.1
CR  - STOWELL D, 2016, IEEE INT WORKS MACH
CR  - Stowell D., 2018, METHODS ECOLOGY EVOL, V1, P1774
CR  - Stowell D, 2014, PEERJ, V2, DOI 10.7717/peerj.488
CR  - Sueur J, 2008, BIOACOUSTICS, V18, P213, DOI 10.1080/09524622.2008.9753600
CR  - Swiston KA, 2009, J FIELD ORNITHOL, V80, P42, DOI 10.1111/j.1557-9263.2009.00204.x
CR  - Turesson HK, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0163041
CR  - Walters CL, 2012, J APPL ECOL, V49, P1064, DOI 10.1111/j.1365-2664.2012.02182.x
CR  - Xie J, 2018, ARTIF INTELL REV, V49, P375, DOI 10.1007/s10462-016-9529-z
CR  - Yip D. A., 2019, REMOTE SENSING ECOLO, DOI 10.1002/rse2.118
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - OCT
PY  - 2020
VL  - 30
IS  - 7
DO  - 10.1002/eap.2140
AN  - WOS:000535305700001
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  5
Cited Reference Count:  38
ER  -

TY  - CPAPER
AU  - Akula, A
AU  - Kaur, V
AU  - Guleria, N
AU  - Ghosh, R
AU  - Kumar, S
ED  - Hammoud, RI
ED  - Overman, TL
ED  - Mahalanobis, A
TI  - Real-time thermal infrared moving target detection and recognition using deep learned features
T2  - AUTOMATIC TARGET RECOGNITION XXX
LA  - English
CP  - Conference on Automatic Target Recognition XXX
KW  - deep neural network
KW  - infrared imaging
KW  - transfer learning
KW  - classification
KW  - wildlife monitoring
KW  - poaching
AB  - Surveillance applications demand round the clock monitoring of regions in constrained illumination conditions. Thermal infrared cameras which capture the heat emitted by the objects present in the scene appear as a suitable sensor technology for such applications. However, developing of AI techniques for automatic detection of targets for monitoring applications is challenging due to high variability of targets within a class, variations in pose of targets, widely varying environmental conditions, etc. This paper presents a real-time framework to detect and classify targets in a forest landscape. The system comprises of two main stages: the moving target detection and detected target classification. For the first stage, Mixture of Gaussians (MoG) background subtraction is used for detection of Region of Interest (ROI) from individual frames of the IR video sequence. For the second stage, a pre-trained Deep Convolutional Neural Network with additional custom layers has been used for the feature extraction and classification. A challenging thermal dataset created by using both experimentally generated thermal infrared images and from publically available FLIR Thermal Dataset. This dataset is used for training and validating the proposed deep learning framework. The model demonstrated a preliminary testing accuracy of 95%. The real-time deployment of the framework is done on embedded platform having an 8-core ARM v8.2 64-bit CPU and 512-core Volta GPU with Tensor Cores. The moving target detection and recognition framework achieved a frame rate of approximately 23 fps on this embedded computing platform, making it suitable for deployment in resource constrained environments.
AD  - CSIR Cent Sci Instruments Org India, New Delhi, IndiaFU  - Council of Scientific and Industrial Research, India, under CSIR Mission Project -Safety and Security of Vital Installations [HCP17]; Wildlife Institute of India under WII; CSIR-CSIO collaborative research project [CLP31]
FX  - This work was supported in part by the funds from the Council of Scientific and Industrial Research, India, under CSIR Mission Project -Safety and Security of Vital Installations under Grant HCP17 and from the Wildlife Institute of India under WII and CSIR-CSIO collaborative research project, CLP31. The authors thank Uttarakhand forest department and Wildlife Institute of India specially Dr. V B Mathur and Dr. Bivash Pandav for facilitating the data collection using captive elephants.
CR  - Akula A, 2019, TENCON IEEE REGION, P2370, DOI 10.1109/TENCON.2019.8929697
CR  - Akula A, 2014, INFRARED PHYS TECHN, V63, P103, DOI 10.1016/j.infrared.2013.12.012
CR  - Apps PJ, 2018, AFR J ECOL, V56, P702, DOI 10.1111/aje.12563
CR  - Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
CR  - Bondi E., 2019, ARTIFICIAL INTELLIGE, V77
CR  - Bondi E, 2018, THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE, P7741
CR  - Burghardt T, 2006, IEE P-VIS IMAGE SIGN, V153, P305, DOI 10.1049/ip-vis:20050052
CR  - Choudhury A., 2008, ELEPHAS MAXIMUS
CR  - d'Acremont A, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19092040
CR  - Gonzalez LF, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010097
CR  - Gula R, 2010, EUR J WILDLIFE RES, V56, P803, DOI 10.1007/s10344-010-0392-y
CR  - Hodgson JC, 2016, SCI REP-UK, V6, DOI 10.1038/srep22574
CR  - Jowit J., 2010, HUMANS DRIVING EXTIN
CR  - Kale A., 2017, RES MATTERS
CR  - Khorrami P., 2012, P 21 INT C PATT REC, P11
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Nasrabadi NM, 2019, IEEE T AERO ELEC SYS, V55, P2687, DOI 10.1109/TAES.2019.2894050
CR  - Phelan J., 2015, 6 ENDANGERED ANIMALS
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - *SYST F, 2018, FLIR THERM DAT ALG T
CR  - Vinson S. G., 2020, AUSTR MAMMALOGY
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
CR  - Zhang L, 2008, BIODIVERS CONSERV, V17, P1493, DOI 10.1007/s10531-008-9358-8
CR  - Zivkovic Z, 2006, PATTERN RECOGN LETT, V27, P773, DOI 10.1016/j.patrec.2005.11.005
CR  - Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
PU  - SPIE-INT SOC OPTICAL ENGINEERING
PI  - BELLINGHAM
PA  - 1000 20TH ST, PO BOX 10, BELLINGHAM, WA 98227-0010 USA
PY  - 2020
VL  - 11394
DO  - 10.1117/12.2559058
AN  - WOS:000589995700023
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  26
ER  -

TY  - JOUR
AU  - Zulkifley, MA
TI  - Two Streams Multiple-Model Object Tracker for Thermal Infrared Video
T2  - IEEE ACCESS
LA  - English
KW  - Siamese networks
KW  - single object tracking
KW  - thermal infrared video
AB  - Thermal infrared (TIR) visual object tracking has been applied in various applications, such as pedestrian detection, wildlife observation, surveillance systems, and so on. The tracker function is to track a particular object of interest and generate its trajectory which will be integrated into a decision making process. Many trackers that are based on fully convolutional neural networks (CNNs) have good performance for RGB input, but it is not the case for the TIR input. Its lack of texture information and the fact that it produces similar heat maps between two nearby objects make the tracking task very challenging. By relying on a fully CNN network alone, a tracker can learn the appearance model but it will not work well if the object heat map looks too similar to the background. Hence, a Siamese CNN network can be implemented to complement the fully CNN, as it allows a set of recent object templates to be used for matching purposes. Yet, the Siamese network alone is not accurate especially in the case of occlusions, as the stored templates rarely produce robust matching. Thus, we propose a two-stream CNN tracker that combines the fully CNN and the Siamese CNN such that each network keeps a set of matching models to cater to diverse appearance changes. Furthermore, the CNN layers are shared between both CNN streams to reduce computational burden. A single dense score map is produced by overlaying the normalized scores of the two streams. The experiments on VOT-TIR 2016 database show that our tracker works well for the datasets with high motion blur, occlusion, and appearance deformation. Besides, the Siamese CNN response map can also be used as an indicator to decide the size of the search region.
AD  - Univ Kebangsaan Malaysia, Fac Engn & Built Environm, Ctr Integrated Syst Engn & Adv Technol, Bangi 43650, MalaysiaC3  - Universiti Kebangsaan MalaysiaFU  - Universiti Kebangsaan Malaysia through Geran Universiti Penyelidikan [GUP-2015-053]; Universiti Kebangsaan Malaysia through Dana Impak Perdana [DIP-2015-006]; Nvidia Corporation through the Titan V Grant
FX  - This work was supported in part by the Universiti Kebangsaan Malaysia through Geran Universiti Penyelidikan under Grant GUP-2015-053, in part by the Universiti Kebangsaan Malaysia through Dana Impak Perdana under Grant DIP-2015-006, and in part by the Nvidia Corporation through the Titan V Grant.
CR  - Ar I, 2014, IEEE T NEUR SYS REH, V22, P1160, DOI 10.1109/TNSRE.2014.2326254
CR  - Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
CR  - Buyval A, 2018, IEEE INT CONF ROBOT, P2064
CR  - Cilulko J, 2013, EUR J WILDLIFE RES, V59, P17, DOI 10.1007/s10344-012-0688-1
CR  - Dai KH, 2017, IEEE IMAGE PROC, P3640, DOI 10.1109/ICIP.2017.8296961
CR  - Danelljan M, 2014, P BRIT MACH VIS C
CR  - Du DW, 2017, IEEE T CYBERNETICS, V47, P4182, DOI 10.1109/TCYB.2016.2626275
CR  - Felsberg M., 2016, THERMAL INFRARED VIS, P824
CR  - GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
CR  - Han, 2016, MODELING PROPAGATING
CR  - Han B, 2017, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2017.63
CR  - Hart AG, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0131584
CR  - Jung I., 2018, ECCV, P83
CR  - KaewTraKulPong P, 2002, VIDEO-BASED SURVEILLANCE SYSTEMS: COMPUTER VISION AND DISTRIBUTED PROCESSING, P135
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
CR  - Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
CR  - Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
CR  - Li X, 2016, KNOWL-BASED SYST, V113, P88, DOI 10.1016/j.knosys.2016.09.014
CR  - Li Y., 2015, SCALE ADAPTIVE KERNE, P254
CR  - Liu Q, 2017, KNOWL-BASED SYST, V134, P189, DOI 10.1016/j.knosys.2017.07.032
CR  - Loce R. P., 2017, PEDESTRIAN DETECTION, P432
CR  - Lukezic A, 2018, IEEE T CYBERNETICS, V48, P1849, DOI 10.1109/TCYB.2017.2716101
CR  - NAM H, 2016, PROC CVPR IEEE, P4293, DOI DOI 10.1109/CVPR.2016.465
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Tan Y, 2013, INFRARED PHYS TECHN, V61, P230, DOI 10.1016/j.infrared.2013.08.014
CR  - Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
CR  - Wang GK, 2017, INT CONF ACOUST SPEE, P1917, DOI 10.1109/ICASSP.2017.7952490
CR  - Wang LJ, 2016, PROC CVPR IEEE, P1373, DOI 10.1109/CVPR.2016.153
CR  - Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
CR  - Yang LC, 2017, IEEE IMAGE PROC, P2567, DOI 10.1109/ICIP.2017.8296746
CR  - Yu XG, 2017, PATTERN RECOGN LETT, V100, P152, DOI 10.1016/j.patrec.2017.10.026
CR  - Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
CR  - Zulkifley MA, 2018, IEEE ACCESS, V6, P42790, DOI 10.1109/ACCESS.2018.2859595
PU  - IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI  - PISCATAWAY
PA  - 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
PY  - 2019
VL  - 7
SP  - 32383
EP  - 32392
DO  - 10.1109/ACCESS.2019.2903829
AN  - WOS:000463434100001
N1  - Times Cited in Web of Science Core Collection:  16
Total Times Cited:  16
Cited Reference Count:  34
ER  -

TY  - JOUR
AU  - Abrams, JF
AU  - Vashishtha, A
AU  - Wong, ST
AU  - Nguyen, A
AU  - Mohamed, A
AU  - Wieser, S
AU  - Kuijper, A
AU  - Wilting, A
AU  - Mukhopadhyay, A
TI  - Habitat-Net: Segmentation of habitat images using deep learning
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Habitat interpretation
KW  - Image segmentation
KW  - Convolutional neural network
KW  - Deep learning
KW  - Canopy closure
KW  - Understory vegetation density
KW  - Forest
KW  - FOREST CANOPY
KW  - UNDERSTOREY VEGETATION
KW  - COVER
AB  - Understanding environmental factors that influence forest health, as well as the occurrence and abundance of wildlife, is a central topic in forestry and ecology. However, the manual processing of field habitat data is time-consuming and months are often needed to progress from data collection to data interpretation. To shorten the time to process the data we propose here Habitat-Net: a novel deep learning application based on Convolutional Neural Networks (CNN) to segment habitat images of tropical rainforests. Habitat-Net takes color images as input and after multiple layers of convolution and deconvolution, produces a binary segmentation of the input image. We worked on two different types of habitat datasets that are widely used in ecological studies to characterize the forest conditions: canopy closure and understory vegetation. We trained the model with 800 canopy images and 700 understory images separately and then used 149 canopy and 172 understory images to test the performance of Habitat-Net. We compared the performance of Habitat-Net to the performance of a simple threshold based method, manual processing by a second researcher and a CNN approach called U-Net, upon which Habitat-Net is based. Habitat-Net, U-Net and simple thresholding reduced total processing time to milliseconds per image, compared to 45 s per image for manual processing. However, the higher mean Dice coefficient of Habitat-Net (0.94 for canopy and 0.95 for understory) indicates that accuracy of Habitat-Net is higher than that of both the simple thresholding (0.64, 0.83) and U-Net (0.89, 0.94). Habitat-Net will be of great relevance for ecologists and foresters, who need to monitor changes in their forest structures. The automated workflow not only reduces the time, it also standardizes the analytical pipeline and, thus, reduces the degree of uncertainty that would be introduced by manual processing of images by different people (either over time or between study sites).
AD  - Leibniz Inst Zoo & Wildlife Res IZW, D-10315 Berlin, GermanyAD  - Tech Univ Darmstadt, Dept Comp Sci, D-64283 Darmstadt, GermanyAD  - WWF Malaysia, Petaling Jaya 46150, Selangor, MalaysiaAD  - Fraunhofer IGD, D-64283 Darmstadt, GermanyC3  - Leibniz Institut fur Zoo und WildtierforschungC3  - Technical University of DarmstadtC3  - World Wildlife FundFU  - German Federal Ministry of Education and Research [BMBF FKZ: 01LN1301A]; Point Defiance Zoo and Aquarium through Dr. Holly Reed Conservation Fund; San Francisco Zoo; Sabah Biodiversity Center; Sabah Forestry Department
FX  - We thank the Sabah Biodiversity Center and the Sabah Forestry Department, especially Johnny Kissing and Peter Lagan for support and involvement in this project. Many thanks go to the field team for their hard work to take all the habitat photographs. We would also like to thank Srijita Guha for helping with the manual cropping of understory images. This project received financial support from the German Federal Ministry of Education and Research (BMBF FKZ: 01LN1301A), Point Defiance Zoo and Aquarium through Dr. Holly Reed Conservation Fund and San Francisco Zoo.
CR  - Brenes-Arguedas T, 2011, OECOLOGIA, V166, P443, DOI 10.1007/s00442-010-1832-9
CR  - Chopping M, 2008, REMOTE SENS ENVIRON, V112, P2051, DOI 10.1016/j.rse.2007.07.024
CR  - Cristescu B, 2013, AMBIO, V42, P805, DOI 10.1007/s13280-013-0410-x
CR  - D'Amato AW, 2009, FOREST ECOL MANAG, V257, P1043, DOI 10.1016/j.foreco.2008.11.003
CR  - DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
CR  - Draper B. A., 1999, Computer Vision Systems. First International Conference, ICVS'99. Proceedings, P522
CR  - Erfanifard Y, 2014, EUR J REMOTE SENS, V47, P773, DOI 10.5721/EuJRS20144744
CR  - Gilliam FS, 2007, BIOSCIENCE, V57, P845, DOI 10.1641/B571007
CR  - HALPERN CB, 1995, ECOL APPL, V5, P913, DOI 10.2307/2269343
CR  - Hamraz H, 2017, ISPRS J PHOTOGRAMM, V130, P385, DOI 10.1016/j.isprsjprs.2017.07.001
CR  - Ioffe S., 2015, P INT C MACH LEARN P
CR  - Jaccard P., 1907, REV GEN SCI, V18, P961
CR  - Jennings SB, 1999, FORESTRY, V72, P59, DOI 10.1093/forestry/72.1.59
CR  - Jonckheere I, 2005, AGR FOREST METEOROL, V132, P96, DOI 10.1016/j.agrformet.2005.06.003
CR  - JONES R, 1968, Folia Geobotanica and Phytotaxonomica, V3, P355
CR  - Jorgensen CF, 2013, APPL VEG SCI, V16, P552, DOI 10.1111/avsc.12037
CR  - Korhonen L, 2006, SILVA FENN, V40, P577, DOI 10.14214/sf.315
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Levner I, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P870
CR  - Limb RF, 2007, RANGELAND ECOL MANAG, V60, P548, DOI 10.2111/1551-5028(2007)60[548:DPRIVI]2.0.CO;2
CR  - Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
CR  - Marsden SJ, 2002, FOREST ECOL MANAG, V165, P117, DOI 10.1016/S0378-1127(01)00653-3
CR  - Morrison LW, 2016, J PLANT ECOL, V9, P367, DOI 10.1093/jpe/rtv077
CR  - Niedballa J, 2015, SCI REP-UK, V5, DOI 10.1038/srep17041
CR  - Nobis M, 2005, AGR FOREST METEOROL, V128, P243, DOI 10.1016/j.agrformet.2004.10.002
CR  - NUDDS T D, 1977, Wildlife Society Bulletin, V5, P113
CR  - OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
CR  - Paletto A, 2009, EUR J FOREST RES, V128, P265, DOI 10.1007/s10342-009-0262-x
CR  - Perez L., 2017, ABS17120 CORR
CR  - Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
CR  - Rahman Z, 2018, INT J COMPUT APPL, P1
CR  - Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
CR  - Russell MB, 2014, FORESTRY, V87, P629, DOI 10.1093/forestry/cpu023
CR  - Sermanet P., 2013, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/CVPR.2015.7299176
CR  - Sorensen T., 1948, KONGELIGE DANSKE VID, V5, P1
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - Stojanova D, 2010, ECOL INFORM, V5, P256, DOI 10.1016/j.ecoinf.2010.03.004
CR  - Sutskever I., 2013, INT C MACHINE LEARNI, P1139
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Tao SL, 2015, ISPRS J PHOTOGRAMM, V110, P66, DOI 10.1016/j.isprsjprs.2015.10.007
CR  - van der Walt S, 2014, PEERJ, V2, DOI 10.7717/peerj.453
CR  - Vickers AD, 2000, FORESTRY, V73, P37, DOI 10.1093/forestry/73.1.37
CR  - Wilson A., 2017, ADV NEURAL INFORM PR, P4148
CR  - Zeng YJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0075661
CR  - Zhao KG, 2011, REMOTE SENS ENVIRON, V115, P1978, DOI 10.1016/j.rse.2011.04.001
CR  - 2002, SILVA FENNICA, V36, P353
CR  - 2006, ANN FOREST SCI, V63, P31, DOI DOI 10.1051/FOREST:2005091
CR  - 2015, ISPRS J PHOTOGRAMM, V101, P89, DOI DOI 10.1016/J.ISPRSJPRS.2014.11.007
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2019
VL  - 51
SP  - 121
EP  - 128
DO  - 10.1016/j.ecoinf.2019.01.009
AN  - WOS:000467661300013
N1  - Times Cited in Web of Science Core Collection:  13
Total Times Cited:  13
Cited Reference Count:  48
ER  -

TY  - JOUR
AU  - Willi, M
AU  - Pitman, RT
AU  - Cardoso, AW
AU  - Locke, C
AU  - Swanson, A
AU  - Boyer, A
AU  - Veldthuis, M
AU  - Fortson, L
TI  - Identifying animal species in camera trap images using deep learning and citizen science
T2  - METHODS IN ECOLOGY AND EVOLUTION
LA  - English
KW  - animal identification
KW  - camera trap
KW  - citizen science
KW  - convolutional neural networks
KW  - deep learning
KW  - machine learning
AB  - Ecologists often study wildlife populations by deploying camera traps. Large datasets are generated using this approach which can be difficult for research teams to manually evaluate. Researchers increasingly enlist volunteers from the general public as citizen scientists to help classify images. The growing number of camera trap studies, however, makes it ever more challenging to find enough volunteers to process all projects in a timely manner. Advances in machine learning, especially deep learning, allow for accurate automatic image classification. By training models using existing datasets of images classified by citizen scientists and subsequent application of such models on new studies, human effort may be reduced substantially. The goals of this study were to (a) assess the accuracy of deep learning in classifying camera trap data, (b) investigate how to process datasets with only a few classified images that are generally difficult to model, and (c) apply a trained model on a live online citizen science project. Convolutional neural networks (CNNs) were used to differentiate among images of different animal species, images of humans or vehicles, and empty images (no animals, vehicles, or humans). We used four different camera trap datasets featuring a wide variety of species, different habitats, and a varying number of images. All datasets were labelled by citizen scientists on Zooniverse. Accuracies for identifying empty images across projects ranged between 91.2% and 98.0%, whereas accuracies for identifying specific species were between 88.7% and 92.7%. Transferring information from CNNs trained on large datasets ("transfer-learning") was increasingly beneficial as the size of the training dataset decreased and raised accuracy by up to 10.3%. Removing low-confidence predictions increased model accuracies to the level of citizen scientists. By combining a trained model with classifications from citizen scientists, human effort was reduced by 43% while maintaining overall accuracy for a live experiment running on Zooniverse. Ecology researchers can significantly reduce image classification time and manual effort by combining citizen scientists and CNNs, enabling faster processing of data from large camera trap studies.
AD  - Univ Minnesota, Sch Phys & Astron, Minneapolis, MN 55455 USAAD  - Panthera, New York, NY USAAD  - Univ Cape Town, Inst Communities & Wildlife Africa, Dept Biol Sci, Cape Town, South AfricaAD  - Univ Oxford, Sch Geog & Environm, Oxford, EnglandAD  - Wisconsin Dept Nat Resources, Off Appl Sci, Madison, WI USAAD  - Univ Oxford, Dept Astrophys, Oxford, EnglandAD  - Adler Planetarium, Chicago, IL USAC3  - University of Minnesota SystemC3  - University of Minnesota Twin CitiesC3  - University of Cape TownC3  - League of European Research Universities - LERUC3  - University of OxfordC3  - League of European Research Universities - LERUC3  - University of OxfordFU  - National Science Foundation [IIS 1619177]
FX  - National Science Foundation, Grant/Award Number: IIS 1619177
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Bowyer A., 2015, THIS IMAGE INTENTION
CR  - Branson S., 2017, CVPR
CR  - Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
CR  - Dickinson JL, 2010, ANNU REV ECOL EVOL S, V41, P149, DOI 10.1146/annurev-ecolsys-102209-144636
CR  - Fortson L, 2012, CH CRC DATA MIN KNOW, P213
CR  - Gal Y., 2015, P ICLR
CR  - Gal Y., 2016, THESIS, P174
CR  - Giraldo-Zuluaga JH, 2017, PROC INT C TOOLS ART, P53, DOI 10.1109/ICTAI.2017.00020
CR  - Glorot X., 2010, P 13 INT C ARTIFICIA, V9, P249
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Guo CA, 2017, PR MACH LEARN RES, V70
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hines G, 2015, PROCEEDINGS OF THE TWENTY-NINTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3975
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
CR  - Parham J., 2016, APPL COMP VIS WORKSH, P1
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
CR  - Simpson E., 2013, DECISION MAKING IMPE, P1, DOI DOI 10.1007/978-3-642-36406-8_1
CR  - Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Yosinski J., 2014, ADV NEURAL INFORM PR, P3320
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zevin M., 2016, CLASSICAL QUANTUM GR, V34, P1
CR  - Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - JAN
PY  - 2019
VL  - 10
IS  - 1
SP  - 80
EP  - 91
DO  - 10.1111/2041-210X.13099
AN  - WOS:000457750600008
N1  - Times Cited in Web of Science Core Collection:  95
Total Times Cited:  97
Cited Reference Count:  28
ER  -

TY  - JOUR
AU  - Hans, WJ
AU  - Venkateswaran, N
AU  - Solomi, VS
TI  - On-Road Deer Detection for Advanced Driver Assistance using Convolutional Neural Network
T2  - INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS
LA  - English
KW  - Computer vision
KW  - animal detection
KW  - deep learning
KW  - Animal Vehicle Collision (AVC)
KW  - VEHICLE COLLISIONS
AB  - Animal-vehicle collision (AVC) is a major concern in road safety that affects human life, properties, and wildlife. Most of the collisions happen with large animals especially deer that enters the road suddenly. Furthermore, the threat is even more alarming in poor visibility conditions such as night-time, fog, rain, etc. Therefore, it is vital to detect the presence of deer on roadways to mitigate the severity of deer-vehicle collision (DVC). This paper presents an efficient methodology to detect deer on roadways both during the day and night-time conditions using deep learning framework. A two-class CNN model differentiating a deer from its background is developed. The background will have a few classes of objects such as motorcycles, cars, and trees which are frequently encountered on roadways. A self-constructed dataset with both RGB and thermal images is used to train the CNN model. Sliding window technique is used to localize the spatial region of deer in an image. The performance of the proposed CNN model is compared with state-of-the art classifiers and pre-trained CNN models and the results validate its effectiveness.
AD  - SSN Coll Engn, ECE, Chennai, Tamil Nadu, IndiaAD  - Hindustan Inst Technol & Sci, ECE, Chennai, Tamil Nadu, IndiaC3  - SSN College of EngineeringC3  - Hindustan Institute of Technology & ScienceCR  - Benenson R, 2015, LECT NOTES COMPUT SC, V8926, P613, DOI 10.1007/978-3-319-16181-5_47
CR  - Bengler K, 2014, IEEE INTEL TRANSP SY, V6, P6, DOI 10.1109/MITS.2014.2336271
CR  - Burghardt T., 2004, P 5 INT PENG C USH A
CR  - Burghardt T, 2006, NEUREL 2006: EIGHT SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL ENGINEERING, PROCEEDINGS, P27
CR  - Conn JM, 2004, J SAFETY RES, V35, P571, DOI 10.1016/j.jsr.2004.10.002
CR  - CONOVER MR, 1995, WILDLIFE SOC B, V23, P407
CR  - Dey KC, 2015, IEEE T INTELL TRANSP, V16, P1107, DOI 10.1109/TITS.2014.2371455
CR  - Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155
CR  - Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
CR  - Glorot X., 2011, AISTATS, P315, DOI DOI 10.1.1.208.6449
CR  - Hedlund James H, 2004, Traffic Inj Prev, V5, P122, DOI 10.1080/15389580490435079
CR  - Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
CR  - Hothorn T, 2015, ACCIDENT ANAL PREV, V81, P143, DOI 10.1016/j.aap.2015.04.037
CR  - Hughes WE, 1996, ITE J, V66, P24
CR  - Hurney P, 2015, IET INTELL TRANSP SY, V9, P824, DOI 10.1049/iet-its.2014.0236
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Khorrami P., 2012, P WORKSH VIS OBS AN
CR  - Knapp K., 2004, MIDWEST REGIONAL
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Li FL, 2017, IET IMAGE PROCESS, V11, P833, DOI 10.1049/iet-ipr.2016.0931
CR  - Liu W., 2016, ICML 16, V21, P765
CR  - Mammeri A, 2016, IEEE T SYST MAN CY-S, V46, P1287, DOI 10.1109/TSMC.2015.2497235
CR  - Meister S. R., 2016, ROAD SAFETY MONITOR
CR  - Ministry of Home Affairs, 2007, ACC DEATHS SUIC IND
CR  - Ministry of Home Affairs, 2013, ACC DEATHS SUIC IND
CR  - Mrtka J, 2013, TRANSPORT RES D-TR E, V18, P51, DOI 10.1016/j.trd.2012.09.001
CR  - Nair V., 2010, P INT C MACH LEARN
CR  - Ouyang W., 2013, COMPUTER VISION PATT
CR  - Paisitkriangkrai S, 2008, IET COMPUT VIS, V2, P236, DOI 10.1049/iet-cvi:20080026
CR  - Putzu N, 2014, ITAL J ZOOL, V81, P463, DOI 10.1080/11250003.2014.945974
CR  - Rabinovich A., 2015, CVPR
CR  - Ramanan D, 2006, IEEE T PATTERN ANAL, V28, P1319, DOI 10.1109/TPAMI.2006.155
CR  - Ramesh T, 2012, MAMM BIOL, V77, P53, DOI 10.1016/j.mambio.2011.09.003
CR  - Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123
CR  - Sharafsaleh M. A., 2012, EVALUATION ANIMAL WA
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Wang S, 2012, P INT C IM PROC COMP, P1
CR  - Williams Allan F, 2005, Traffic Inj Prev, V6, P56, DOI 10.1080/15389580590903186
CR  - Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
CR  - Zhang L, 2007, LECT NOTES COMPUT SC, V4642, P11
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
CR  - Zhao Y, 2015, IEEE INTEL TRANSP SY, V7, P29, DOI 10.1109/MITS.2015.2427366
CR  - Zhou D., 2014, REAL TIME ANIMAL DET
PU  - SCIENCE & INFORMATION SAI ORGANIZATION LTD
PI  - WEST YORKSHIRE
PA  - 19 BOLLING RD, BRADFORD, WEST YORKSHIRE, 00000, ENGLAND
DA  - APR
PY  - 2020
VL  - 11
IS  - 4
SP  - 762
EP  - 773
AN  - WOS:000537489900099
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  43
ER  -

TY  - JOUR
AU  - Yousif, H
AU  - Yuan, JH
AU  - Kays, R
AU  - He, ZH
TI  - Object detection from dynamic scene using joint background modeling and fast deep learning classification
T2  - JOURNAL OF VISUAL COMMUNICATION AND IMAGE REPRESENTATION
LA  - English
KW  - Human-animal detection
KW  - Camera-trap images
KW  - Background subtraction
KW  - Deep convolutional neural networks
KW  - Wildlife monitoring
KW  - SEGMENTATION
AB  - In this paper, we couple effective dynamic background modeling with fast deep learning classification to develop an accurate scheme for human-animal detection from camera-trap images with cluttered moving objects. We introduce a new block-wise background model, named as Minimum Feature Difference (MFD), to model the variation of the background of the camera-trap sequences and generate the foreground object proposals. We then develop a region proposals verification to reduce the number of false alarms. Finally, we perform complexity-accuracy analysis of DCNN to construct a fast deep learning classification scheme to classify these region proposals into three categories: human, animals, and background patches. The optimized DCNN is able to maintain high level of accuracy while reducing the computational complexity by 14 times, which allows near real-time implementation of the proposed method on CPU machines. Our experimental results demonstrate that the proposed method outperforms existing methods on our and Alexander von Humboldt Institute camera-trap datasets in both foreground segmentation and object detection. (C) 2018 Elsevier Inc. All rights reserved.
AD  - Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USAAD  - North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27601 USAC3  - University of Missouri SystemC3  - University of Missouri ColumbiaC3  - University of North CarolinaC3  - North Carolina State UniversityFU  - National Science Foundation [CyberSEES-1539389]
FX  - This work has been supported in part by National Science Foundation under grant CyberSEES-1539389.
CR  - Azzam R, 2016, J VIS COMMUN IMAGE R, V36, P90, DOI 10.1016/j.jvcir.2015.11.009
CR  - Balzano L., 2010, 2010 48 ANN ALL C CO, P704
CR  - BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P293, DOI 10.1109/36.377929
CR  - Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
CR  - Bouwmans T, 2014, COMPUT VIS IMAGE UND, V122, P22, DOI 10.1016/j.cviu.2013.11.009
CR  - Bubnicki J. W., 2016, METHODS ECOL EVOL
CR  - Bunyak Filiz, 2007, Journal of Multimedia, V2, P20, DOI 10.4304/jmm.2.4.20-33
CR  - Cai ZW, 2016, LECT NOTES COMPUT SC, V9908, P354, DOI 10.1007/978-3-319-46493-0_22
CR  - Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
CR  - Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
CR  - Cheung SCS, 2005, EURASIP J APPL SIG P, V2005, P2330, DOI 10.1155/ASP.2005.2330
CR  - Choudhury S. K., 2016, EVALUATION BACKGROUN
CR  - Dalal N., 2005, IEEE COMPUTER SOC C, P886, DOI 10.1109/CVPR.2005.177
CR  - Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Gillis N, 2015, SIAM J MATRIX ANAL A, V36, P1404, DOI 10.1137/140993272
CR  - Giraldo-Zuluaga J.-H., ARXIV170108180
CR  - GIRSHICK R, 2014, PROC CVPR IEEE, P580, DOI DOI 10.1109/CVPR.2014.81
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
CR  - Hu WC, 2015, J VIS COMMUN IMAGE R, V30, P164, DOI 10.1016/j.jvcir.2015.03.003
CR  - Kays R, 2009, C LOCAL COMPUT NETW, P811, DOI 10.1109/LCN.2009.5355046
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lee K., RESIDUAL FEATURES UN
CR  - Ling Q, 2014, NEUROCOMPUTING, V133, P32, DOI 10.1016/j.neucom.2013.11.034
CR  - Liu LH, 2016, LECT NOTES COMPUT SC, V9914, P676, DOI 10.1007/978-3-319-48881-3_48
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - Lu CY, 2013, J VIS COMMUN IMAGE R, V24, P111, DOI 10.1016/j.jvcir.2012.05.003
CR  - Lucas TCD, 2015, METHODS ECOL EVOL, V6, P500, DOI 10.1111/2041-210X.12346
CR  - Ma Y., 2009, ADV NEURAL INFORM PR, P2080
CR  - Miller AB, 2017, J OUTDOOR REC TOUR, V17, P44, DOI 10.1016/j.jort.2016.09.007
CR  - Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
CR  - O'Connell A, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, pV
CR  - Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
CR  - Paragios, 2004, P 2004 IEEE COMP SOC, V2, pII
CR  - Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
CR  - Portmann J, 2014, IEEE INT CONF ROBOT, P1794, DOI 10.1109/ICRA.2014.6907094
CR  - Reddy V, 2013, IEEE T CIRC SYST VID, V23, P83, DOI 10.1109/TCSVT.2012.2203199
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Ren XB, 2013, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2013.254
CR  - Schick A., 2012, P IEEE COMP SOC C CO, V10, P27, DOI DOI 10.1109/CVPRW.2012.6238923
CR  - Shaoqing Ren R. G. J. S., ARXIV150601497
CR  - Shihao Zhang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P447, DOI 10.1109/ICMEW.2017.8026235
CR  - Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495
CR  - Sobral A, 2016, HANDBOOK OF ROBUST LOW-RANK AND SPARSE MATRIX DECOMPOSITION: APPLICATIONS IN IMAGE AND VIDEO PROCESSING
CR  - Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
CR  - Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
CR  - Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
CR  - Vedaldi A., 2008, VLFEAT OPEN PORTABLE
CR  - Wang SH, 2014, J VIS COMMUN IMAGE R, V25, P263, DOI 10.1016/j.jvcir.2013.11.005
CR  - Yeh CH, 2014, J VIS COMMUN IMAGE R, V25, P891, DOI 10.1016/j.jvcir.2014.02.012
CR  - Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
PU  - ACADEMIC PRESS INC ELSEVIER SCIENCE
PI  - SAN DIEGO
PA  - 525 B ST, STE 1900, SAN DIEGO, CA 92101-4495 USA
DA  - AUG
PY  - 2018
VL  - 55
SP  - 802
EP  - 815
DO  - 10.1016/j.jvcir.2018.08.013
AN  - WOS:000445318100071
N1  - Times Cited in Web of Science Core Collection:  5
Total Times Cited:  7
Cited Reference Count:  54
ER  -

TY  - JOUR
AU  - Vargas-Felipe, M
AU  - Pellegrin, L
AU  - Guevara-Carrizales, AA
AU  - Lopez-Monroy, AP
AU  - Escalante, HJ
AU  - Gonzalez-Fraga, JA
TI  - Desert bighorn sheep (Ovis canadensis) recognition from camera traps based on learned features
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Desert bighorn sheep
KW  - Deep learning
KW  - Convolutional neural networks
KW  - Image classification
KW  - Species recognition
KW  - Monitoring
KW  - Camera traps
KW  - ANIMALS
AB  - Monitoring wildlife in geographical areas is essential for the conservation of the biological heritage. At present, the strenuous task of animal observation on-site has been mitigated with the use of camera traps. The information gathered by these devices comprises a sequence of images, which are triggered by motion or heat, and enables the monitoring of several geographical areas at the same time without perturbing the fauna. Notwithstanding the advances of these kind of monitoring approaches, the captured images still must be manually classified, becoming into an expensive process. This paper describes an automatic methodology for labeling images captured with camera traps as a support tool for animal behaviour analysis. Specifically, we focus on the analysis of the Ovis canadensis better known as desert bighorn sheep, a species that inhabits in northwestern Mexico, USA and Canada. The importance of this species lies in that it is an emblematic one, of great historical, cultural and social value. We adopted a methodology based on a residual neural network (ResNet) as feature extractor and standard models for the classification of images depicting species of interest. The method is built (trained) and evaluated on realistic images captured by camera traps in the field. We achieve classification performances ranging from 89% for a multiclass classification setting (7 classes associated to the animal of interest) to 99% in a binary classification scenario (presence vs. absence of the species). The collected data set, model and extracted features are publicly available under request. We foresee the released data set and the proposed solution will boost research on the analysis of this species.
AD  - Univ Autonoma Baja California UABC, Fac Sci, Ensenada, Baja California, MexicoAD  - Ctr Invest Matemat CIMAT AC, Comp Sci Dept, Guanajuato, MexicoAD  - Inst Nacl Astrofis Opt & Elect INAOE, Comp Sci Dept, Cholula, MexicoC3  - CIMAT - Centro de Investigacion en MatematicasC3  - Instituto Nacional de Astrofisica, Optica y ElectronicaFU  - PRODEP [UABC-PTC-792]
FX  - The authors thank to Director of APFF'Valle de los Cirios'Biol. Victor Gelasio Sanchez Sotomayor for his kind attention and offers facilities to this project. Also, we thank to Dr. Guillermo Romero for his help during the data set labeling. This work was supported by PRODEP, grant UABC-PTC-792.
CR  - AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470
CR  - Beery S., 2019, ABS190707617 CORR
CR  - Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
CR  - Buehler P, 2019, ECOL INFORM, V50, P191, DOI 10.1016/j.ecoinf.2019.02.003
CR  - Carl C, 2020, EUR J WILDLIFE RES, V66, DOI 10.1007/s10344-020-01404-y
CR  - Cheema GS, 2017, LECT NOTES ARTIF INT, V10536, P27, DOI 10.1007/978-3-319-71273-4_3
CR  - CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
CR  - Escobar-Flores Jonathan G., 2015, Therya, V6, P519, DOI 10.12933/therya-15-284
CR  - Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
CR  - Fan RE, 2008, J MACH LEARN RES, V9, P1871
CR  - Gobierno del Estado de Baja California, 2012, ESTRATEGIA ESTATAL C
CR  - Gomez Alexander, 2016, Advances in Visual Computing. 12th International Symposium, ISVC 2016. Proceedings: LNCS 10072, P747, DOI 10.1007/978-3-319-50835-1_67
CR  - Guevara-Carrizales AA, 2016, RIQUEZA CONSERVACION, P63
CR  - Hall M., 2009, ACM SIGKDD EXPLORATI, V11, P10, DOI [DOI 10.1145/1656274.1656278, 10.1145/1656274.1656278]
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - John G. H., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P338
CR  - Korschens M., 2018, ICEI 2018 10 INT C E, DOI [10.22032/dbt.37903, DOI 10.22032/DBT.37903]
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Lee R., 2013, DESERT BIGHORN COUNC, V52, P40
CR  - Lee R., 2011, DESERT BIGHORN COUNC, V51, P46
CR  - Long J., 2014, P INT C NEUR INF PRO, P1601
CR  - Manterola-y-Pina C., 2000, TECHNICAL REPORT UNI
CR  - Martinez-Gallardo R., 2017, ESTUDIOS BORREGO CIM
CR  - Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
CR  - Mitchell TM., 1997, MACH LEARN, V45, P81
CR  - Monson G., 1980, DESERT BIGHORN ITS L
CR  - Montoya M., 2017, ESTUDIOS BORREGO CIM
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Quinlan J. R., 1993, C4 5 PROGRAMS MACHIN
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Ruiz E., 2014, THESIS U NACL AUTONO
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
CR  - Simonyan K., 2015, P INT C LEARN REPR I
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Timm M, 2018, IEEE COMPUT SOC CONF, P1977, DOI 10.1109/CVPRW.2018.00252
CR  - Valdez R., 2018, THERYA, V9, P219
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Yousif H, 2019, ECOL EVOL, V9, P1578, DOI 10.1002/ece3.4747
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - SEP
PY  - 2021
VL  - 64
DO  - 10.1016/j.ecoinf.2021.101328
AN  - WOS:000691763100004
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  46
ER  -

TY  - JOUR
AU  - Wu, SH
AU  - Chang, HW
AU  - Lin, RS
AU  - Tuanmu, MN
TI  - SILIC: A cross database framework for automatically extracting robust biodiversity information from soundscape recordings based on object detection and a tiny training dataset
T2  - ECOLOGICAL INFORMATICS
LA  - English
KW  - Sound Identification and Labeling Intelligence for Creatures
KW  - Automated wildlife sound identification
KW  - Passive acoustic monitoring
KW  - Autonomous recording unit
KW  - Object detection
KW  - DATA AUGMENTATION
KW  - BIRD-SONG
AB  - 1. Passive acoustic monitoring (PAM) offers many advantages comparing with other survey methods and gains an increasing use in terrestrial ecology, but the massive effort needed to extract species information from a large number of recordings limits its application. The convolutional neural network (CNN) has been demonstrated with its high performance and effectiveness in identifying sound sources automatically. However, requiring a large amount of training data still constitutes a challenge.
   2. Object detection is used to detect multiple objects in photos or videos and is effective at detecting small objects in a complex context, such as animal sounds in a spectrogram and shows the opportunity to build a good performance model with a small training dataset. Therefore, we developed the Sound Identification and Labeling Intelligence for Creatures (SILIC), which integrates online animal sound databases, PAM databases and an object detection-based model, for extracting information on the sounds of multiple species from complex soundscape recordings.
   3. We used the sounds of six owl species in Taiwan to demonstrate the effectiveness, efficiency and application potential of the SILIC framework. Using only 786 sound labels in 133 recordings, our model successfully identified the species' sounds from the recordings collected at five PAM stations, with a macro-average AUC of 0.89 and a mAP of 0.83. The model also provided the time and frequency information, such as the duration and bandwidth, of the sounds.
   4. To our best knowledge, this is the first time that the object detection algorithm has been used to identify sounds of multiple wildlife species. With an online sound-labeling platform embedded and a novel data preprocessing approach (i.e., rainbow mapping) applied, the SILIC shows its good performance and high efficiency in identifying wildlife sounds and extracting robust species, time and frequency information from a massive amount of soundscape recordings based on a tiny training dataset acquired from existing animal sound databases. The SILIC can help expand the application of PAM as a tool to evaluate the state of and detect the change in biodiversity by, for example, providing high temporal resolution and continuous information on species presence across a monitoring network.
AD  - Natl Sun Yat Sen Univ, Dept Biol Sci, Kaohsiung 804, TaiwanAD  - Endem Species Res Inst, Nantou 552, TaiwanAD  - Acad Sinica, Biodivers Res Ctr, Taipei 115, TaiwanC3  - National Sun Yat Sen UniversityC3  - Academia Sinica - TaiwanFU  - Academia Sinica Grid Computing Centre (ASGC) , Institute of Physics and Biodiversity Research Center, Academia Sinica, Taiwan; Council of Agriculture, Executive Yuan, R.O.C. (Taiwan); Biodiversity Research Center, Academia Sinica
FX  - We thank Jun-Yi Wu, Li-Chung Lu, Yi-Wei Lu, Wen-Chi Lin, Yong-Lun Lin, Hsin Tang, Chia-Hao Chang, Ta-chih Chen, Ji-En Shie, Yi-Hua Hsieh for assistance with sound labeling and review. We thank Chie-Jen Ko and Cheng-hsiung Yang for the help on animal sound identifi-cation. We thank Yu-Cheng Hsu and Chia-Yun Lee for suppling the soundscape recordings and Matt Medler for suppling the sound collection of Macaulay Library. We acknowledge the Academia Sinica Grid Computing Centre (ASGC) , Institute of Physics and Biodiversity Research Center, Academia Sinica, Taiwan for supporting the SILIC server. We are also extremely grateful to Wen-Chi Lin for helping the design of sound-labeling platform. Funding for the project SILIC was provided by Council of Agriculture, Executive Yuan, R.O.C. (Taiwan) and Biodiversity Research Center, Academia Sinica.
CR  - Abrahams C, 2020, ECOL INDIC, V112, DOI 10.1016/j.ecolind.2020.106131
CR  - Askeyev A., 2019, European Journal of Ecology, V5, P8, DOI 10.2478/eje-2019-0015
CR  - BAKER MC, 1985, BEHAV BRAIN SCI, V8, P85, DOI 10.1017/S0140525X00019750
CR  - Barre K, 2019, METHODS ECOL EVOL, V10, P1171, DOI 10.1111/2041-210X.13198
CR  - Bellisario KM, 2019, ECOL INFORM, V51, P96, DOI 10.1016/j.ecoinf.2019.02.009
CR  - Blake JG, 2021, PEERJ, V9, DOI 10.7717/peerj.10565
CR  - Browning E., 2017, PASSIVE ACOUSTIC MON
CR  - Brownlie KC, 2020, EMU, V120, P123, DOI 10.1080/01584197.2020.1732828
CR  - Campos-Cerqueira M., 2019, REMOTE SENSING ECOLO
CR  - Concepcion C.B., 2018, BIRDS PREY BULL, P395
CR  - Crunchant AS, 2020, METHODS ECOL EVOL, V11, P542, DOI 10.1111/2041-210X.13362
CR  - Dai YS, 2021, ELECTRON LETT, V57, P454, DOI 10.1049/ell2.12160
CR  - Darras K, 2019, ECOL APPL, V29, DOI 10.1002/eap.1954
CR  - Dent JM, 2016, EMU, V116, P315, DOI 10.1071/MU15079
CR  - Desjonqueres C, 2020, FRESHWATER BIOL, V65, P7, DOI 10.1111/fwb.13356
CR  - Dhillon A, 2020, PROG ARTIF INTELL, V9, P85, DOI 10.1007/s13748-019-00203-0
CR  - Dobbins M, 2020, J APPL ECOL, V57, P2100, DOI 10.1111/1365-2664.13750
CR  - Doser JW, 2021, METHODS ECOL EVOL, V12, P1040, DOI 10.1111/2041-210X.13578
CR  - Duchac LS, 2020, CONDOR, V122, DOI 10.1093/condor/duaa017
CR  - Ducrettet M, 2020, BIOL CONSERV, V245, DOI 10.1016/j.biocon.2020.108574
CR  - Ehnes M, 2015, BIOACOUSTICS, V24, P111, DOI 10.1080/09524622.2014.994228
CR  - Enari H, 2019, ECOL INDIC, V98, P753, DOI 10.1016/j.ecolind.2018.11.062
CR  - Fanioudakis L., 2017, ARXIV 171104347
CR  - Ferreira AC, 2020, METHODS ECOL EVOL, V11, P1072, DOI 10.1111/2041-210X.13436
CR  - Furnas BJ, 2020, BIOL CONSERV, V241, DOI 10.1016/j.biocon.2019.108347
CR  - Gibb R, 2019, METHODS ECOL EVOL, V10, P169, DOI 10.1111/2041-210X.13101
CR  - Hagens SV, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0199396
CR  - Hao ZZ, 2021, URBAN FOR URBAN GREE, V57, DOI 10.1016/j.ufug.2020.126822
CR  - Jocher G, 2020, ULTRALYTICSYOLOV5 V3
CR  - Joly A., 2020, LECT NOTES COMPUTER
CR  - Kahl S., 2020, CLEF 2020
CR  - Kahl S., 2017, CLEF 2017
CR  - Kahl S, 2021, ECOL INFORM, V61, DOI 10.1016/j.ecoinf.2021.101236
CR  - Kalan AK, 2016, FRONT ZOOL, V13, DOI 10.1186/s12983-016-0167-8
CR  - Ko T, 2017, INT CONF ACOUST SPEE, P5220, DOI 10.1109/ICASSP.2017.7953152
CR  - Kucuktopcu O, 2019, APPL ACOUST, V148, P194, DOI 10.1016/j.apacoust.2018.12.028
CR  - Laiolo P, 2010, BIOL CONSERV, V143, P1635, DOI 10.1016/j.biocon.2010.03.025
CR  - Lasseck M., 2019, CLEF WORKING NOTES, V2380
CR  - LeBien J, 2020, ECOL INFORM, V59, DOI 10.1016/j.ecoinf.2020.101113
CR  - Leseberg NP, 2020, METHODS ECOL EVOL, V11, P1520, DOI 10.1111/2041-210X.13475
CR  - Li SW, 2021, CONSTR BUILD MATER, V273, DOI 10.1016/j.conbuildmat.2020.121949
CR  - Lin TH, 2017, PROCEEDINGS OF THE 2017 PACIFIC NEIGHBORHOOD CONSORTIUM ANNUAL CONFERENCE AND JOINT MEETINGS (PNC), P128, DOI 10.23919/PNC.2017.8203533
CR  - Marcus G., 2018, DEEP LEARNING CRITIC
CR  - Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
CR  - Nanni L., Ecol. Inform., V57
CR  - Nanni L, 2020, EURASIP J AUDIO SPEE, V2020, DOI 10.1186/s13636-020-00175-3
CR  - Pak M., 2017, P1
CR  - Pandeya YR, 2020, IEEE ACCESS, V8, P162625, DOI 10.1109/ACCESS.2020.3022058
CR  - Perez-Granados C, 2021, STUD NEOTROP FAUNA E, DOI 10.1080/01650521.2021.1933699
CR  - Perez-Granados C, 2021, IBIS, V163, P765, DOI 10.1111/ibi.12944
CR  - Perez-Granados C, 2021, AM J PRIMATOL, V83, DOI 10.1002/ajp.23241
CR  - Perez-Granados C, 2019, ECOL INDIC, V107, DOI 10.1016/j.ecolind.2019.105608
CR  - Pijanowski BC, 2011, BIOSCIENCE, V61, P203, DOI 10.1525/bio.2011.61.3.6
CR  - Priyadarshani N, 2018, J AVIAN BIOL, V49, DOI 10.1111/jav.01447
CR  - Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/neco_a_00990, 10.1162/NECO_a_00990]
CR  - Redmon J., 2018, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2017.690
CR  - RICHARDS DG, 1980, AM NAT, V115, P381, DOI 10.1086/283568
CR  - Ruff ZJ, 2021, ECOL INDIC, V124, DOI 10.1016/j.ecolind.2021.107419
CR  - Rusin IY, 2019, NAT CONSERV RES, V4, P34, DOI 10.24189/ncr.2019.039
CR  - Segal Y., 2019, SPEECHYOLO DETECTION, V1904
CR  - Shamon H., Ecol. Indic., V120, P2021
CR  - Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
CR  - Shrestha R, 2021, LECT NOTES COMPUT SC, V12891, P415, DOI 10.1007/978-3-030-86362-3_34
CR  - Smith DG, 2020, REMOTE SENS ECOL CON, V6, P286, DOI 10.1002/rse2.173
CR  - Sprengel E., 2016, WORKING NOTES CLEF 2, P547
CR  - Sturley S., 2020, 2020 1 INT C INN RES, P1
CR  - Szymanski P, 2021, ECOL INDIC, V122, DOI 10.1016/j.ecolind.2020.107271
CR  - Terry Andrew M.R., 2005, Frontiers in Zoology, V2, P1
CR  - Thanapol Panissara, 2020, 2020 5th International Conference on Information Technology (InCIT), P300, DOI 10.1109/InCIT50588.2020.9310787
CR  - Thuan D., 2021, EVOLUTION YOLO ALGOR
CR  - Tsai PY, 2020, BIODIVERS DATA J, V8, DOI 10.3897/BDJ.8.e49735
CR  - Tuncer T, 2021, APPL ACOUST, V176, DOI 10.1016/j.apacoust.2020.107866
CR  - Tuneu-Corral C, 2020, ECOL INDIC, V110, DOI 10.1016/j.ecolind.2019.105849
CR  - Venkatesh S., 2021, ARXIV 210900962
CR  - Vidana-Vila E, 2020, APPL ACOUST, V166, DOI 10.1016/j.apacoust.2020.107312
CR  - Wallis D., 2020, BIOACOUSTICS, P1
CR  - Whelan CJ, 2008, ANN NY ACAD SCI, V1134, P25, DOI 10.1196/annals.1439.003
CR  - Wood CM, 2021, CONSERV BIOL, V35, P336, DOI 10.1111/cobi.13516
CR  - Wood CM, 2019, ECOLOGY, V100, DOI 10.1002/ecy.2764
CR  - Xie J, 2019, IEEE ACCESS, V7, P175353, DOI 10.1109/ACCESS.2019.2957572
CR  - Xie J, 2019, ECOL INFORM, V52, P74, DOI 10.1016/j.ecoinf.2019.05.007
CR  - Xu WT, 2020, AD HOC NETW, V102, DOI 10.1016/j.adhoc.2020.102115
CR  - Yip DA, 2020, REMOTE SENS ECOL CON, V6, P301, DOI 10.1002/rse2.118
CR  - Yoo S, 2020, IBIS, V162, P1001, DOI 10.1111/ibi.12741
CR  - Zhang X, 2019, ECOL INFORM, V54, DOI 10.1016/j.ecoinf.2019.101009
CR  - Zhao B, 2017, INT J AUTOM COMPUT, V14, P119, DOI 10.1007/s11633-017-1053-3
CR  - Zsebok Sandor, 2019, ORNIS HUNGARICA, V27, P59, DOI 10.2478/orhu-2019-0015
CR  - Zwart MC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102770
PU  - ELSEVIER
PI  - AMSTERDAM
PA  - RADARWEG 29, 1043 NX AMSTERDAM, NETHERLANDS
DA  - MAY
PY  - 2022
VL  - 68
DO  - 10.1016/j.ecoinf.2021.101534
AN  - WOS:000792134500008
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  88
ER  -

TY  - JOUR
AU  - Borowicz, A
AU  - Lynch, HJ
AU  - Estro, T
AU  - Foley, C
AU  - Goncalves, B
AU  - Herman, KB
AU  - Adamczak, SK
AU  - Stirling, I
AU  - Thorne, L
TI  - Social Sensors for Wildlife : Ecological Opportunities in the Era of Camera Ubiquity
T2  - FRONTIERS IN MARINE SCIENCE
LA  - English
KW  - tourism
KW  - IAATO
KW  - Antarctic Peninsula
KW  - Weddell seal
KW  - social media
KW  - citizen science
KW  - community science
KW  - citizen sensors
KW  - WEDDELL SEALS
KW  - MCMURDO SOUND
KW  - POPULATION
KW  - PHENOLOGY
KW  - MOVEMENTS
KW  - ABUNDANCE
KW  - SURVIVAL
AB  - Expansive study areas, such as those used by highly-mobile species, provide numerous logistical challenges for researchers. Community science initiatives have been proposed as a means of overcoming some of these challenges but often suffer from low uptake or limited long-term participation rates. Nevertheless, there are many places where the public has a much higher visitation rate than do field researchers. Here we demonstrate a passive means of collecting community science data by sourcing ecological image data from the digital public, who act as "eco-social sensors," via a public photo-sharing platform-Flickr. To achieve this, we use freely-available Python packages and simple applications of convolutional neural networks. Using the Weddell seal (Leptonychotes weddellii) on the Antarctic Peninsula as an example, we use these data with field survey data to demonstrate the viability of photo-identification for this species, supplement traditional field studies to better understand patterns of habitat use, describe spatial and sex-specific signals in molt phenology, and examine behavioral differences between the Antarctic Peninsula's Weddell seal population and better-studied populations in the species' more southerly fast-ice habitat. While our analyses are unavoidably limited by the relatively small volume of imagery currently available, this pilot study demonstrates the utility an eco-social sensors approach, the value of ad hoc wildlife photography, the role of geographic metadata for the incorporation of such imagery into ecological analyses, the remaining challenges of computer vision for ecological applications, and the viability of pelage patterns for use in individual recognition for this species.
AD  - SUNY Stony Brook, Dept Ecol & Evolut, Stony Brook, NY 11794 USAAD  - SUNY Stony Brook, Inst Adv Computat Sci, Stony Brook, NY 11794 USAAD  - SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USAAD  - Univ Hawaii, Hawai Inst Marine Biol, Kaneohe, HI USAAD  - Georgia Aquarium, Atlanta, GA USAAD  - Univ Calif Santa Cruz, Dept Ecol & Evolutionary Biol, Santa Cruz, CA 95064 USAAD  - Univ Alberta, Dept Biol Sci, Edmonton, AB, CanadaAD  - SUNY Stony Brook, Sch Marine & Atmospher Sci, Stony Brook, NY 11794 USAC3  - State University of New York (SUNY) SystemC3  - State University of New York (SUNY) Stony BrookC3  - State University of New York (SUNY) SystemC3  - State University of New York (SUNY) Stony BrookC3  - State University of New York (SUNY) SystemC3  - State University of New York (SUNY) Stony BrookC3  - University of Hawaii SystemC3  - University of California SystemC3  - University of California Santa CruzC3  - University of AlbertaC3  - State University of New York (SUNY) SystemC3  - State University of New York (SUNY) Stony BrookFU  -  [1531492]
FX  - We gratefully acknowledge the assistance of members of the Oceanites field team for collecting photographs; numerous expedition guides for submitting photographs; Nicole Cassale, Emily Enzinger, and Yanbing Gu for matching assistance; Ted Cheeseman for gathering photographs in the runup to HappyWhale; and computational time from the SeaWulf cluster at the Institute of Advanced Computational Science (NSF award #1531492) .
CR  - Aslam S, 2020, INSTAGRAM NUMBERS ST
CR  - Baruh L, 2017, NEW MEDIA SOC, V19, P579, DOI 10.1177/1461444815614001
CR  - Beltran R. S, 2018, BRIDGING GAP PUPPING
CR  - Beltran RS, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-41635-x
CR  - Bigg M., 1982, REP INT WHAL COMM, V36, P655
CR  - Bonenfant C, 2004, P ROY SOC B-BIOL SCI, V271, P883, DOI 10.1098/rspb.2003.2661
CR  - Burns JM, 1999, POLAR BIOL, V21, P23, DOI 10.1007/s003000050329
CR  - Burton R, 2015, POLAR REC, V51, P667, DOI 10.1017/S0032247414000953
CR  - Calambokidis J, 2001, MAR MAMMAL SCI, V17, P769, DOI 10.1111/j.1748-7692.2001.tb01298.x
CR  - Cameron MF, 2004, CAN J ZOOL, V82, P601, DOI 10.1139/Z04-025
CR  - Castellini M. A., 1992, B SCRIPPS I OCEANOGR, V28, P1
CR  - Cheeseman T, 2017, REPORT NO SC67APH02
CR  - Chen K, 2017, ICRAWLER VERSION 063
CR  - Cronje R., 2011, APPL ENV ED COMMUNIC, V10, P135, DOI [10.1080/1533015X.2011.603611., DOI 10.1080/1533015X.2011, https://doi.org/10.1080/1533015X.2011.603611, DOI 10.1080/1533015X.2011.603611]
CR  - CROXALL JP, 1983, J APPL ECOL, V20, P19, DOI 10.2307/2403373
CR  - Dean AJ, 2018, J ENVIRON MANAGE, V213, P409, DOI 10.1016/j.jenvman.2018.02.080
CR  - Forcada J, 2006, POLAR BIOL, V29, P1052, DOI 10.1007/s00300-006-0149-y
CR  - Gailey G, 2012, DISCOVERY PHOTOIDENT
CR  - Gil-Delgado JA, 2013, POLAR BIOL, V36, P607, DOI 10.1007/s00300-012-1280-6
CR  - Haywood BK, 2016, INT J SCI EDUC PART, V6, P239, DOI 10.1080/21548455.2015.1043659
CR  - He K., 2015, CVPR
CR  - Herfindal I, 2006, OECOLOGIA, V150, P213, DOI 10.1007/s00442-006-0519-8
CR  - Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
CR  - Hucke-Gaete R, 2004, POLAR BIOL, V27, P304, DOI 10.1007/s00300-003-0587-8
CR  - Huckstadt LA, 2017, P ROY SOC B-BIOL SCI, V284, DOI 10.1098/rspb.2017.0927
CR  - Humphries GRW, 2017, POLAR REC, V53, P160, DOI 10.1017/S0032247417000055
CR  - Jordan RC, 2011, CONSERV BIOL, V25, P1148, DOI 10.1111/j.1523-1739.2011.01745.x
CR  - Kaschner K, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0044075
CR  - Katona S.K., 1981, Polar Record, V20, P439
CR  - Kelly MJ, 2001, J MAMMAL, V82, P440, DOI 10.1644/1545-1542(2001)082<0440:CAPMIS>2.0.CO;2
CR  - Koivuniemi M, 2016, ENDANGER SPECIES RES, V30, P29, DOI 10.3354/esr00723
CR  - Kosmala M, 2016, FRONT ECOL ENVIRON, V14, P551, DOI 10.1002/fee.1436
CR  - LaRue MA, 2020, REMOTE SENS ECOL CON, V6, P70, DOI 10.1002/rse2.124
CR  - LaRue MA, 2019, MAR ECOL PROG SER, V612, P193, DOI 10.3354/meps12877
CR  - Lodi L, 2018, OCEAN COAST MANAGE, V158, P45, DOI 10.1016/j.ocecoaman.2018.03.029
CR  - Lynch HJ, 2012, MAR ECOL PROG SER, V454, P135, DOI 10.3354/meps09252
CR  - McLean BS, 2021, ECOLOGY, V102, DOI 10.1002/ecy.3258
CR  - Naveen R., 2011, ANTARCTIC PENINSULA
CR  - Nov, 2011, P 2011 ICONFERENCE, P68, DOI [DOI 10.1145/1940761.1940771, 10.1145/1940761.1940771]
CR  - Paszke A, 2019, ADV NEUR IN, V32
CR  - Paterson WD, 2013, MAR MAMMAL SCI, V29, pE537, DOI 10.1111/mms.12043
CR  - Poisson AC, 2020, FRONT ECOL ENVIRON, V18, P19, DOI 10.1002/fee.2128
CR  - Runge CA, 2014, FRONT ECOL ENVIRON, V12, P395, DOI 10.1890/130237
CR  - Santora JA, 2013, MAR BIOL, V160, P1383, DOI 10.1007/s00227-013-2190-z
CR  - Sequeira AMM, 2019, ECOL APPL, V29, DOI 10.1002/eap.1947
CR  - Siniff DB, 2008, ANTARCT SCI, V20, P425, DOI 10.1017/S0954102008001351
CR  - Smith TD, 1999, MAR MAMMAL SCI, V15, P1, DOI 10.1111/j.1748-7692.1999.tb00779.x
CR  - Speed Conrad W., 2007, Frontiers in Zoology, V4, P1
CR  - STIRLING I, 1969, ECOLOGY, V50, P573, DOI 10.2307/1936247
CR  - Stirling I., 1977, ADAPTATIONS ANTARCTI
CR  - TESTA JW, 1987, ECOL MONOGR, V57, P149, DOI 10.2307/1942622
CR  - Thornley-Brown A, 2019, SOCIAL MEDIA PHOTOSH
CR  - VAUGHAN R. W., 1968, BRIT ANTARCTIC SURV BULL, V15., P71
CR  - Walcott SM, 2020, CONSERV PHYSIOL, V8, DOI 10.1093/conphys/coaa022
CR  - Yochem PK., 1990, REP INT WHALING COMM, V12, P87, DOI [10.1097/00002727- 199003000-00014, DOI 10.1097/00002727-199003000-00014]
PU  - FRONTIERS MEDIA SA
PI  - LAUSANNE
PA  - AVENUE DU TRIBUNAL FEDERAL 34, LAUSANNE, CH-1015, SWITZERLAND
DA  - MAY 26
PY  - 2021
VL  - 8
DO  - 10.3389/fmars.2021.645288
AN  - WOS:000657768500001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  55
ER  -

TY  - JOUR
AU  - Gupta, S
AU  - Mohan, N
AU  - Nayak, P
AU  - Nagaraju, KC
AU  - Karanam, M
TI  - Deep vision-based surveillance system to prevent train-elephant collisions
T2  - SOFT COMPUTING
LA  - English
KW  - Human-elephant collision
KW  - Rail track monitoring
KW  - Deep vision
KW  - Data augmentation
KW  - Transfer learning
KW  - DETECT ELEPHANTS
KW  - ANIMALS
AB  - Animal conservation is imperative, and technology can certainly assist in different ways. The extinction of endangered species like tigers and elephants has boosted the necessity for such efforts. Human-elephant collision (HEC) has been an active area of research for years. Apart from deforestation, the roads and rail tracks laid down through forest areas intervene a lot in wildlife. Collisions and tragedies are every day, especially in green belts in India and other Asian countries. Therefore, it is crucial to develop vision-based, automated, warning-generating systems to identify the animal/elephant near-site. In the proposed work, different deep learning-based models are proposed to identify elephants in image/video. Several convolutional neural network (CNN)-based models and three transfer learning (TL)-based models, i.e., ResNet50, MobileNet, Inception V3, have been experimented with and tuned for elephant detection. All the models are tested on a synthesized dataset having about 4200 images built using two public datasets, i.e., ELPephant and RailSem19. Two accurate CNN and transfer learning-based models are presented in detail. These highly accurate and precise models can alarm the trains and generate warning signals on site. The proposed CNN and inception network demonstrated high accuracy of 99.53% and 99.91%, respectively, and are remarkable in identifying elephants and hence preventing HEC. The same model can be trained for other animals for their preservation in similar scenarios.
AD  - Punjab Agr Univ, COAET, Dept Elect Engn & IT, Ludhiana, Punjab, IndiaAD  - IK Gujral Punjab Tech Univ, Dept CSE, Mohali, IndiaAD  - Gokaraju Rangaraju Inst Engn & Technol, Dept CSE, Hyderabad, IndiaC3  - Punjab Agricultural UniversityC3  - I. K. Gujral Punjab Technical UniversityC3  - Gokaraju Rangaraju Institute of Engineering & TechnologyCR  - Ardovini A, 2008, PATTERN RECOGN, V41, P1867, DOI 10.1016/j.patcog.2007.11.010
CR  - Backs JAJ, 2017, ECOL ENG, V106, P563, DOI 10.1016/j.ecoleng.2017.06.024
CR  - Banupriya N., 2020, J CRIT REV, V7, P434
CR  - Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
CR  - Berteaux, 2019, FOX MASK NEW AUTOATE
CR  - Bil M, 2019, J ENVIRON MANAGE, V237, P297, DOI 10.1016/j.jenvman.2019.02.076
CR  - Burghardt T, 2006, IEE P-VIS IMAGE SIGN, V153, P305, DOI 10.1049/ip-vis:20050052
CR  - Chen GB, 2014, IEEE IMAGE PROC, P858, DOI 10.1109/ICIP.2014.7025172
CR  - Dhanaraj JSA, 2021, J EXP THEOR ARTIF IN, V33, P561, DOI 10.1080/0952813X.2018.1552316
CR  - Farah R., 2011, 2011 IEEE International Symposium on Robotic and Sensors Environments (ROSE 2011), P65, DOI 10.1109/ROSE.2011.6058509
CR  - Gadekallu TR, 2021, COMPLEX INTELL SYST, V7, P1855, DOI 10.1007/s40747-021-00324-x
CR  - Gadekallu TR, 2020, J AMB INTEL HUM COMP, DOI 10.1007/s12652-020-01963-7
CR  - Gibson, 2005, SEGMENTING QUADRUPED
CR  - Gupta S, 2019, MULTIMED TOOLS APPL, V78, P34157, DOI 10.1007/s11042-019-08232-6
CR  - Howard A. G., 2017, ARXIV
CR  - Ibrahim, 2012, INT J FUTURE COMPUT, V1, P24, DOI 10.7763/IJFCC.2012.V1.7
CR  - Iwendi C, 2020, MULTIMEDIA SYST, DOI 10.1007/s00530-020-00701-5
CR  - Jeeva MS, 2018, PREVENTION, V5
CR  - Joubert, 2019, P IEEE C COMP VIS PA, P48
CR  - Kellenberger B, 2019, IEEE T GEOSCI REMOTE, V57, P9524, DOI 10.1109/TGRS.2019.2927393
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Kellenberger B, 2017, INT GEOSCI REMOTE SE, P866, DOI 10.1109/IGARSS.2017.8127090
CR  - Korschens M, 2019, IEEE INT CONF COMP V, P263, DOI 10.1109/ICCVW.2019.00035
CR  - Kumar M, 2019, IEEE ACCESS, V7, P163912, DOI 10.1109/ACCESS.2019.2952176
CR  - Langbein, 2011, 20113 DEER IN
CR  - Mammeri A, 2014, IEEE ICC, P1854, DOI 10.1109/ICC.2014.6883593
CR  - Mandal RK, 2018, ADV INTELL SYST, V706, P1, DOI 10.1007/978-981-10-8237-5_1
CR  - Marais, 2018, THESIS STELLENBOSCH
CR  - Monck H.J., 2018, ARXIV PREPRINT ARXIV
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Palaniappan, 2018, 2018 IEEE APPL IM PA, P1, DOI [DOI 10.1109/AIPR.2018.8707411, 10.1109/AIPR.2018.8707411]
CR  - Panwar, 2017, INT J FUTURE REVOLUT, V3, P33
CR  - Potluri VP, 2019, INT J COMPUT SCI MOB, V8, P104
CR  - Praczyk T, 2020, SOFT COMPUT, V24, P1315, DOI 10.1007/s00500-019-03969-6
CR  - Ramanan D, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P338
CR  - Ramesh G, 2020, SPRING P 2 INT C INN
CR  - Ravikumar S, 2020, J INTELL FUZZY SYST, V38, P6291, DOI 10.3233/JIFS-179710
CR  - Redmon J., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2017.690
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Rey N, 2017, REMOTE SENS ENVIRON, V200, P341, DOI 10.1016/j.rse.2017.08.026
CR  - Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
CR  - Sharkawy AN, 2020, SOFT COMPUT, V24, P6687, DOI 10.1007/s00500-019-04306-7
CR  - Sharma SU, 2017, IEEE ACCESS, V5, P347, DOI 10.1109/ACCESS.2016.2642981
CR  - Shukla P, 2017, IEEE INT CONF COMP V, P2883, DOI 10.1109/ICCVW.2017.340
CR  - Sugumar SJ, 2014, SCI WORLD J, DOI 10.1155/2014/393958
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Tuia, 2018, JOINT EUR C MACH LEA, P630
CR  - Tweed D, 2002, INT C PATT RECOG, P24, DOI 10.1109/ICPR.2002.1048227
CR  - Venkataraman AB, 2005, CURR SCI INDIA, V88, P1827
CR  - Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
CR  - Woods, 2014, ANAL RISK AN LIN
CR  - Zendel O, 2019, IEEE COMPUT SOC CONF, P1221, DOI 10.1109/CVPRW.2019.00161
CR  - Zeppelzauer Matthias, 2015, BMC Research Notes, V8, P409, DOI 10.1186/s13104-015-1370-y
CR  - Zeppelzauer M, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-46
CR  - Zotin AG, 2019, INT ARCH PHOTOGRAMM, V42-2, P249, DOI 10.5194/isprs-archives-XLII-2-W12-249-2019
PU  - SPRINGER
PI  - NEW YORK
PA  - ONE NEW YORK PLAZA, SUITE 4600, NEW YORK, NY, UNITED STATES
DA  - APR
PY  - 2022
VL  - 26
IS  - 8
SP  - 4005
EP  - 4018
DO  - 10.1007/s00500-021-06493-8
AN  - WOS:000717919900001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  55
ER  -

TY  - CPAPER
AU  - Thompson, P
AU  - Brink, W
A1  - IEEE Comp Soc
TI  - Image identification of Protea species with attributes and subgenus scaling
T2  - 2020 IEEE WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION (WACV)
LA  - English
CP  - IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)
KW  - BIODIVERSITY HOTSPOT
AB  - The flowering plant genus Protea is a dominant representative for the biodiversity of the Cape Floristic Region in South Africa, and from a conservation point of view important to monitor. The recent surge in popularity of crowd-sourced wildlife monitoring platforms presents both challenges and opportunities for automatic image based species identification. We consider the problem of identifying the Protea species in a given image with additional (but optional) attributes linked to the observation, such as location and date. We collect training and test data from a crowd-sourced platform, and find that the Protea identification problem is exacerbated by considerable inter-class similarity, data scarcity, class imbalance, as well as large variations in image quality, composition and background. Our proposed solution consists of three parts. The first part incorporates a variant of multi-region attention into a pre-trained convolutional neural network, to focus on the flower-head in the image. The second part performs coarser-grained classification on subgenera (superclasses) and then rescales the output of the first part. The third part conditions a probabilistic model on the additional attributes associated with the observation. We perform an ablation study on the proposed model and its constituents, and find that all three components together outperform our baselines and all other variants quite significantly.
AD  - Stellenbosch Univ, Stellenbosch, South AfricaC3  - Stellenbosch UniversityCR  - Apriyanti DH, 2013, 2013 INTERNATIONAL CONFERENCE ON COMPUTER, CONTROL, INFORMATICS AND ITS APPLICATIONS (IC3INA), P53, DOI 10.1109/IC3INA.2013.6819148
CR  - Barre P, 2017, ECOL INFORM, V40, P50, DOI 10.1016/j.ecoinf.2017.05.005
CR  - Beery S., 2019, ARXIV190405986
CR  - Berg T, 2014, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2014.259
CR  - Chandler M., 2017, GEO HDB BIODIVERSITY, DOI [10.1007/978-3-319-27288-7_9, DOI 10.1007/978-3-319-27288-7_9]
CR  - Chaoyun Zhang, 2015, 2015 IEEE International Conferences on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; and Pervasive Intelligence and Computing (CIT/IUCC/DASC/PICOM). Proceedings, P2143, DOI 10.1109/CIT/IUCC/DASC/PICOM.2015.318
CR  - Cho SY, 2006, INT C PATT RECOG, P1038
CR  - Cho S, 2012, PROCEEDINGS OF THE 3RD IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT (IEEE IC-NIDC 2012), P54, DOI 10.1109/ICNIDC.2012.6418710
CR  - Cowling RM, 2003, BIOL CONSERV, V112, P191, DOI 10.1016/S0006-3207(02)00425-1
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
CR  - Gaston KJ, 2004, PHILOS T R SOC B, V359, P655, DOI 10.1098/rstb.2003.1442
CR  - Hong SW, 2012, INT CONF IMAG PROC, P141, DOI 10.1109/IPTA.2012.6469535
CR  - Hsu TH, 2011, MULTIMED TOOLS APPL, V53, P53, DOI 10.1007/s11042-010-0490-6
CR  - Huang R., 2009, P MOMM, P618, DOI DOI 10.1145/1821748.1821868
CR  - Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
CR  - Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
CR  - Krizhevsky A., 2012, ADV NEURAL INFORM PR, P1097, DOI [10.1145/3065386, DOI 10.1145/3065386]
CR  - Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
CR  - Nilsback ME, 2010, IMAGE VISION COMPUT, V28, P1049, DOI 10.1016/j.imavis.2009.10.001
CR  - Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
CR  - Nilsback ME, 2006, IEEE COMP SOC C COMP, V2, P1447, DOI [10.1109/CVPR.2006.42, DOI 10.1109/CVPR.2006.42]
CR  - Phyu KH, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P366, DOI 10.1109/SITIS.2012.60
CR  - Rebelo, 2001, SASOL PROTEAS FIELD
CR  - Rebelo T., 2004, PROTEA ATLAS
CR  - Silvertown J, 2009, TRENDS ECOL EVOL, V24, P467, DOI 10.1016/j.tree.2009.03.017
CR  - Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
CR  - Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
CR  - Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
CR  - Valente LM, 2010, EVOLUTION, V64, P745, DOI 10.1111/j.1558-5646.2009.00856.x
CR  - Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
CR  - Waldchen J, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005993
CR  - Waldchen J, 2018, ARCH COMPUT METHOD E, V25, P507, DOI 10.1007/s11831-016-9206-z
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Wenjing Qi, 2012, Proceedings of the 2012 IEEE International Conference on Computer Science and Automation Engineering (CSAE 2012), P670, DOI 10.1109/CSAE.2012.6273040
CR  - Wu SG, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1-3, P120
CR  - Zawbaa HM, 2014, 2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P895, DOI 10.1109/ICACCI.2014.6968612
CR  - Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
PU  - IEEE COMPUTER SOC
PI  - LOS ALAMITOS
PA  - 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA
PY  - 2020
SP  - 2094
EP  - 2102
AN  - WOS:000578444802018
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  38
ER  -

TY  - JOUR
AU  - Munian, Y
AU  - Martinez-Molina, A
AU  - Miserlis, D
AU  - Hernandez, H
AU  - Alamaniotis, M
TI  - Intelligent System Utilizing HOG and CNN for Thermal Image-Based Detection of Wild Animals in Nocturnal Periods for Vehicle Safety
T2  - APPLIED ARTIFICIAL INTELLIGENCE
LA  - English
KW  - COLLISIONS
KW  - ALGORITHM
KW  - FEATURES
AB  - Animal Vehicle Collision, commonly called roadkill, is an emerging threat to drivers and wild animals, increasing fatalities every year. Currently, prevalent methods using visible light cameras are efficient for animal detection in daylight time. This paper focuses on locating wildlife close to roads during nocturnal hours by utilizing thermographic obtained images, thus enhancing vehicle safety. In particular, it proposes an intelligent system for animal detection during nighttime that combines the technique of Histogram of Oriented Gradients (HOG) with a Convolutional Neural Network (CNN). The proposed intelligent system is benchmarked against a variety of CNN's like basic CNN and VGG16-based CNN and also with the machine learning algorithms such as Support Vector Machine (SVM), Random Forest (RF), Decision Tree Algorithm (DT), Linear Regression (LR), and Gaussian Naive Bayes (GNB). The proposed detection system was tested on a set of real-world data acquired with a thermal camera on the move in the city of San Antonio, TX, USA that includes images of wild deer. Obtained results exhibit that the HOG-CNN combination achieved approximately 91% correct detection accuracy of wild deer on roadsides, while it outperformed the rest of the tested machine learning algorithms.
AD  - Univ Texas San Antonio Utsa, Dept Elect & Comp Engn, One UTSA Circle, San Antonio, TX 78249 USAAD  - Univ Texas San Antonio Utsa, Sch Architecture & Design, San Antonio, TX 78249 USAAD  - Univ Texas Hlth Sci Ctr, Dept Surg Vasc Surg, Houston, TX USAAD  - 25 2 Solut Corp, Pocatello, ID USAC3  - University of Texas SystemC3  - University of Texas Health Science Center HoustonCR  - Affonso C, 2017, EXPERT SYST APPL, V85, P114, DOI 10.1016/j.eswa.2017.05.039
CR  - Aslan MF, 2020, MEASUREMENT, V158, DOI 10.1016/j.measurement.2020.107704
CR  - Bai S, 2017, EXPERT SYST APPL, V71, P279, DOI 10.1016/j.eswa.2016.10.038
CR  - Benten A, 2018, ACCIDENT ANAL PREV, V120, P64, DOI 10.1016/j.aap.2018.08.003
CR  - Christiansen P, 2014, SENSORS-BASEL, V14, P13778, DOI 10.3390/s140813778
CR  - Dalal N., P IEEE C COMP VIS PA, P886
CR  - Emine Cengil., 2017, INT J ADV RES, V3, P244
CR  - Ferreira A, 2017, EXPERT SYST APPL, V84, P1, DOI 10.1016/j.eswa.2017.04.053
CR  - Gade R, 2014, MACH VISION APPL, V25, P245, DOI 10.1007/s00138-013-0570-5
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Guo L, 2012, EXPERT SYST APPL, V39, P4274, DOI 10.1016/j.eswa.2011.09.106
CR  - Islam K. Nusrat, 2018, 2018 IEEE International Conference on Plasma Science (ICOPS), DOI 10.1109/ICOPS35962.2018.9575733
CR  - Kwasniewska A, 2020, ENG APPL ARTIF INTEL, V87, DOI 10.1016/j.engappai.2019.103263
CR  - Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
CR  - Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
CR  - Mallick S, 2016, LEARN OPEN
CR  - Massaro A, 2021, 2021 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR INDUSTRY 4.0 & IOT (IEEE METROIND4.0 & IOT), P225, DOI [10.1109/MetroInd4.0IoT51437.2021.9488561, 10.1109/METROIND4.0IOT51437.2021.9488561]
CR  - Massaro A, 2020, 2020 IEEE INTERNATIONAL WORKSHOP ON METROLOGY FOR INDUSTRY 4.0 & IOT (METROIND4.0&IOT), P554, DOI 10.1109/MetroInd4.0IoT48571.2020.9138207
CR  - Munian Yuvaraj, 2020, 2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA), DOI 10.1109/IISA50023.2020.9284365
CR  - Peeters J, 2018, OPTIM ENG, V19, P163, DOI 10.1007/s11081-017-9368-z
CR  - Pons P, 2017, EXPERT SYST APPL, V86, P235, DOI 10.1016/j.eswa.2017.05.063
CR  - Riaz I, 2013, IADIS-INT J COMPUT S, V8, P1
CR  - Santhi V, 2017, ADV CIV IND ENG BOOK, P1, DOI 10.4018/978-1-5225-2423-6
CR  - Sawyer H, 2016, WILDLIFE SOC B, V40, P211, DOI 10.1002/wsb.650
CR  - Sibanda V, 2019, PROC CIRP, V84, P755, DOI 10.1016/j.procir.2019.04.175
CR  - Su XQ, 2013, IEEE INT SYMP CIRC S, P2892, DOI 10.1109/ISCAS.2013.6572483
CR  - Wang JF, 2020, APPL ENERG, V257, DOI 10.1016/j.apenergy.2019.113998
CR  - Wang Y, 2019, TRANSPORT RES C-EMER, V99, P144, DOI 10.1016/j.trc.2018.12.004
CR  - Wei XK, 2019, ENG APPL ARTIF INTEL, V80, P66, DOI 10.1016/j.engappai.2019.01.008
CR  - Wilkins DC, 2019, ACCIDENT ANAL PREV, V131, P157, DOI 10.1016/j.aap.2019.05.030
CR  - Wu XW, 2020, NEUROCOMPUTING, V396, P39, DOI 10.1016/j.neucom.2020.01.085
CR  - Yang K, 2018, TRANSPORT RES C-EMER, V96, P192, DOI 10.1016/j.trc.2018.09.020
CR  - Zhao YD, 2020, NEUROCOMPUTING, V380, P259, DOI 10.1016/j.neucom.2019.10.067
CR  - Zhou D., 2012, P INT C IM PROC COMP, V2, P969
CR  - Zhu Q, 2006, IEEE COMP SOC C COMP, V2, P1491, DOI DOI 10.1109/CVPR.2006.119
CR  - Zilkha M, 2019, PROC SPIE, V11169, DOI 10.1117/12.2532908
PU  - TAYLOR & FRANCIS INC
PI  - PHILADELPHIA
PA  - 530 WALNUT STREET, STE 850, PHILADELPHIA, PA 19106 USA
DO  - 10.1080/08839514.2022.2031825
AN  - WOS:000753031000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  36
ER  -

TY  - JOUR
AU  - Malcolm, JR
AU  - Brousseau, B
AU  - Jones, T
AU  - Thomas, SC
TI  - Use of Sentinel-2 Data to Improve Multivariate Tree Species Composition in a Forest Resource Inventory
T2  - REMOTE SENSING
LA  - English
KW  - remote sensing
KW  - multivariate tree species composition
KW  - Sentinel-2
KW  - forest resource inventory
KW  - random forest
KW  - convolutional neural network
KW  - forest management
KW  - WILDLIFE HABITAT
KW  - CLASSIFICATION
KW  - MANAGEMENT
KW  - VARIABLES
KW  - ONTARIO
KW  - GROWTH
KW  - MAPS
AB  - Aerial-photo interpreted inventories of forest resources, including tree species composition, are valuable in forest resource management, but are expensive to create and can be relatively inaccurate. Because of differences among tree species in their spectral properties and seasonal phenologies, it might be possible to improve such forest resource inventory information (FRI) by using it in concert with multispectral satellite information from multiple time periods. We used Sentinel-2 information from nine spectral bands and 12 dates within a two-year period to model multivariate percent tree species composition in > 51,000 forest stands in the FRI of south-central Ontario, Canada. Accuracy of random forest (RF) and convolutional neural network (CNN) predictions were tested using species-specific basal area information from 155 0.25-ha field plots. Additionally, we created models using the Sentinel-2 information in concert with the field data and compared the accuracy of these models and the FRI-based models by use of basal areas from a second (13.7-ha) field data set. Based on average R-2 values across species in the two field data sets, the Sentinel-FRI models outperformed the FRI, showing 1.5- and 1.7-fold improvements relative to the FRI for RF and 2.1- and 2.2-fold improvements for CNN (mean R-2: 0.141-0.169 (FRI); 0.217-0.295 (RF); 0.307-0.352 (CNN)). Models created with the field data performed even better: improvements relative to the FRI were 2.1-fold for RF and 2.8-fold for CNN (mean R-2: 0.169 (FRI); 0.356 (RF); 0.469 (CNN)). As predicted, R-2 values between FRI- and field-trained predictions were higher than R-2 values with the FRI. Of the 21 tree species evaluated, 8 relatively rare species had poor models in all cases. Our multivariate approach allowed us to use more FRI stands in model creation than if we had been restricted to stands dominated by single species and allowed us to map species abundances at higher resolution. It might be possible to improve models further by use of tree stem maps and incorporation of the effects of canopy disturbances.
AD  - Univ Toronto, Inst Forestry & Conservat, 33 Willcocks St, Toronto, ON M5S 3B3, CanadaAD  - Univ Toronto, Dept Elect & Comp Engn, 10 Kings Coll Rd, Toronto, ON M5S 3G4, CanadaAD  - Canadian Wood Fibre Ctr, Nat Resources Canada, Canadian Forest Serv, 1219 Queen St East, Sault Ste Marie, ON P6A 2E5, CanadaC3  - University of TorontoC3  - University of TorontoC3  - Natural Resources CanadaC3  - Canadian Forest ServiceFU  - Natural Sciences and Engineering Research Council of Canada; Ontario Ministry of Natural Resources, and Ontario Power Generation
FX  - Funding was from the Natural Sciences and Engineering Research Council of Canada (Discovery Grants to J.R.M. and S.C.T.; Canada Research Chair to S.C.T.), the Ontario Ministry of Natural Resources, and Ontario Power Generation.
CR  - Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
CR  - Anderson-Teixeira KJ, 2015, GLOBAL CHANGE BIOL, V21, P528, DOI 10.1111/gcb.12712
CR  - Axelsson A, 2021, INT J APPL EARTH OBS, V100, DOI 10.1016/j.jag.2021.102318
CR  - Belgiu M, 2016, ISPRS J PHOTOGRAMM, V114, P24, DOI 10.1016/j.isprsjprs.2016.01.011
CR  - Bennamoun M., 2018, GUIDE CONVOLUTIONAL
CR  - Berger J, 2014, PHILOS PSYCHOL, V27, P829, DOI 10.1080/09515089.2013.771241
CR  - Bohlman S, 2012, J ECOL, V100, P508, DOI 10.1111/j.1365-2745.2011.01935.x
CR  - Bolyn C, 2018, BIOTECHNOL AGRON SOC, V22, P172
CR  - Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
CR  - Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
CR  - Brousseau B, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020543
CR  - Clevert D, 2016, FAST ACCURATE DEEP N
CR  - Condit R., 1998, Tropical forest census plots: methods and results from Barro Colorado Island, Panama and a comparison with other plots.
CR  - Duchi J, 2011, J MACH LEARN RES, V12, P2121
CR  - Environment Canada, CANADIAN CLIMATE NOR
CR  - Feret JB, 2012, REMOTE SENS-BASEL, V4, P2457, DOI 10.3390/rs4082457
CR  - Grabska E, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11101197
CR  - Hamraz H, 2019, ISPRS J PHOTOGRAMM, V158, P219, DOI 10.1016/j.isprsjprs.2019.10.011
CR  - Hartling S, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19061284
CR  - Hennigar CR, 2008, FOREST ECOL MANAG, V256, P786, DOI 10.1016/j.foreco.2008.05.037
CR  - Hoscilo A, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11080929
CR  - Immitzer M, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11222599
CR  - Isaac E, 2017, REMOTE SENS LETT, V8, P350, DOI [10.1080/2150704X.2016.1274443, 10.1080/2150704x.2016.1274443]
CR  - Ishwaran H., FAST UNIFIED RANDOM
CR  - King DB, 2015, ACS SYM SER, V1214, P1
CR  - KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
CR  - Leckie DG, 2005, CAN J REMOTE SENS, V31, P175, DOI 10.5589/m05-004
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Lim J, 2019, ISPRS INT J GEO-INF, V8, DOI 10.3390/ijgi8030150
CR  - Liu YA, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10060946
CR  - Magnussen S, 2012, FOREST CHRON, V88, P439, DOI 10.5558/tfc2012-080
CR  - Malcolm JR, 2020, CLIMATIC CHANGE, V161, P433, DOI 10.1007/s10584-020-02711-8
CR  - Malcolm JR, 2004, FOREST CHRON, V80, P91, DOI 10.5558/tfc80091-1
CR  - Maltamo M, 2021, CAN J FOREST RES, V51, P257, DOI 10.1139/cjfr-2020-0322
CR  - Maltamo M, 2014, MANAG FOR ECOSYST, V27, P1, DOI 10.1007/978-94-017-8663-8
CR  - Maxie AJ, 2010, FOREST CHRON, V86, P77, DOI 10.5558/tfc86077-1
CR  - McDermid GJ, 2009, FOREST ECOL MANAG, V257, P2262, DOI 10.1016/j.foreco.2009.03.005
CR  - McRoberts RE, 2017, FOREST ECOL MANAG, V401, P295, DOI 10.1016/j.foreco.2017.07.017
CR  - Nair V., 2010, P 27 INT C MACH LEAR
CR  - Oksanen J, VEGAN COMMUNITY ECOL
CR  - OMNR, 2001, FOR INF MAN
CR  - Pan L, 2021, INT J APPL EARTH OBS, V102, DOI 10.1016/j.jag.2021.102376
CR  - Penner M, 2008, FOREST CHRON, V84, P46, DOI 10.5558/tfc84046-1
CR  - Persson M, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111794
CR  - Ping Wei, 2018, P ICLR, P214
CR  - Pinto F, 2007, FOREST CHRON, V83, P247, DOI 10.5558/tfc83247-2
CR  - Potvin F, 1999, FOREST CHRON, V75, P851, DOI 10.5558/tfc75851-5
CR  - R.Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
CR  - Ranghetti L, 2020, COMPUT GEOSCI-UK, V139, DOI 10.1016/j.cageo.2020.104473
CR  - Rejou-Mechain M, 2014, BIOGEOSCIENCES, V11, P6827, DOI 10.5194/bg-11-6827-2014
CR  - Rettie WJ, 1997, FOREST ECOL MANAG, V93, P245, DOI 10.1016/S0378-1127(96)03940-0
CR  - Rezaee M, 2018, IAPR WORKS PATTERN
CR  - Rowe J. S., 1972, Publication, Canadian Forestry Service, pmap
CR  - Segal M, 2011, WIRES DATA MIN KNOWL, V1, P80, DOI 10.1002/widm.12
CR  - Shang C, 2019, INT J APPL EARTH OBS, V78, P360, DOI 10.1016/j.jag.2018.10.002
CR  - Spriggs RA, 2015, CAN J FOREST RES, V45, P1338, DOI 10.1139/cjfr-2015-0018
CR  - Stoffels J, 2012, EUR J FOREST RES, V131, P1071, DOI 10.1007/s10342-011-0577-2
CR  - Thomas SC, 2010, TREE PHYSIOL, V30, P555, DOI 10.1093/treephys/tpq005
CR  - Thompson ID, 2007, FOREST ECOL MANAG, V252, P208, DOI 10.1016/j.foreco.2007.06.033
CR  - Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26
CR  - Tomppo E, 2008, REMOTE SENS ENVIRON, V112, P1982, DOI 10.1016/j.rse.2007.03.032
CR  - Wang S., 2020, IEEE T KNOWL DATA EN, DOI [DOI 10.1109/TKDE.2020.3025580, 10.1109/TKDE.2020.3025580]
CR  - Zhang YQ, 2018, IEEE J-STARS, V11, P1082, DOI 10.1109/JSTARS.2018.2809781
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - NOV
PY  - 2021
VL  - 13
IS  - 21
DO  - 10.3390/rs13214297
AN  - WOS:000723178600001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  63
ER  -

TY  - JOUR
AU  - Shepley, A
AU  - Falzon, G
AU  - Meek, P
AU  - Kwan, P
TI  - Automated location invariant animal detection in camera trap images using publicly available data sources
T2  - ECOLOGY AND EVOLUTION
LA  - English
KW  - animal identification
KW  - artificial intelligence
KW  - camera trap images
KW  - camera trapping
KW  - deep convolutional neural networks
KW  - deep learning
KW  - infusion
KW  - location invariance
KW  - wildlife ecology
KW  - wildlife monitoring
AB  - A time-consuming challenge faced by camera trap practitioners is the extraction of meaningful data from images to inform ecological management. An increasingly popular solution is automated image classification software. However, most solutions are not sufficiently robust to be deployed on a large scale due to lack of location invariance when transferring models between sites. This prevents optimal use of ecological data resulting in significant expenditure of time and resources to annotate and retrain deep learning models.
   We present a method ecologists can use to develop optimized location invariant camera trap object detectors by (a) evaluating publicly available image datasets characterized by high intradataset variability in training deep learning models for camera trap object detection and (b) using small subsets of camera trap images to optimize models for high accuracy domain-specific applications.
   We collected and annotated three datasets of images of striped hyena, rhinoceros, and pigs, from the image-sharing websites FlickR and iNaturalist (FiN), to train three object detection models. We compared the performance of these models to that of three models trained on the Wildlife Conservation Society and Camera CATalogue datasets, when tested on out-of-sample Snapshot Serengeti datasets. We then increased FiN model robustness by infusing small subsets of camera trap images into training.
   In all experiments, the mean Average Precision (mAP) of the FiN trained models was significantly higher (82.33%-88.59%) than that achieved by the models trained only on camera trap datasets (38.5%-66.74%). Infusion further improved mAP by 1.78%-32.08%.
   Ecologists can use FiN images for training deep learning object detection solutions for camera trap image processing to develop location invariant, robust, out-of-the-box software. Models can be further optimized by infusion of 5%-10% camera trap images into training data. This would allow AI technologies to be deployed on a large scale in ecological applications. Datasets and code related to this study are open source and available on this repository: .
AD  - Univ New England, Sch Sci & Technol, Armidale, NSW, AustraliaAD  - Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA, AustraliaAD  - NSW Dept Primary Ind, Vertebrate Pest Res Unit, Coffs Harbour, NSW, AustraliaAD  - Univ New England, Sch Environm & Rural Sci, Armidale, NSW, AustraliaAD  - Melbourne Inst Technol, Sch IT & Engn, Melbourne, Vic, AustraliaC3  - University of New EnglandC3  - Flinders University South AustraliaC3  - NSW Department of Primary IndustriesC3  - University of New EnglandFU  - University of New England; Australian Department of Agriculture and Water Resources; NSW Department of Primary Industries; NSW Environmental Trust; Centre for Invasive Animals Solutions
FX  - Centre for Invasive Animals Solutions; University of New England; Australian Department of Agriculture and Water Resources; NSW Department of Primary Industries; NSW Environmental Trust
CR  - Aradhya H.V.R., 2018, 2018 INT C COMM SIGN
CR  - Christensen JH, 2018, 2018 IEEE/OES AUTONOMOUS UNDERWATER VEHICLE WORKSHOP (AUV)
CR  - Christin S, 2019, METHODS ECOL EVOL, V10, P1632, DOI 10.1111/2041-210X.13256
CR  - Clune, 2017, P NATL ACAD SCI USA, V115
CR  - Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
CR  - Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
CR  - Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
CR  - Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
CR  - Gibb R, 2019, METHODS ECOL EVOL, V10, P169, DOI 10.1111/2041-210X.13101
CR  - Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Kellenberger B, 2017, JOINT URB REMOTE SEN
CR  - Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
CR  - Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
CR  - Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
CR  - Maurice, 2019, SURV STAT PANG CAM T
CR  - Meek PD, 2015, AUST MAMMAL, V37, P13, DOI 10.1071/AM14023
CR  - Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
CR  - Sugai LSM, 2019, BIOSCIENCE, V69, P15, DOI 10.1093/biosci/biy147
CR  - Nguyen H, 2017, PR INT CONF DATA SC, P40, DOI 10.1109/DSAA.2017.31
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - O'Connell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P191, DOI 10.1007/978-4-431-99495-4_11
CR  - Perona, 2018, RECOGNITION TERRA IN
CR  - Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
CR  - Ren SQ, 2015, ADV NEUR IN, V28
CR  - Rodin CD, 2018, IEEE IJCNN
CR  - Rovero F., 2016, CAMERA TRAPPING WILD
CR  - Schneider S, 2019, METHODS ECOL EVOL, V10, P461, DOI 10.1111/2041-210X.13133
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Shahinfar S, 2020, ECOL INFORM, V57, DOI 10.1016/j.ecoinf.2020.101085
CR  - Singh P., 2020, 2020 IEEE SW S IM AN
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Swinnen KRR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0098881
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tambe M., 2020, 2020 IEEE WINT C APP
CR  - Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951
CR  - Vedaldi, 2017, LEARNING MULTIPLE VI, P506
CR  - Wang G., 2017, 2017 IEEE WINT C APP
CR  - Wang X., 2019, 2019 IEEE CVF C COMP
CR  - Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
CR  - Wearn O.R., 2017, WWF CONSERVATION TEC, DOI DOI 10.13140/RG.2.2.23409.17767
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Xu BB, 2020, INT J REMOTE SENS, V41, P8121, DOI 10.1080/01431161.2020.1734245
CR  - Yang XY, 2019, IEEE INT CONF COMP V, P255, DOI 10.1109/ICCVW.2019.00034
CR  - Young S, 2018, ECOL EVOL, V8, P9947, DOI 10.1002/ece3.4464
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
CR  - Zisserman, 2007, DATASET ISSUES OBJEC, V4170, P29
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - MAY
PY  - 2021
VL  - 11
IS  - 9
SP  - 4494
EP  - 4506
DO  - 10.1002/ece3.7344
AN  - WOS:000626984400001
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  48
ER  -

TY  - CPAPER
AU  - Zualkernan, IA
AU  - Dhou, S
AU  - Judas, J
AU  - Sajun, AR
AU  - Gomez, BR
AU  - Hussain, LA
AU  - Sakhnini, D
A1  - IEEE
TI  - Towards an IoT-based Deep Learning Architecture for Camera Trap Image Classification
T2  - 2020 IEEE GLOBAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INTERNET OF THINGS (GCAIOT)
LA  - English
CP  - IEEE Global Conference on Artificial Intelligence and Internet of Things (GCAIoT)
KW  - deep learning
KW  - transfer learning
KW  - convolutional neural networks
KW  - animal classification
KW  - camera trap
KW  - wildlife monitoring
KW  - edge computing
KW  - TensorFlow lite
KW  - raspberry pi
KW  - IoT
AB  - Maintaining biodiversity is a key component of the United Nations (UN) "Life on Land" sustainability goal. Remote camera traps monitoring animals' movements support research in biodiversity. However, images from these camera traps are currently labeled manually resulting in high processing costs and long delays. This paper proposes an IoT-based system that leverages deep learning and edge computing to automatically label camera trap images and transmit this information to scientists in a timely manner. Inception-V3, MobileNet-V2, ResNet-18, and DenseNet-121 were trained on data consisting of 33,984 images taken during day and night with 6 animal classes. Inception-V3 yielded the highest macro average Fl-score of 0.93 and an accuracy of 94%. An IoT-based system was developed that directly captures images from a commercial camera trap, does the inference on the edge using a Raspberry Pi (RPi), and sends the classification results back to a cloud database system. A mobile App is used to monitor the camera images classified on camera traps in real-time. The RPi could easily sustain a rate of processing 1 image every 2 seconds with an average latency of 1.8 second/image. After capture and pre-processing, each inference took an average of 0.2 Millisecond/image on a RPi Model 4B.
AD  - Amer Univ Sharjah, Comp Sci & Engn, Sharjah, U Arab EmiratesAD  - Emirates Nat WWF, Conservat Unit, Duai, U Arab EmiratesC3  - American University of SharjahCR  - Al Balushi T, 2019, IEEE INTL CONF IND I, P1035, DOI 10.1109/INDIN41052.2019.8972063
CR  - Allken V, 2019, ICES J MAR SCI, V76, P342, DOI 10.1093/icesjms/fsy147
CR  - [Anonymous], 2018, LIVING PLANET REPORT
CR  - Ayanzadeh A., 2018, MODIFIED DEEP NEURAL, DOI [10.20944/preprints201812.0232.v1, DOI 10.20944/PREPRINTS201812.0232.V1]
CR  - Ayoub W, 2019, IEEE COMMUN SURV TUT, V21, P1561, DOI 10.1109/COMST.2018.2877382
CR  - Beery S, 2018, LECT NOTES COMPUT SC, V11220, P472, DOI 10.1007/978-3-030-01270-0_28
CR  - Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
CR  - Curtin BH, 2019, 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P82, DOI 10.1109/UEMCON47517.2019.8993061
CR  - Elias Andy Rosales, 2017, 2017 IEEE/ACM Second International Conference on Internet-of-Things Design and Implementation (IoTDI), P247, DOI 10.1145/3054977.3054986
CR  - Gogul I, 2017, 2017 FOURTH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATION AND NETWORKING (ICSCN)
CR  - Haupt J., 2018, LARGE SCALE PLANT CL
CR  - Mac Aodha O, 2019, IEEE I CONF COMP VIS, P9595, DOI 10.1109/ICCV.2019.00969
CR  - Mathur A., 2019, REAL TIME WILDLIFE D
CR  - Monburinon N, 2019, PROCEEDINGS OF THE 2019 4TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY (INCIT), P294, DOI 10.1109/INCIT.2019.8912138
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Popat Param, 2019, Information and Communication Technology for Intelligent Systems. Proceedings of ICTIS 2018. Smart Innovation, Systems and Technologies (SIST 106), P319, DOI 10.1007/978-981-13-1742-2_31
CR  - Shi W., 2018, DOG BREED IDENTIFICA
CR  - Sreedevi C. K., 2019, 3447740 SSRN
CR  - Tan M., 2019, ARXIV190511946 CS ST
CR  - Teto J. Kamdem, 2018, THESIS
CR  - Thomassen S., 2017, EMBEDDED ANALYTICS A
CR  - Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
CR  - Wang H, 2019, IEEE INT CONF INDUST, P1796, DOI 10.1109/ICIT.2019.8755153
CR  - Weinstein BG, 2018, J ANIM ECOL, V87, P533, DOI 10.1111/1365-2656.12780
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Zimmerman G., 2019, WYSS CAMPAIGN NA SEP
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2020
SP  - 111
EP  - 116
DO  - 10.1109/GCAIOT51063.2020.9345858
AN  - WOS:000675459100019
N1  - Times Cited in Web of Science Core Collection:  7
Total Times Cited:  7
Cited Reference Count:  26
ER  -

TY  - JOUR
AU  - Bertran, M
AU  - Alsina-Pages, RM
AU  - Tena, E
TI  - Pipistrellus pipistrellus and Pipistrellus pygmaeus in the Iberian Peninsula: An Annotated Segmented Dataset and a Proof of Concept of a Classifier in a Real Environment
T2  - APPLIED SCIENCES-BASEL
LA  - English
KW  - acoustic bat recognition
KW  - dataset
KW  - bat call
KW  - Chiropthera
KW  - Convolutional Neural Network
KW  - dataset
KW  - echolocation
KW  - Feedforward Neural Network
KW  - Machine learning
KW  - ultrasounds
KW  - Wireless Acoustic Sensor Network
KW  - BAT ECHOLOCATION CALLS
KW  - AUTOMATED IDENTIFICATION
KW  - FIELD IDENTIFICATION
KW  - CHIROPTERA
KW  - RECOGNITION
KW  - VARIABILITY
KW  - HABITATS
KW  - NETWORKS
KW  - PATTERNS
KW  - DESIGN
AB  - Bats have an important role in the ecosystem, and therefore an effective detection of their prevalence can contribute to their conservation. At present, the most commonly methodology used in the study of bats is the analysis of echolocation calls. However, many other ultrasound signals can be simultaneously recorded, and this makes species location and identification a long and difficult task. This field of research could be greatly improved through the use of bioacoustics which provide a more accurate automated detection, identification and count of the wildlife of a particular area. We have analyzed the calls of two bat species-Pipistrellus pipistrellus and Pipistrellus pygmaeus-both of which are common types of bats frequently found in the Iberian Peninsula. These two cryptic species are difficult to identify by their morphological features, but are more easily identified by their echolocation calls. The real-life audio files have been obtained by an Echo Meter Touch Pro 1 bat detector. Time-expanded recordings of calls were first classified manually by means of their frequency, duration and interpulse interval. In this paper, we first detail the creation of a dataset with three classes, which are the two bat species but also the silent intervals. This dataset can be useful to work in mixed species environment. Afterwards, two automatic bat detection and identification machine learning approaches are described, in a laboratory environment, which represent the previous step to real-life in an urban scenario. The priority in that approaches design is the identification using short window analysis in order to detect each bat pulse. However, given that we are concerned with the risks of automatic identification, the main aim of the project is to accelerate the manual ID process for the specialists in the field. The dataset provided will help researchers develop automatic recognition systems for a more accurate identification of the bat species in a laboratory environment, and in a near future, in an urban environment, where those two bat species are common.
AD  - Grp Recerca Tecnol Media, C Quatre Camins 30, Barcelona 08022, SpainAD  - Univ Complutense, Fac Ciencias Biol, Dept Biodiversidad Ecol & Evoluc, C Jose Antonio Novais 12, E-28040 Madrid, SpainC3  - Complutense University of MadridFU  - Ministry of Education and Vocational Training; Obra Social La Caixa; Council for Culture, Education and Sport in the Autonomous Community in Madrid; European Social Fund
FX  - Ministry of Education and Vocational Training, Obra Social La Caixa, Council for Culture, Education and Sport in the Autonomous Community in Madrid and European Social Fund.
CR  - Ahlen I, 1999, ACTA CHIROPTEROL, V1, P137
CR  - Altringham JD, 2011, BATS: FROM EVOLUTION TO CONSERVATION, 2ND EDITION, P1
CR  - Bain M., 2014, SENTILO SENSOR ACTUA
CR  - BARATAUD M, 2015, ACOUSTIC ECOLOGY EUR
CR  - Barataud M, 2012, ECOLOGIE ACOUSTIQUE
CR  - Bello JP, 2019, COMMUN ACM, V62, P68, DOI 10.1145/3224204
CR  - BERGLAND GD, 1969, IEEE SPECTRUM, V6, P41, DOI 10.1109/MSPEC.1969.5213896
CR  - Botteldooren D., 2011, ACOUSTICS 2011
CR  - Boulmaiz A, 2016, INT J SPEECH TECHNOL, V19, P631, DOI 10.1007/s10772-016-9354-4
CR  - Brabant R., 2018, BELG J ZOOL, P148
CR  - Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
CR  - Camps-Farres J., 2018, P EURONOISE 2018 CRE, P693
CR  - Caragliu A, 2011, J URBAN TECHNOL, V18, P65, DOI 10.1080/10630732.2011.601117
CR  - Socoro JC, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17102323
CR  - Cleveland CJ, 2006, FRONT ECOL ENVIRON, V4, P238, DOI 10.1890/1540-9295(2006)004[0238:EVOTPC]2.0.CO;2
CR  - Davidson-Watts I, 2006, BIOL CONSERV, V133, P118, DOI 10.1016/j.biocon.2006.05.027
CR  - de la Piedra A, 2013, 2013 IEEE EUROCON, P267, DOI 10.1109/EUROCON.2013.6624996
CR  - de la Piedra A, 2012, SENSORS-BASEL, V12, P12235, DOI 10.3390/s120912235
CR  - Fine T.L., 2006, FEEDFORWARD NEURAL N
CR  - Gehrt SD, 2003, ECOL APPL, V13, P939, DOI 10.1890/02-5188
CR  - Gehrt SD, 2004, ECOL APPL, V14, P625, DOI 10.1890/03-5013
CR  - Gevaert W., 2010, J AUTOMATIC CONTROL, V20, P1
CR  - Gillam EH, 2007, ANIM BEHAV, V74, P277, DOI 10.1016/j.anbehav.2006.12.006
CR  - Gros-Desormeaux H., 2010, WILDLIFE ASSESSMENT
CR  - HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837
CR  - Hayes JP, 1997, J MAMMAL, V78, P514, DOI 10.2307/1382902
CR  - Hosmer D. W., 2013, APPL LOGISTIC REGRES, V398
CR  - Hulva P, 2004, MOL PHYLOGENET EVOL, V32, P1023, DOI 10.1016/j.ympev.2004.04.007
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
CR  - Lopez-Baucells A, 2019, ECOL INFORM, V49, P45, DOI 10.1016/j.ecoinf.2018.11.004
CR  - Medellin RA, 1999, BIOTROPICA, V31, P478, DOI 10.1111/j.1744-7429.1999.tb00390.x
CR  - Mellinger DK, 2000, J ACOUST SOC AM, V107, P3518, DOI 10.1121/1.429434
CR  - Mermelstein P., 1976, PATTERN RECOGN, V116, P374
CR  - Mietlicki F., 2015, P EURONOISE 2015 MAA, P2309
CR  - Obrist MK, 2004, MAMMALIA, V68, P307, DOI 10.1515/mamm.2004.030
CR  - Oppenheim A. V., 1999, DISCRETE TIME SIGNAL
CR  - Palomo LJ., 2007, ATLAS LIBRO ROJO MAM
CR  - Porter J, 2005, BIOSCIENCE, V55, P561, DOI 10.1641/0006-3568(2005)055[0561:WSNFE]2.0.CO;2
CR  - Rawat P, 2014, J SUPERCOMPUT, V68, P1, DOI 10.1007/s11227-013-1021-9
CR  - Rish I., IJCAI 2001 WORKSH EM, VVolume 3, P41, DOI DOI 10.1039/B104835J
CR  - Russ J., 2012, BRIT BAT CALLS GUIDE
CR  - Russo D, 2002, J ZOOL, V258, P91, DOI 10.1017/S0952836902001231
CR  - Russo D, 2003, ECOGRAPHY, V26, P197, DOI 10.1034/j.1600-0587.2003.03422.x
CR  - Russo D, 2016, ECOL INDIC, V66, P598, DOI 10.1016/j.ecolind.2016.02.036
CR  - Rydell J, 2017, ECOL INDIC, V78, P416, DOI 10.1016/j.ecolind.2017.03.023
CR  - Sevillano X, 2016, NOISE MAPP, V3, P172, DOI 10.1515/noise-2016-0013
CR  - Stahlschmidt P, 2012, METHODS ECOL EVOL, V3, P503, DOI 10.1111/j.2041-210X.2012.00188.x
CR  - Stattner E., 2011, WOWMOM, P1
CR  - Trejo-Salazar RE, 2016, NAT AREA J, V36, P523, DOI 10.3375/043.036.0417
CR  - Trifa V., 2007, AUTOMATED WILDLIFE M
CR  - van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
CR  - van Rijsbergen C. J., 1979, INFORM RETRIEVAL
CR  - Vaughan N, 1997, J APPL ECOL, V34, P716, DOI 10.2307/2404918
CR  - Walters CL, 2012, J APPL ECOL, V49, P1064, DOI 10.1111/j.1365-2664.2012.02182.x
CR  - Wang H., 2003, EURASIP J ADV SIG PR, V2003
CR  - WELCH PD, 1967, IEEE T ACOUST SPEECH, VAU15, P70, DOI 10.1109/TAU.1967.1161901
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - SEP 1
PY  - 2019
VL  - 9
IS  - 17
DO  - 10.3390/app9173467
AN  - WOS:000488603600029
N1  - Times Cited in Web of Science Core Collection:  2
Total Times Cited:  2
Cited Reference Count:  57
ER  -

TY  - CPAPER
AU  - Bowley, C
AU  - Mattingly, M
AU  - Barnas, A
AU  - Ellis-Felege, S
AU  - Desell, T
A1  - IEEE
TI  - Toward Using Citizen Scientists to Drive Automated Ecological Object Detection in Aerial Imagery
T2  - 2017 IEEE 13TH INTERNATIONAL CONFERENCE ON E-SCIENCE (E-SCIENCE)
LA  - English
CP  - 13th Annual IEEE International Conference on e-Science (e-Science)
KW  - GALAXY ZOO
KW  - SCIENCE
AB  - Automated object detection within imagery is challenging in the field of wildlife biology. Uncontrolled conditions, along with the relative size of target species to the more abundant background makes manual detection tedious and error-prone. In order to address these concerns, the Wildlife@Home project has been developed with a web portal to allow citizen scientists to inspect and catalog these images, which in turn provides training data for computer vision algorithms to automate the detection process. This work focuses on a project with over 65,000 Unmanned Aerial System (UAS) images from flights in the Hudson Bay area of Canada gathered in the years 2015 and 2016. This data set comprises over 3TB of raw imagery and also contains a further 2 million images from related ecological projects. Given the data scale, the person-hours that would be needed to manually inspect the data is extremely high. This work examines the efficacy of using citizen science data as inputs to convolutional neural networks (CNNs) used for object detection. Three CNNs were trained with expert observations, citizen scientist observations, and matched observations made by pairing citizen scientist observations of the same object and taking the intersection of the two observations. The expert, matched, and unmatched CNNs overestimated the number of lesser snow geese in the testing images by 88%, 150%, and 250%, respectively, which is less than current work using similar techniques on all visible (RGB) UAS imagery. These results show that the accuracy of the input data is more important than the quantity of the input data, as the unmatched citizen scientists observations are shown to be highly variable, but substantial in number, while the matched observations are much closer to the expert observations, though less in number. To increase the accuracy of the CNNs, it is proposed to use a feedback loop to ensure the CNN gets continually trained using extracted observations that it did poorly on during the testing phase.
AD  - Univ North Dakota, Dept Comp Sci, Grand Forks, ND 58202 USAAD  - Univ North Dakota, Dept Biol, Grand Forks, ND 58202 USAC3  - University of North Dakota Grand ForksC3  - University of North Dakota Grand ForksFU  - North Dakota EPSCoR; Hudson Bay Project; UND College of Arts and Sciences; National Science Foundation [1319700]
FX  - Funding was provided by North Dakota EPSCoR, the Hudson Bay Project, Central and Mississippi Flyways, and the UND College of Arts and Sciences. UAS data collection supported by the Hudson Bay Project. Permissions and in-kind assistance were provided by Parks Canada, Wapusk National Park Management Board, and the community of Churchill, Manitoba.
FX  - This work has been partially supported by the National Science Foundation under Grant Number 1319700. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
CR  - Abd-Elrahman A, 2005, SURVEYING LAND INFOR, V65, P37
CR  - Bonney R, 2009, BIOSCIENCE, V59, P977, DOI 10.1525/bio.2009.59.11.9
CR  - Bowley C, 2016, P IEEE INT C E-SCI, P251, DOI 10.1109/eScience.2016.7870906
CR  - Breckenridge R.P., 2005, TECH REP
CR  - Breckenridge RP, 2011, RANGELAND ECOL MANAG, V64, P521, DOI 10.2111/REM-D-10-00030.1
CR  - Cariappa C., 2008, ESTIMATING POPULATIO
CR  - CAUGHLEY G, 1974, J WILDLIFE MANAGE, V38, P921, DOI 10.2307/3800067
CR  - CAUGHLEY G, 1977, J WILDLIFE MANAGE, V41, P605, DOI 10.2307/3799980
CR  - Chretien LP, 2016, WILDLIFE SOC B, V40, P181, DOI 10.1002/wsb.629
CR  - Cox J, 2015, COMPUT SCI ENG, V17, P28, DOI 10.1109/MCSE.2015.65
CR  - Fischer DA, 2012, MON NOT R ASTRON SOC, V419, P2900, DOI 10.1111/j.1365-2966.2011.19932.x
CR  - Ioffe S., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2
CR  - Israel M, 2011, INT ARCH PHOTOGRAMM, V38-1, P51
CR  - Koski W., 2009, SC61E IWC, V9
CR  - Lecun Y., 2010, CORTES
CR  - Leonardo M., 2013, ASME 2013 INT DES EN
CR  - Linchant J, 2013, USING DRONES COUNT E
CR  - Lintott C, 2011, MON NOT R ASTRON SOC, V410, P166, DOI 10.1111/j.1365-2966.2010.17432.x
CR  - Lintott CJ, 2008, MON NOT R ASTRON SOC, V389, P1179, DOI 10.1111/j.1365-2966.2008.13689.x
CR  - Maas A. L., 2013, PROC ICML, V28, P1
CR  - Mattingly M, 2016, P IEEE INT C E-SCI, P223, DOI 10.1109/eScience.2016.7870903
CR  - Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
CR  - Sasse DB, 2003, WILDLIFE SOC B, V31, P1015
CR  - Simpson R, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P1049, DOI 10.1145/2567948.2579215
CR  - Soriano P., 2009, 40 INT S ROB BARC ES
CR  - Steen K.A., 2012, INT C AGR ENG
CR  - Vermeulen C, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0054700
CR  - Voss MA, 2010, AM BIOL TEACH, V72, P437, DOI 10.1525/abt.2010.72.7.9
CR  - Wilkinson B. E., 2007, THESIS
CR  - Wood C, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1001220
PU  - IEEE
PI  - NEW YORK
PA  - 345 E 47TH ST, NEW YORK, NY 10017 USA
PY  - 2017
SP  - 99
EP  - 108
DO  - 10.1109/eScience.2017.22
AN  - WOS:000426071600011
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Yousif, H
AU  - Yuan, JH
AU  - Kays, R
AU  - He, ZH
TI  - Animal Scanner: Software for classifying humans, animals, and empty frames in camera trap images
T2  - ECOLOGY AND EVOLUTION
LA  - English
KW  - background subtraction
KW  - camera trap images
KW  - deep convolutional neural networks
KW  - human-animal detection
KW  - wildlife monitoring
KW  - OCCUPANCY
AB  - Camera traps are a popular tool to sample animal populations because they are noninvasive, detect a variety of species, and can record many thousands of animal detections per deployment. Cameras are typically set to take bursts of multiple photographs for each detection and are deployed in arrays of dozens or hundreds of sites, often resulting in millions of photographs per study. The task of converting photographs to animal detection records from such large image collections is daunting, and made worse by situations that generate copious empty pictures from false triggers (e.g., camera malfunction or moving vegetation) or pictures of humans. We developed computer vision algorithms to detect and classify moving objects to aid the first step of camera trap image filteringseparating the animal detections from the empty frames and pictures of humans. Our new work couples foreground object segmentation through background subtraction with deep learning classification to provide a fast and accurate scheme for human-animal detection. We provide these programs as both Matlab GUI and command prompt developed with C++. The software reads folders of camera trap images and outputs images annotated with bounding boxes around moving objects and a text file summary of results. This software maintains high accuracy while reducing the execution time by 14 times. It takes about 6 seconds to process a sequence of ten frames (on a 2.6 GHZ CPU computer). For those cameras with excessive empty frames due to camera malfunction or blowing vegetation automatically removes 54% of the false-triggers sequences without influencing the human/animal sequences. We achieve 99.58% on image-level empty versus object classification of Serengeti dataset. We offer the first computer vision tool for processing camera trap images providing substantial time savings for processing large image datasets, thus improving our ability to monitor wildlife across large scales with camera traps.
AD  - Univ Missouri Columbia, Dept Elect & Comp Engn, Columbia, MO 65211 USAAD  - North Carolina State Univ, Dept Forestry & Environm Resources, Raleigh, NC 27695 USAAD  - North Carolina Museum Nat Sci, Raleigh, NC USAC3  - University of Missouri SystemC3  - University of Missouri ColumbiaC3  - University of North CarolinaC3  - North Carolina State UniversityFU  - National Science Foundation [CyberSEES-1539389]
FX  - National Science Foundation, Grant/Award Number: CyberSEES-1539389
CR  - BARALDI A, 1995, IEEE T GEOSCI REMOTE, V33, P293, DOI 10.1109/36.377929
CR  - Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
CR  - Bowler MT, 2017, REMOTE SENS ECOL CON, V3, P146, DOI 10.1002/rse2.35
CR  - Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
CR  - Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
CR  - Dong P, 2016, IEEE T IMAGE PROCESS, V25, P5035, DOI 10.1109/TIP.2016.2598680
CR  - Fei-Fei L, 2005, PROC CVPR IEEE, P524
CR  - Gregory T, 2014, METHODS ECOL EVOL, V5, P443, DOI 10.1111/2041-210X.12177
CR  - He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
CR  - He ZH, 2016, IEEE CIRC SYST MAG, V16, P73, DOI 10.1109/MCAS.2015.2510200
CR  - Huang HC, 2015, SENSORS-BASEL, V15, P27116, DOI 10.3390/s151027116
CR  - Kays R., 2016, CANDID CREATURES CAM
CR  - Kays R, 2017, J APPL ECOL, V54, P242, DOI 10.1111/1365-2664.12700
CR  - Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
CR  - Lee C.-Y., 2014, ARXIV14095185
CR  - Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
CR  - McShea WJ, 2016, LANDSCAPE ECOL, V31, P55, DOI 10.1007/s10980-015-0262-9
CR  - Miguel A, 2016, IEEE IMAGE PROC, P1334, DOI 10.1109/ICIP.2016.7532575
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - Ojala T., 2001, Advances in Pattern Recognition - ICAPR 2001. Second International Conference. Proceedings (Lecture Notes in Computer Science Vol.2013), P397
CR  - Ren J, 2017, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2017.87
CR  - Ren S., 2015, ARXIV150601497
CR  - Shu XB, 2014, PROC CVPR IEEE, P3874, DOI 10.1109/CVPR.2014.495
CR  - Steenweg R, 2016, BIOL CONSERV, V201, P192, DOI 10.1016/j.biocon.2016.06.020
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Trigeorgis G, 2014, PR MACH LEARN RES, V32, P1692
CR  - Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
CR  - Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768
CR  - Yousif H., 2017, IEEE INT C IM PROC
CR  - Yousif H., 2017, CIRC SYST 2017 ISCAS, P1894
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DA  - FEB
PY  - 2019
VL  - 9
IS  - 4
SP  - 1578
EP  - 1589
DO  - 10.1002/ece3.4747
AN  - WOS:000461114900005
N1  - Times Cited in Web of Science Core Collection:  22
Total Times Cited:  22
Cited Reference Count:  30
ER  -

TY  - JOUR
AU  - Du, L
AU  - McCarty, GW
AU  - Zhang, X
AU  - Lang, MW
AU  - Vanderhoof, MK
AU  - Li, X
AU  - Huang, CQ
AU  - Lee, S
AU  - Zou, ZH
TI  - Mapping Forested Wetland Inundation in the Delmarva Peninsula, USA Using Deep Convolutional Neural Networks
T2  - REMOTE SENSING
LA  - English
KW  - forested wetland
KW  - inundation
KW  - deep learning
KW  - lidar
KW  - WorldView-3
KW  - DEM
KW  - topographic wetness index
KW  - GEOGRAPHICALLY ISOLATED WETLANDS
KW  - LIDAR INTENSITY
KW  - CLASSIFICATION
KW  - MARYLAND
AB  - The Delmarva Peninsula in the eastern United States is partially characterized by thousands of small, forested, depressional wetlands that are highly sensitive to weather variability and climate change, but provide critical ecosystem services. Due to the relatively small size of these depressional wetlands and their occurrence under forest canopy cover, it is very challenging to map their inundation status based on existing remote sensing data and traditional classification approaches. In this study, we applied a state-of-the-art U-Net semantic segmentation network to map forested wetland inundation in the Delmarva area by integrating leaf-off WorldView-3 (WV3) multispectral data with fine spatial resolution light detection and ranging (lidar) intensity and topographic data, including a digital elevation model (DEM) and topographic wetness index (TWI). Wetland inundation labels generated from lidar intensity were used for model training and validation. The wetland inundation map results were also validated using field data, and compared to the U.S. Fish and Wildlife Service National Wetlands Inventory (NWI) geospatial dataset and a random forest output from a previous study. Our results demonstrate that our deep learning model can accurately determine inundation status with an overall accuracy of 95% (Kappa = 0.90) compared to field data and high overlap (IoU = 70%) with lidar intensity-derived inundation labels. The integration of topographic metrics in deep learning models can improve the classification accuracy for depressional wetlands. This study highlights the great potential of deep learning models to improve the accuracy of wetland inundation maps through use of high-resolution optical and lidar remote sensing datasets.
AD  - USDA ARS, Hydrol & Remote Sensing Lab, Beltsville, MD 20705 USAAD  - Manchester Metropolitan Univ, Sch Comp Math & Digital Technol, Manchester M1 5GD, Lancs, EnglandAD  - US Fish & Wildlife Serv, Natl Wetlands Inventory, Falls Church, VA 22041 USAAD  - US Geol Survey, Geosci & Environm Change Sci Ctr, DFC, POB 25046,MS980, Denver, CO 80225 USAAD  - Univ Maryland, Dept Geog Sci, College Pk, MD 20742 USAAD  - Univ Maryland, Dept Environm Sci & Technol, College Pk, MD 20742 USAC3  - United States Department of Agriculture (USDA)C3  - Manchester Metropolitan UniversityC3  - United States Department of the InteriorC3  - US Fish & Wildlife ServiceC3  - United States Department of the InteriorC3  - United States Geological SurveyC3  - University System of MarylandC3  - University of Maryland College ParkC3  - University System of MarylandC3  - University of Maryland College ParkFU  - U.S. Department of Agriculture (USDA) Natural Resources Conservation Service; Wetland Component of the National Conservation Effects Assessment Project; U.S. Fish andWildlife Service (USFWS)
FX  - This work was supported by the U.S. Department of Agriculture (USDA) Natural Resources Conservation Service, in association with the Wetland Component of the National Conservation Effects Assessment Project and interagency agreement with U.S. Fish andWildlife Service (USFWS). The findings and conclusions in this article are those of the author(s), and do not necessarily represent the views of the USFWS. Any use of trade, firm, or product names is for descriptive purposes only, and does not imply endorsement by the U.S. Government.
CR  - Ator S.W., 2005, SURFICIAL HYDROGEOLO
CR  - Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
CR  - BENEDIKTSSON JA, 1990, IEEE T GEOSCI REMOTE, V28, P540, DOI 10.1109/TGRS.1990.572944
CR  - Bolanos S, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8040285
CR  - Chan RH, 2005, IEEE T IMAGE PROCESS, V14, P1479, DOI 10.1109/TIP.2005.852196
CR  - Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
CR  - Choi S.S., 2010, J SYSTEMICS CYBERNET, V8, P43
CR  - Cohen MJ, 2016, P NATL ACAD SCI USA, V113, P1978, DOI 10.1073/pnas.1512650113
CR  - DeVries B, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9080807
CR  - Diakogiannis F. I., 2019, ARXIV190400592
CR  - Ding P, 2018, ISPRS J PHOTOGRAMM, V141, P208, DOI 10.1016/j.isprsjprs.2018.05.005
CR  - Du ZR, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070888
CR  - Flood N, 2019, INT J APPL EARTH OBS, V82, DOI 10.1016/j.jag.2019.101897
CR  - Hayes MM, 2014, REMOTE SENS LETT, V5, P112, DOI 10.1080/2150704X.2014.882526
CR  - Homer C, 2015, PHOTOGRAMM ENG REM S, V81, P345, DOI 10.14358/PERS.81.5.345
CR  - Huang CQ, 2014, REMOTE SENS ENVIRON, V141, P231, DOI 10.1016/j.rse.2013.10.020
CR  - Huang WL, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10050797
CR  - Jin HR, 2017, REMOTE SENS ENVIRON, V190, P26, DOI 10.1016/j.rse.2016.12.001
CR  - Kellenberger B, 2018, REMOTE SENS ENVIRON, V216, P139, DOI 10.1016/j.rse.2018.06.028
CR  - Khatami R, 2016, REMOTE SENS ENVIRON, V177, P89, DOI 10.1016/j.rse.2016.02.028
CR  - Lang M, 2013, WETLANDS, V33, P141, DOI 10.1007/s13157-012-0359-8
CR  - Lang M, 2012, WETLANDS, V32, P461, DOI 10.1007/s13157-012-0279-7
CR  - Lang MW, 2008, IEEE T GEOSCI REMOTE, V46, P535, DOI 10.1109/TGRS.2007.909950
CR  - Lang MW, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12040707
CR  - Lang MW, 2009, WETLANDS, V29, P1166, DOI 10.1672/08-197.1
CR  - Li J., 2013, INT J ADV INFORM SCI, V5, P791, DOI [10.4156/aiss.vol5.issue3.92, DOI 10.4156/AISS.VOL5.ISSUE3.92]
CR  - Li RR, 2018, IEEE J-STARS, V11, P3954, DOI 10.1109/JSTARS.2018.2833382
CR  - Li X, 2018, J GEOPHYS RES-BIOGEO, V123, P3649, DOI 10.1029/2018JG004824
CR  - Li X, 2018, GEODERMA, V309, P41, DOI 10.1016/j.geoderma.2017.09.003
CR  - Lowrance R, 1997, ENVIRON MANAGE, V21, P687, DOI 10.1007/s002679900060
CR  - Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
CR  - SHEDLOCK RJ, 1999, 2355A USGS
CR  - Stedman S., STATUS TRENDS WETLAN
CR  - Sun Y, 2019, FORESTS, V10, DOI 10.3390/f10111047
CR  - Tiner RW, 2003, WETLANDS, V23, P494, DOI 10.1672/0277-5212(2003)023[0494:GIWOTU]2.0.CO;2
CR  - Vanderhoof MK, 2018, WETL ECOL MANAG, V26, P63, DOI 10.1007/s11273-017-9554-y
CR  - Vanderhoof MK, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9020105
CR  - Wu QS, 2019, REMOTE SENS ENVIRON, V228, P1, DOI 10.1016/j.rse.2019.04.015
CR  - Zhang X, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11131554
CR  - Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
CR  - Zhu WT, 2019, MED PHYS, V46, P576, DOI 10.1002/mp.13300
CR  - Zou ZH, 2018, P NATL ACAD SCI USA, V115, P3810, DOI 10.1073/pnas.1719275115
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - FEB
PY  - 2020
VL  - 12
IS  - 4
DO  - 10.3390/rs12040644
AN  - WOS:000519564600055
N1  - Times Cited in Web of Science Core Collection:  19
Total Times Cited:  19
Cited Reference Count:  42
ER  -

TY  - JOUR
AU  - Shepley, A
AU  - Falzon, G
AU  - Lawson, C
AU  - Meek, P
AU  - Kwan, P
TI  - U-Infuse: Democratization of Customizable Deep Learning for Object Detection
T2  - SENSORS
LA  - English
KW  - animal identification
KW  - artificial intelligence
KW  - camera-trap images
KW  - camera trapping
KW  - deep convolutional neural networks
KW  - deep learning
KW  - environmental software
KW  - wildlife ecology
KW  - wildlife monitoring
KW  - ecological object detection
KW  - CAMERA TRAP IMAGES
AB  - Image data is one of the primary sources of ecological data used in biodiversity conservation and management worldwide. However, classifying and interpreting large numbers of images is time and resource expensive, particularly in the context of camera trapping. Deep learning models have been used to achieve this task but are often not suited to specific applications due to their inability to generalise to new environments and inconsistent performance. Models need to be developed for specific species cohorts and environments, but the technical skills required to achieve this are a key barrier to the accessibility of this technology to ecologists. Thus, there is a strong need to democratize access to deep learning technologies by providing an easy-to-use software application allowing non-technical users to train custom object detectors. U-Infuse addresses this issue by providing ecologists with the ability to train customised models using publicly available images and/or their own images without specific technical expertise. Auto-annotation and annotation editing functionalities minimize the constraints of manually annotating and pre-processing large numbers of images. U-Infuse is a free and open-source software solution that supports both multiclass and single class training and object detection, allowing ecologists to access deep learning technologies usually only available to computer scientists, on their own device, customised for their application, without sharing intellectual property or sensitive data. It provides ecological practitioners with the ability to (i) easily achieve object detection within a user-friendly GUI, generating a species distribution report, and other useful statistics, (ii) custom train deep learning models using publicly available and custom training data, (iii) achieve supervised auto-annotation of images for further training, with the benefit of editing annotations to ensure quality datasets. Broad adoption of U-Infuse by ecological practitioners will improve ecological image analysis and processing by allowing significantly more image data to be processed with minimal expenditure of time and resources, particularly for camera trap images. Ease of training and use of transfer learning means domain-specific models can be trained rapidly, and frequently updated without the need for computer science expertise, or data sharing, protecting intellectual property and privacy.
AD  - Univ New England, Sch Sci & Technol, Armidale, NSW 2350, AustraliaAD  - Flinders Univ S Australia, Coll Sci & Engn, Adelaide, SA 5001, AustraliaAD  - NSW Dept Primary Ind, Vertebrate Pest Res Unit, POB 530, Coffs Harbour, NSW 2450, AustraliaAD  - Univ New England, Sch Environm & Rural Sci, Armidale, NSW 2350, AustraliaAD  - Melbourne Inst Technol, Sch IT & Engn, Melbourne, Vic 3000, AustraliaC3  - University of New EnglandC3  - Flinders University South AustraliaC3  - NSW Department of Primary IndustriesC3  - University of New EnglandFU  - NSW Environmental Trust "Developing Strategies for Effective Feral Cat Management" project; Australian Government Research Training Program (RTP) Scholarship; University of New England
FX  - This research was funded by the NSW Environmental Trust "Developing Strategies for Effective Feral Cat Management" project. Andrew Shepley acknowledges the support provided through the Australian Government Research Training Program (RTP) Scholarship. The APC was funded by the University of New England.
CR  - Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/3022670.2976746, 10.1145/2951913.2976746]
CR  - Ahumada JA, 2020, ENVIRON CONSERV, V47, P1, DOI 10.1017/S0376892919000298
CR  - Ahumada JA, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073707
CR  - [Anonymous], 2017, DRIVEN DATA PROJECT
CR  - [Anonymous], 2015, TZUTALIN LABELIMG GI
CR  - Anton Victor, 2018, Journal of Urban Ecology, V4, pjuy002, DOI 10.1093/jue/juy002
CR  - Bengsen A, 2014, ECOL MANAG RESTOR, V15, P97, DOI 10.1111/emr.12086
CR  - Falzon G, 2020, ANIMALS-BASEL, V10, DOI 10.3390/ani10010058
CR  - Falzon G, 2014, CAMERA TRAPPING: WILDLIFE MANAGEMENT AND RESEARCH, P299
CR  - Fegraus E.H., 2016, CAMERA TRAPPING WILD, P33
CR  - Fleming P, 2014, CAMERA TRAPPING WILD
CR  - Glover-Kapfer P, 2019, REMOTE SENS ECOL CON, V5, P209, DOI 10.1002/rse2.106
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Greenberg S, 2019, ECOL EVOL, V9, P13706, DOI 10.1002/ece3.5767
CR  - Hendry H, 2018, ORYX, V52, P15, DOI 10.1017/S0030605317001818
CR  - Lashley MA, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-22638-6
CR  - Legge S, 2020, WILDLIFE RES, V47, P731, DOI 10.1071/WR20089
CR  - Legge S, 2020, WILDLIFE RES, V47, P523, DOI [10.1071/WR19174, 10.1071/WRv47n8_ED]
CR  - Li XY, 2018, DIVERS DISTRIB, V24, P1560, DOI 10.1111/ddi.12792
CR  - Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI [10.1109/TPAMI.2018.2858826, 10.1109/ICCV.2017.324]
CR  - Miao ZQ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-44565-w
CR  - Nickolls John, 2008, ACM Queue, V6, DOI 10.1145/1365490.1365500
CR  - Norouzzadeh MS, 2018, P NATL ACAD SCI USA, V115, pE5716, DOI 10.1073/pnas.1719367115
CR  - OConnell AF, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P1, DOI 10.1007/978-4-431-99495-4
CR  - Perona, 2018, RECOGNITION TERRA IN
CR  - R.Core Team, 2018, R LANG ENV STAT COMP, DOI DOI 10.1007/978-3-540-74686-7
CR  - Rahman DA, 2017, ORYX, V51, P665, DOI 10.1017/S0030605316000429
CR  - Redmon J, 2016, ARXIV 161208242
CR  - Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Rovero F., 2016, CAMERA TRAPPING WILD
CR  - Rowcliffe JM, 2014, METHODS ECOL EVOL, V5, P1170, DOI 10.1111/2041-210X.12278
CR  - Schneider S, 2020, ECOL EVOL, V10, P3503, DOI 10.1002/ece3.6147
CR  - Schneider S, 2018, 2018 15TH CONFERENCE ON COMPUTER AND ROBOT VISION (CRV), P321, DOI 10.1109/CRV.2018.00052
CR  - Shepley A, 2021, ECOL EVOL, V11, P4494, DOI [10.1002/ece3.7344, 10.5281/ZENODO.4544074, 10.5281/ZENODO.4544073]
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tiwary U.S, 2018, INT HUM COMP INT 10
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Yu XY, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-52
CR  - Zaumyslova OY., 2015, ACHIEV LIFE SCI, V9, P15, DOI [DOI 10.1016/J.ALS.2015.05.003, 10.1016/j.als.2015.05.003]
CR  - Zhang E, 2009, ENCY DATABASE SYSTEM
CR  - Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - APR
PY  - 2021
VL  - 21
IS  - 8
DO  - 10.3390/s21082611
AN  - WOS:000644820300001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  42
ER  -

TY  - JOUR
AU  - Dujon, AM
AU  - Ierodiaconou, D
AU  - Geeson, JJ
AU  - Arnould, JPY
AU  - Allan, BM
AU  - Katselidis, KA
AU  - Schofield, G
TI  - Machine learning to detect marine animals in UAV imagery: effect of morphology, spacing, behaviour and habitat
T2  - REMOTE SENSING IN ECOLOGY AND CONSERVATION
LA  - English
KW  - aerial surveys
KW  - demography
KW  - satellite imagery
KW  - deep learning
KW  - artificial intelligence
KW  - movement ecology
KW  - UNMANNED AERIAL VEHICLES
KW  - IDENTIFICATION
KW  - ACCURATE
KW  - DRONES
AB  - Machine learning algorithms are being increasingly used to process large volumes of wildlife imagery data from unmanned aerial vehicles (UAVs); however, suitable algorithms to monitor multiple species are required to enhance efficiency. Here, we developed a machine learning algorithm using a low-cost computer. We trained a convolutional neural network and tested its performance in: (1) distinguishing focal organisms of three marine taxa (Australian fur seals, loggerhead sea turtles and Australasian gannets; body size ranges: 0.8-2.5 m, 0.6-1.0 m, and 0.8-0.9 m, respectively); and (2) simultaneously delineating the fine-scale movement trajectories of multiple sea turtles at a fish cleaning station. For all species, the algorithm performed best at detecting individuals of similar body length, displaying consistent behaviour or occupying uniform habitat (proportion of individuals detected, or recall of 0.94, 0.79 and 0.75 for gannets, seals and turtles, respectively). For gannets, performance was impacted by spacing (huddling pairs with offspring) and behaviour (resting vs. flying shapes, overall precision: 0.74). For seals, accuracy was impacted by morphology (sexual dimorphism and pups), spacing (huddling and creches) and habitat complexity (seal sized boulders) (overall precision: 0.27). For sea turtles, performance was impacted by habitat complexity, position in water column, spacing, behaviour (interacting individuals) and turbidity (overall precision: 0.24); body size variation had no impact. For sea turtle trajectories, locations were estimated with a relative positioning error of <50 cm. In conclusion, we demonstrate that, while the same machine learning algorithm can be used to survey multiple species, no single algorithm captures all components optimally within a given site. We recommend that, rather than attempting to fully automate detection of UAV imagery data, semi-automation is implemented (i.e. part automated and part manual, as commonly practised for photo-identification). Approaches to enhance the efficiency of manual detection are required in parallel to the development of effective implementation of machine learning algorithms.
AD  - Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, Geelong, Vic 3216, AustraliaAD  - Deakin Univ, Sch Life & Environm Sci, Ctr Integrat Ecol, Warrnambool, Vic 3280, AustraliaAD  - Deakin Univ, Sch Life & Environm Sci, Burwood, Vic 3125, AustraliaAD  - Natl Marine Pk Zakynthos, Elefteriou Venizelou St, GR-29100 Zakynthos, GreeceAD  - Queen Mary Univ London, Sch Biol & Chem Sci, London E1 4NS, EnglandC3  - Deakin UniversityC3  - Deakin UniversityC3  - Deakin UniversityC3  - University of LondonC3  - Queen Mary University LondonCR  - Allan BM, 2019, DRONES-BASEL, V3, DOI 10.3390/drones3010024
CR  - Allan BM, 2015, FRONT ECOL ENVIRON, V13, P354, DOI 10.1890/15.WB.015
CR  - Anderson K, 2013, FRONT ECOL ENVIRON, V11, P138, DOI 10.1890/120150
CR  - Bonnin N, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2020017
CR  - Brack IV, 2018, METHODS ECOL EVOL, V9, P1864, DOI 10.1111/2041-210X.13026
CR  - Bradski G, 2000, DR DOBBS J, V25, P120
CR  - Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011
CR  - Chabot D, 2016, J FIELD ORNITHOL, V87, P343, DOI 10.1111/jofo.12171
CR  - Chabot D, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0122588
CR  - Chollet F., 2015, KERAS GITHUB
CR  - Christiansen F, 2016, ECOSPHERE, V7, DOI 10.1002/ecs2.1468
CR  - Christie KS, 2016, FRONT ECOL ENVIRON, V14, P242, DOI 10.1002/fee.1281
CR  - Clement MJ, 2017, ECOL EVOL, V7, P7304, DOI 10.1002/ece3.3284
CR  - Cruz JA, 2006, CANCER INFORM, V2, P59
CR  - Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
CR  - Domingos P, 2012, COMMUN ACM, V55, P78, DOI 10.1145/2347736.2347755
CR  - Dujon AM, 2019, ENDANGER SPECIES RES, V39, P91, DOI 10.3354/esr00958
CR  - Dujon AM, 2018, MAR ECOL-EVOL PERSP, V39, DOI 10.1111/maec.12489
CR  - Dujon AM, 2017, MAR BIOL, V164, DOI 10.1007/s00227-017-3216-8
CR  - Dujon AM, 2014, METHODS ECOL EVOL, V5, P1162, DOI 10.1111/2041-210X.12286
CR  - Eikelboom JAJ, 2019, METHODS ECOL EVOL, V10, P1875, DOI [10.1111/2041-210X.13277, 10.4121/UUID:BA99A206-3E5A-4673-B830-B5C866445B8C]
CR  - Fu YQ, 2018, METHODS ECOL EVOL, V9, P1531, DOI 10.1111/2041-210X.12992
CR  - Gaspar T., 2011, Proceedings of the 2011 IEEE International Conference on Mechatronics and Automation (ICMA 2011), P1050, DOI 10.1109/ICMA.2011.5985805
CR  - Gonzalez LF, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010097
CR  - Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P1490, DOI 10.1111/2041-210X.13246
CR  - Gray PC, 2019, METHODS ECOL EVOL, V10, P345, DOI 10.1111/2041-210X.13132
CR  - Hadfield JD, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i02
CR  - Hays GC, 2019, TRENDS ECOL EVOL, V34, P459, DOI 10.1016/j.tree.2019.01.009
CR  - Hazen EL, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aar3001
CR  - He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI 10.1109/TPAMI.2018.2844175
CR  - Hodgson A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0079556
CR  - Hodgson JC, 2018, METHODS ECOL EVOL, V9, P1160, DOI 10.1111/2041-210X.12974
CR  - Howell EA, 2015, FISH OCEANOGR, V24, P57, DOI 10.1111/fog.12092
CR  - Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
CR  - Ioffe S, 2015, PR MACH LEARN RES, V37, P448
CR  - Jones GP, 2006, WILDLIFE SOC B, V34, P750, DOI 10.2193/0091-7648(2006)34[750:AAOSUA]2.0.CO;2
CR  - Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415
CR  - Kampichler C, 2010, ECOL INFORM, V5, P441, DOI 10.1016/j.ecoinf.2010.06.003
CR  - Kingma DP, 2014, ARXIV PREPRINT ARXIV
CR  - Ko T, 2008, IEEE APP IMG PAT, P84
CR  - Koh LP, 2012, TROP CONSERV SCI, V5, P121, DOI 10.1177/194008291200500202
CR  - Krizhevsky A., 2009, LEARNING MULTIPLE LA
CR  - Kuhn HW., 1955, NAV RES LOG, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]
CR  - Lalonde R., 2017, IEEE CVPR, V1704, P1
CR  - Lamba A, 2019, CURR BIOL, V29, pR977, DOI 10.1016/j.cub.2019.08.016
CR  - Linchant J, 2015, MAMMAL REV, V45, P239, DOI 10.1111/mam.12046
CR  - Maire F, 2015, LECT NOTES ARTIF INT, V9457, P379, DOI 10.1007/978-3-319-26350-2_33
CR  - Montabone S, 2010, IMAGE VISION COMPUT, V28, P391, DOI 10.1016/j.imavis.2009.06.006
CR  - Mulero-Pazmany M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0083873
CR  - Olden JD, 2008, Q REV BIOL, V83, P171, DOI 10.1086/587826
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - QGIS.org, 2021, QGIS GEOGR INF SYST
CR  - R Development Core Team, 2013, **DATA OBJECT**
CR  - Raoult V, 2020, DRONES-BASEL, V4, DOI 10.3390/drones4040064
CR  - Raoult V, 2018, DRONES-BASEL, V2, DOI 10.3390/drones2040037
CR  - Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
CR  - Ren Shaoqing, 2017, IEEE Trans Pattern Anal Mach Intell, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
CR  - Sabokrou M., 2016, ABS160806037, P1
CR  - Schneider CA, 2012, NAT METHODS, V9, P671, DOI 10.1038/nmeth.2089
CR  - Schofield G, 2019, BIOL CONSERV, V238, DOI 10.1016/j.biocon.2019.108214
CR  - Schofield G, 2017, MAR ECOL PROG SER, V575, P153, DOI 10.3354/meps12193
CR  - Simard PY, 2003, PROC INT CONF DOC, P958
CR  - Srivastava N, 2014, J MACH LEARN RES, V15, P1929
CR  - Stieglitz TC, 2017, MAR ECOL PROG SER, V571, P139, DOI 10.3354/meps12123
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tanimoto S., 1975, COMPUT VISION GRAPH, V119, P104, DOI DOI 10.1016/S0146-664X(75)80003-7
CR  - Thessen A., 2016, ONE ECOSYST, V1, P8621, DOI [10.3897/oneeco.1.e8621, DOI 10.3897/ONEECO.1.E8621]
CR  - van Gemert JC, 2015, LECT NOTES COMPUT SC, V8925, P255, DOI 10.1007/978-3-319-16178-5_17
CR  - Villon S, 2018, ECOL INFORM, V48, P238, DOI 10.1016/j.ecoinf.2018.09.007
CR  - Wich SA, 2018, CONSERVATION DRONES: MAPPING AND MONITORING BIODIVERSITY, P1, DOI 10.1093/oso/9780198787617.001.0001
CR  - Yosinski J, 2014, ADV NEUR IN, V27
CR  - Zeng HQ, 2017, PROC INT CONF RECON
CR  - Zuur Alain F., 2009, P1
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN, NJ 07030 USA
DA  - SEP
PY  - 2021
VL  - 7
IS  - 3
SP  - 341
EP  - 354
DO  - 10.1002/rse2.205
AN  - WOS:000647190700001
N1  - Times Cited in Web of Science Core Collection:  6
Total Times Cited:  6
Cited Reference Count:  74
ER  -

TY  - JOUR
AU  - Pouliot, D
AU  - Alavi, N
AU  - Wilson, S
AU  - Duffe, J
AU  - Pasher, J
AU  - Davidson, A
AU  - Daneshfar, B
AU  - Lindsay, E
TI  - Assessment of Landsat Based Deep-Learning Membership Analysis for Development of from-to Change Time Series in the Prairie Region of Canada from 1984 to 2018
T2  - REMOTE SENSING
LA  - English
KW  - change detection
KW  - prairie
KW  - agriculture
KW  - grassland
KW  - deep learning
KW  - membership
KW  - Landsat
KW  - CONTERMINOUS UNITED-STATES
KW  - COVER DATABASE
KW  - GRASSLAND
KW  - CLASSIFICATION
KW  - METHODOLOGY
KW  - MISREGISTRATION
KW  - RECONSTRUCTION
KW  - ACCURACY
KW  - WATER
AB  - The prairie region of Canada is a dynamically changing landscape in relation to past and present anthropogenic activities and recent climate change. Improving our understanding of the rate, timing, and distribution of landscape change is needed to determine the impact on wildlife populations and biodiversity, ultimately leading to better-informed management regarding requirements for habitat amount and its connectedness. In this research, we assessed the viability of an approach to detect from-to class changes designed to be scalable to the prairie region with the capacity for local refinement. It employed a deep-learning convolutional neural network to model general land covers and examined class memberships to identify land-cover conversions. For this implementation, eight land-cover categories were derived from the Agriculture and Agri-Food Canada Annual Space-Based Crop Inventory. Change was assessed in three study areas that contained different mixes of grassland, pasture, and forest cover. Results showed that the deep-learning method produced the highest accuracy across all classes relative to an implementation of random forest that included some first-order texture measures. Overall accuracy was 4% greater with the deep-learning classifier and class accuracies were more balanced. Evaluation of change accuracy suggested good performance for many conversions such as grassland to crop, forest to crop, water to dryland covers, and most bare/developed-related changes. Changes involving pasture with grassland or cropland were more difficult to detect due to spectral confusion among classes. Similarly, conversion to forests in some cases was poorly detected due to gradual and subtle change characteristics combined with confusion between forest, shrub, and croplands. The proposed framework involved several processing steps that can be explored to enhance the thematic content and accuracy for large regional implementation. Evaluation for understanding connectivity in natural land covers and related declines in species at risk is planned for future research.
AD  - Environm & Climate Change Canada, Landscape Sci & Technol Div, 1125 Colonel Dr, Ottawa, ON K1A 0H3, CanadaAD  - Environm & Climate Change Canada, Wildlife Res Div, 1125 Colonel Dr, Ottawa, ON K1A 0H3, CanadaAD  - Agr & Agri Food Canada, Sci & Technol Branch, KW Neatby Bldg,960 Carling Ave, Ottawa, ON K1A 0C6, CanadaC3  - Environment & Climate Change CanadaC3  - Environment & Climate Change CanadaC3  - Canadian Wildlife ServiceC3  - Wildlife Research Division - Environment CanadaC3  - Agriculture & Agri Food CanadaFU  - Canadian Space Agency
FX  - Research was supported through a Canadian Space Agency grant for the project "Integrated Earth Observation Monitoring for Essential Ecosystem Information: Resilience to Ecosystem Stress and Climate Change".
CR  - [Anonymous], 2010, CAN BIOD EC STAT TRE
CR  - Bonsal BR, 2011, ATMOS OCEAN, V49, P303, DOI 10.1080/07055900.2011.555103
CR  - Boryan C, 2011, GEOCARTO INT, V26, P341, DOI 10.1080/10106049.2011.562309
CR  - Brown JF, 2020, REMOTE SENS ENVIRON, V238, DOI 10.1016/j.rse.2019.111356
CR  - Canadian Wildlife Service, 1991, FED POL WETL CONS
CR  - Chen J, 2015, ISPRS J PHOTOGRAMM, V103, P7, DOI 10.1016/j.isprsjprs.2014.09.002
CR  - Colditz RR, 2014, PHOTOGRAMM ENG REM S, V80, P918
CR  - Dai XL, 1998, IEEE T GEOSCI REMOTE, V36, P1566, DOI 10.1109/36.718860
CR  - Davidson A, 2017, HDB REMOTE SENSING A, P91
CR  - Fisette T, 2014, INT GEOSCI REMOTE SE, P5095, DOI 10.1109/IGARSS.2014.6947643
CR  - Fisher RJ, 2018, REMOTE SENS ENVIRON, V218, P201, DOI 10.1016/j.rse.2018.10.003
CR  - Gage Anne M., 2016, Great Plains Research, V26, P107
CR  - Gao BC, 1996, REMOTE SENS ENVIRON, V58, P257, DOI 10.1016/S0034-4257(96)00067-3
CR  - Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
CR  - Guo X., 2005, PRAIRIE PERSPECT, V8, P11
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hendrycks Dan, 2016, P 2017 INT C LEARNIN
CR  - Hermosilla T, 2018, CAN J REMOTE SENS, V44, P67, DOI 10.1080/07038992.2018.1437719
CR  - Hoekstra JM, 2005, ECOL LETT, V8, P23, DOI 10.1111/j.1461-0248.2004.00686.x
CR  - Homer C, 2020, ISPRS J PHOTOGRAMM, V162, P184, DOI 10.1016/j.isprsjprs.2020.02.019
CR  - Homer C, 2015, PHOTOGRAMM ENG REM S, V81, P345, DOI 10.14358/PERS.81.5.345
CR  - http://maps.elie.ucl.ac.be/CCI/viewer/download/ESACCI-LC-Ph2-PUGv2_2.0.pdf ESA, LAND COVER CCI PRODU
CR  - Hu X., 2018, INTELL TRANSP SYST, V20, P1
CR  - Jin SM, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11242971
CR  - Kovalskyy V, 2013, REMOTE SENS ENVIRON, V130, P280, DOI 10.1016/j.rse.2012.12.003
CR  - Latifovic R., 2016, REMOTE SENSING LAND, P303
CR  - Latifovic R, 2005, CAN J REMOTE SENS, V31, P347, DOI 10.5589/m05-019
CR  - Latifovic R, 2017, REMOTE SENS-BASEL, V9, DOI 10.3390/rs9111098
CR  - Lindsay EJ, 2019, RANGELAND ECOL MANAG, V72, P92, DOI 10.1016/j.rama.2018.07.005
CR  - Luthcke SB, 2013, J GLACIOL, V59, P613, DOI 10.3189/2013JoG12J147
CR  - McInnes WS, 2015, IEEE J-STARS, V8, P1395, DOI 10.1109/JSTARS.2015.2416713
CR  - North American Bird Conservation Initiative, 2016, STAT N AM BIRDS 201, P8
CR  - Olimb S, 2018, GEOJOURNAL, V83, P819, DOI 10.1007/s10708-017-9805-8
CR  - Olofsson P, 2014, REMOTE SENS ENVIRON, V148, P42, DOI 10.1016/j.rse.2014.02.015
CR  - Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
CR  - Pekel JF, 2016, NATURE, V540, P418, DOI 10.1038/nature20584
CR  - Pouliot D, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11070772
CR  - Pouliot D, 2018, REMOTE SENS ENVIRON, V204, P979, DOI 10.1016/j.rse.2017.07.036
CR  - Pouliot D, 2016, GISCI REMOTE SENS, V53, P382, DOI 10.1080/15481603.2015.1137112
CR  - Pouliot D, 2014, REMOTE SENS ENVIRON, V140, P731, DOI 10.1016/j.rse.2013.10.004
CR  - Riley J.L., 2007, CONSERVATION BLUEPRI
CR  - Roch L, 2014, ENVIRON MONIT ASSESS, V186, P2505, DOI 10.1007/s10661-013-3557-9
CR  - Rosenberg KV, 2019, SCIENCE, V366, P120, DOI 10.1126/science.aaw1313
CR  - Roy DP, 2000, IEEE T GEOSCI REMOTE, V38, P2017, DOI 10.1109/36.851783
CR  - Sermanet Pierre, 2013, ABS13126229 CORR
CR  - Stow D.A., 1980, PROC, V2, P1227
CR  - Sutton A, 2007, CAN J FOREST RES, V37, P1643, DOI 10.1139/X07-021
CR  - Szegedy C., 2015, 2015 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2015.7298594
CR  - Truong C, 2020, SIGNAL PROCESS, V167, DOI 10.1016/j.sigpro.2019.107299
CR  - United States Geological Survey, LANDS COLL 1 VS COLL
CR  - van Oort PAJ, 2007, REMOTE SENS ENVIRON, V108, P1, DOI 10.1016/j.rse.2006.10.012
CR  - Watmough M.D., 2007, ENV CANADAS PRAIRIE
CR  - WEAVER JE, 1958, ECOLOGY, V39, P393, DOI 10.2307/1931749
CR  - Wickham J, 2017, REMOTE SENS ENVIRON, V191, P328, DOI 10.1016/j.rse.2016.12.026
CR  - Xian G, 2009, REMOTE SENS ENVIRON, V113, P1133, DOI 10.1016/j.rse.2009.02.004
CR  - Yang XH, 2017, CAN J REMOTE SENS, V43, P62, DOI 10.1080/07038992.2017.1263151
CR  - Zhang X., 2019, CANADAS CHANGING CLI
CR  - Zhu Z, 2020, REMOTE SENS ENVIRON, V238, DOI 10.1016/j.rse.2019.03.009
CR  - Zhu Z, 2015, REMOTE SENS ENVIRON, V162, P67, DOI 10.1016/j.rse.2015.02.009
CR  - Zhu Z, 2014, REMOTE SENS ENVIRON, V144, P152, DOI 10.1016/j.rse.2014.01.011
CR  - Zhu Z, 2012, REMOTE SENS ENVIRON, V122, P75, DOI 10.1016/j.rse.2011.10.030
PU  - MDPI
PI  - BASEL
PA  - ST ALBAN-ANLAGE 66, CH-4052 BASEL, SWITZERLAND
DA  - FEB
PY  - 2021
VL  - 13
IS  - 4
DO  - 10.3390/rs13040634
AN  - WOS:000624445200001
N1  - Times Cited in Web of Science Core Collection:  1
Total Times Cited:  1
Cited Reference Count:  61
ER  -

TY  - JOUR
AU  - Shi, CM
AU  - Xu, J
AU  - Roberts, NJ
AU  - Liu, D
AU  - Jiang, GS
TI  - Individual automatic detection and identification of big cats with the combination of different body parts
T2  - INTEGRATIVE ZOOLOGY
LA  - English
KW  - combination of body parts
KW  - individual automatic identification
KW  - object detection
KW  - Panthera pardus orientalis
KW  - Panthera tigris altaica
KW  - IMAGES
KW  - SCIENCE
AB  - The development of facial recognition technology has become an increasingly powerful tool in wild animal individual recognition. In this paper, we develop an automatic detection and recognition method with the combinations of body features of big cats based on the deep convolutional neural network (CNN). We collected dataset including 12 244 images from 47 individual Amur tigers (Panthera tigris altaica) at the Siberian Tiger Park by mobile phones and digital camera and 1940 images and videos of 12 individual wild Amur leopard (Panthera pardus orientalis) by infrared cameras. First, the single shot multibox detector algorithm is used to perform the automatic detection process of feature regions in each image. For the different feature regions of the image, like face stripe or spots, CNNs and multi-layer perceptron models were applied to automatically identify tiger and leopard individuals, independently. Our results show that the identification accuracy of Amur tiger can reach up to 93.27% for face front, 93.33% for right body stripe, and 93.46% for left body stripe. Furthermore, the combination of right face, left body stripe, and right body stripe achieves the highest accuracy rate, up to 95.55%. Consequently, the combination of different body parts can improve the individual identification accuracy. However, it is not the higher the number of body parts, the higher the accuracy rate. The combination model with 3 body parts has the highest accuracy. The identification accuracy of Amur leopard can reach up to 86.90% for face front, 89.13% for left body spots, and 88.33% for right body spots. The accuracy of different body parts combination is lower than the independent part. For wild Amur leopard, the combination of face with body spot part is not helpful for the improvement of identification accuracy. The most effective identification part is still the independent left or right body spot part. It can be applied in long-term monitoring of big cats, including big data analysis for animal behavior, and be helpful for the individual identification of other wildlife species.
AD  - Zhejiang Agr & Forestry Univ, Coll Math & Comp Sci, Hangzhou, Peoples R ChinaAD  - Northeast Forestry Univ, Sch Sci, Dept Math, Harbin, Peoples R ChinaAD  - Northeast Forestry Univ, Coll Wildlife & Protected Areas, Feline Res Ctr, Natl Forestry & Grassland Adm, Harbin 150040, Heilongjiang, Peoples R ChinaAD  - Siberian Tiger Pk, Harbin, Peoples R ChinaC3  - Zhejiang A&F UniversityC3  - Northeast Forestry University - ChinaC3  - Northeast Forestry University - ChinaFU  - Fundamental Research Funds for the Central Universities [2572020BC05]; Heilongjiang postdoctoral fund project [LBH-Z18003]; Biodiversity Survey, Monitoring and Assessment Project of Ministry of Ecology and Environment, China [2019HB2096001006]; National Natural Science Foundation of China [NSFC 31872241]; National Innovation and Entrepreneurship Training Program for College Student [S202010225022]; Individual Identification Technological Research on Camera-trapping images of Amur tigers (NFGA 2017)
FX  - This study was funded by the Fundamental Research Funds for the Central Universities (2572020BC05), the Heilongjiang postdoctoral fund project (LBH-Z18003), the Biodiversity Survey, Monitoring and Assessment Project of Ministry of Ecology and Environment, China (2019HB2096001006), the National Natural Science Foundation of China (NSFC 31872241), the Individual Identification Technological Research on Camera-trapping images of Amur tigers (NFGA 2017), and National Innovation and Entrepreneurship Training Program for College Student (S202010225022).
CR  - Berryman A, 2001, OIKOS, V92, P265, DOI 10.1034/j.1600-0706.2001.920208.x
CR  - Carpenter SR, 2009, P NATL ACAD SCI USA, V106, P1305, DOI 10.1073/pnas.0808772106
CR  - Cheema GS, 2017, LECT NOTES ARTIF INT, V10536, P27, DOI 10.1007/978-3-319-71273-4_3
CR  - Chen XY, 2021, IEEE T INTELL TRANSP, DOI 10.1109/TITS.2021.3113608
CR  - Crouse D, 2017, BMC ZOOL, V2, DOI 10.1186/s40850-016-0011-9
CR  - Ding X., 2021, ARXIV PREPRINT ARXIV
CR  - Dosovitskiy A., 2020, ARXIV PREPRINT ARXIV, P2021
CR  - Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
CR  - Villa AG, 2017, ECOL INFORM, V41, P24, DOI 10.1016/j.ecoinf.2017.07.004
CR  - Goodfellow I., 2016, DEEP LEARNING, P125
CR  - Guo ST, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101412
CR  - He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
CR  - Hiby L, 2009, BIOL LETTERS, V5, P383, DOI 10.1098/rsbl.2009.0028
CR  - Hou J, 2020, BIOL CONSERV, V242, DOI 10.1016/j.biocon.2020.108414
CR  - Huang GL, 2017, IEEE ICC
CR  - Karanth KU, 2011, CAMERA TRAPS IN ANIMAL ECOLOGY: METHODS AND ANALYSES, P97, DOI 10.1007/978-4-431-99495-4_7
CR  - LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
CR  - LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
CR  - Liu HJ, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3105702
CR  - Schofield D, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aaw0736
CR  - Shi CM, 2020, INTEGR ZOOL, V15, P461, DOI 10.1111/1749-4877.12453
CR  - Simonyan K., 2014, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1409.1556
CR  - Swanson A, 2016, CONSERV BIOL, V30, P520, DOI 10.1111/cobi.12695
CR  - Swanson A, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.26
CR  - Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
CR  - Tabak MA, 2019, METHODS ECOL EVOL, V10, P585, DOI 10.1111/2041-210X.13120
CR  - Tolstikhin I., 2021, ARXIV PREPRINT ARXIV
CR  - Vaswani A., 2017, ARXIV170603762, P6000, DOI DOI 10.5555/3295222.3295349
CR  - Willi M, 2019, METHODS ECOL EVOL, V10, P80, DOI 10.1111/2041-210X.13099
CR  - Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
CR  - Zhang WW, 2011, IEEE T IMAGE PROCESS, V20, P1696, DOI 10.1109/TIP.2010.2099126
CR  - Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
PU  - WILEY
PI  - HOBOKEN
PA  - 111 RIVER ST, HOBOKEN 07030-5774, NJ USA
DO  - 10.1111/1749-4877.12641
AN  - WOS:000779441000001
N1  - Times Cited in Web of Science Core Collection:  0
Total Times Cited:  0
Cited Reference Count:  32
ER  -

